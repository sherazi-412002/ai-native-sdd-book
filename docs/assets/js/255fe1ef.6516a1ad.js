"use strict";(globalThis.webpackChunkclassic=globalThis.webpackChunkclassic||[]).push([[506],{5359(e,n,a){a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>p});const t=JSON.parse('{"id":"module-3/isaac-ros-vslam","title":"Isaac ROS VSLAM","description":"Visual Simultaneous Localization and Mapping","source":"@site/docs/module-3/isaac-ros-vslam.md","sourceDirName":"module-3","slug":"/module-3/isaac-ros-vslam","permalink":"/ai-native-sdd-book/docs/module-3/isaac-ros-vslam","draft":false,"unlisted":false,"editUrl":"https://github.com/sherazi-412002/ai-native-sdd-book/tree/main/docs/module-3/isaac-ros-vslam.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"curriculumSidebar","previous":{"title":"Synthetic Data","permalink":"/ai-native-sdd-book/docs/module-3/synthetic-data"},"next":{"title":"Nav2 Path Planning","permalink":"/ai-native-sdd-book/docs/module-3/nav2-path-planning"}}');var s=a(4848),o=a(8453);const r={sidebar_position:2},i="Isaac ROS VSLAM",l={},p=[{value:"Visual Simultaneous Localization and Mapping",id:"visual-simultaneous-localization-and-mapping",level:2},{value:"Introduction to VSLAM",id:"introduction-to-vslam",level:2},{value:"Isaac ROS VSLAM Components",id:"isaac-ros-vslam-components",level:2},{value:"Stereo VSLAM Component",id:"stereo-vslam-component",level:3},{value:"Visual Odometry",id:"visual-odometry",level:2},{value:"Loop Closure",id:"loop-closure",level:2},{value:"Mapping",id:"mapping",level:2},{value:"Integration with Navigation",id:"integration-with-navigation",level:2},{value:"Isaac Sim Setup Parameters",id:"isaac-sim-setup-parameters",level:2},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"isaac-ros-vslam",children:"Isaac ROS VSLAM"})}),"\n",(0,s.jsx)(n.h2,{id:"visual-simultaneous-localization-and-mapping",children:"Visual Simultaneous Localization and Mapping"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covers Visual SLAM (VSLAM) techniques using NVIDIA Isaac ROS for humanoid robots. Visual SLAM is a critical technology that enables robots to simultaneously map their environment and determine their location within it using only visual sensors. For humanoid robots, this capability is essential for autonomous navigation, manipulation, and interaction in unknown environments."}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides a comprehensive suite of VSLAM algorithms optimized for NVIDIA hardware, including GPU acceleration for real-time performance. The platform offers several VSLAM solutions including Isaac ROS Stereo VSLAM, Isaac ROS Mono VSLAM, and Isaac ROS Visual Inertial Odometry (VIO) that combine visual and inertial measurements for improved robustness."}),"\n",(0,s.jsx)(n.p,{children:"The key advantages of Isaac ROS VSLAM for humanoid robots include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time performance on NVIDIA GPUs"}),"\n",(0,s.jsx)(n.li,{children:"Robust tracking in dynamic environments"}),"\n",(0,s.jsx)(n.li,{children:"Integration with other Isaac ROS components"}),"\n",(0,s.jsx)(n.li,{children:"Optimized for embedded deployment"}),"\n",(0,s.jsx)(n.li,{children:"Support for multiple camera configurations"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-vslam",children:"Introduction to VSLAM"}),"\n",(0,s.jsx)(n.p,{children:"Visual Simultaneous Localization and Mapping (VSLAM) is a computer vision and robotics technique that allows a robot to construct a map of an unknown environment while simultaneously keeping track of its location within that map. This is achieved through the analysis of visual input from one or more cameras."}),"\n",(0,s.jsx)(n.p,{children:"The VSLAM pipeline typically consists of several stages:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Detection and Extraction"}),": Identifying distinctive visual features in the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Matching"}),": Associating features across different camera views"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pose Estimation"}),": Computing the camera's pose relative to the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mapping"}),": Building a 3D representation of the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loop Closure"}),": Detecting when the robot returns to a previously visited location"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The mathematical foundation of VSLAM is based on the Extended Kalman Filter (EKF), Particle Filter, or optimization-based approaches like Bundle Adjustment. The state vector typically includes the camera pose and 3D landmark positions:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"x = [R, t, p1, p2, ..., pn]\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where R and t represent the camera rotation and translation, and pi represents the 3D coordinates of the i-th landmark."}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-vslam-components",children:"Isaac ROS VSLAM Components"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides several specialized VSLAM components that are optimized for different use cases and sensor configurations:"}),"\n",(0,s.jsx)(n.h3,{id:"stereo-vslam-component",children:"Stereo VSLAM Component"}),"\n",(0,s.jsx)(n.p,{children:"The Isaac ROS Stereo VSLAM component uses stereo cameras to achieve real-time SLAM with metric scale estimation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom stereo_msgs.msg import DisparityImage\nfrom nav_msgs.msg import Odometry\nimport cv2\nimport numpy as np\nfrom cv_bridge import CvBridge\n\nclass IsaacROSStereoVSLAM(Node):\n    \"\"\"\n    Isaac ROS Stereo VSLAM implementation\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_ros_stereo_vslam')\n\n        # Parameters\n        self.declare_parameter('left_topic', '/camera/left/image_rect_color')\n        self.declare_parameter('right_topic', '/camera/right/image_rect_color')\n        self.declare_parameter('baseline', 0.075)  # Stereo baseline in meters\n        self.declare_parameter('focal_length', 320.0)  # Focal length in pixels\n        self.declare_parameter('min_disparity', 0)\n        self.declare_parameter('max_disparity', 64)\n\n        # Get parameters\n        self.left_topic = self.get_parameter('left_topic').value\n        self.right_topic = self.get_parameter('right_topic').value\n        self.baseline = self.get_parameter('baseline').value\n        self.focal_length = self.get_parameter('focal_length').value\n        self.min_disparity = self.get_parameter('min_disparity').value\n        self.max_disparity = self.get_parameter('max_disparity').value\n\n        # Initialize stereo matcher\n        self.stereo = cv2.StereoSGBM_create(\n            minDisparity=self.min_disparity,\n            numDisparities=self.max_disparity - self.min_disparity,\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n        # CV Bridge\n        self.cv_bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.left_sub = self.create_subscription(\n            Image, self.left_topic, self.left_image_callback, 10)\n        self.right_sub = self.create_subscription(\n            Image, self.right_topic, self.right_image_callback, 10)\n        self.odom_pub = self.create_publisher(Odometry, 'stereo_vslam/odometry', 10)\n\n        # Stereo image storage\n        self.left_image = None\n        self.right_image = None\n\n        # Pose tracking\n        self.current_pose = np.eye(4)\n\n    def left_image_callback(self, msg: Image):\n        \"\"\"Handle left camera image\"\"\"\n        self.left_image = self.cv_bridge.imgmsg_to_cv2(msg, 'bgr8')\n        self.process_stereo_pair()\n\n    def right_image_callback(self, msg: Image):\n        \"\"\"Handle right camera image\"\"\"\n        self.right_image = self.cv_bridge.imgmsg_to_cv2(msg, 'bgr8')\n        self.process_stereo_pair()\n\n    def process_stereo_pair(self):\n        \"\"\"Process stereo image pair to generate disparity and estimate pose\"\"\"\n        if self.left_image is None or self.right_image is None:\n            return\n\n        # Convert to grayscale\n        left_gray = cv2.cvtColor(self.left_image, cv2.COLOR_BGR2GRAY)\n        right_gray = cv2.cvtColor(self.right_image, cv2.COLOR_BGR2GRAY)\n\n        # Compute disparity\n        disparity = self.stereo.compute(left_gray, right_gray).astype(np.float32) / 16.0\n\n        # Convert disparity to depth\n        depth = (self.baseline * self.focal_length) / (disparity + 1e-6)\n\n        # Use depth information for 3D reconstruction and pose estimation\n        # (Simplified implementation - real system would use feature-based approach)\n        self.estimate_pose_from_depth(depth)\n\n        # Clear images to save memory\n        self.left_image = None\n        self.right_image = None\n\n    def estimate_pose_from_depth(self, depth_map):\n        \"\"\"Estimate pose change using depth information\"\"\"\n        # This is a simplified implementation\n        # In a real stereo VSLAM system, features would be tracked across frames\n        # and 3D points would be reconstructed using triangulation\n\n        # For now, we'll simulate pose change based on feature tracking\n        # between consecutive stereo pairs\n\n        # Publish odometry\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = 'map'\n        odom_msg.child_frame_id = 'base_link'\n\n        # Update and publish pose\n        self.current_pose[0, 3] += 0.01  # Simulated forward movement\n        self.current_pose[1, 3] += 0.005  # Simulated lateral movement\n\n        # Convert to ROS message\n        odom_msg.pose.pose.position.x = self.current_pose[0, 3]\n        odom_msg.pose.pose.position.y = self.current_pose[1, 3]\n        odom_msg.pose.pose.position.z = self.current_pose[2, 3]\n\n        # Publish odometry\n        self.odom_pub.publish(odom_msg)\n\n### Visual Inertial Odometry (VIO)\n\nThe Isaac ROS VIO component combines visual and inertial measurements for robust pose estimation:\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu\nfrom geometry_msgs.msg import Vector3\nfrom nav_msgs.msg import Odometry\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass IsaacROSVisualInertialOdometry(Node):\n    \"\"\"\n    Isaac ROS Visual Inertial Odometry implementation\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_ros_vio')\n\n        # Parameters\n        self.declare_parameter('image_topic', '/camera/rgb/image_raw')\n        self.declare_parameter('imu_topic', '/imu/data')\n        self.declare_parameter('integration_rate', 100.0)  # Hz\n\n        # Get parameters\n        self.image_topic = self.get_parameter('image_topic').value\n        self.imu_topic = self.get_parameter('imu_topic').value\n        self.integration_rate = self.get_parameter('integration_rate').value\n\n        # Initialize\n        self.cv_bridge = CvBridge()\n        self.feature_detector = cv2.ORB_create(nfeatures=500)\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n\n        # State variables\n        self.current_pose = np.eye(4)\n        self.velocity = np.zeros(3)\n        self.angular_velocity = np.zeros(3)\n        self.acceleration = np.zeros(3)\n\n        # Previous measurements for integration\n        self.prev_imu_time = None\n        self.prev_imu_orientation = R.from_quat([0, 0, 0, 1])\n        self.prev_imu_velocity = np.zeros(3)\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image, self.image_topic, self.image_callback, 10)\n        self.imu_sub = self.create_subscription(\n            Imu, self.imu_topic, self.imu_callback, 10)\n        self.odom_pub = self.create_publisher(Odometry, 'vio/odometry', 10)\n\n    def imu_callback(self, msg: Imu):\n        \"\"\"Handle IMU data\"\"\"\n        # Extract orientation from quaternion\n        quat = [msg.orientation.x, msg.orientation.y, msg.orientation.z, msg.orientation.w]\n        current_orientation = R.from_quat(quat)\n\n        # Extract linear acceleration\n        self.acceleration = np.array([\n            msg.linear_acceleration.x,\n            msg.linear_acceleration.y,\n            msg.linear_acceleration.z\n        ])\n\n        # Extract angular velocity\n        self.angular_velocity = np.array([\n            msg.angular_velocity.x,\n            msg.angular_velocity.y,\n            msg.angular_velocity.z\n        ])\n\n        # Get current time\n        current_time = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n\n        if self.prev_imu_time is not None:\n            dt = current_time - self.prev_imu_time\n\n            # Integrate angular velocity to get orientation change\n            # This is a simplified integration - real systems use more sophisticated methods\n            delta_angle = self.angular_velocity * dt\n            delta_rotation = R.from_rotvec(delta_angle)\n\n            # Update orientation\n            self.prev_imu_orientation = current_orientation\n\n            # Integrate linear acceleration to get velocity\n            self.velocity += self.acceleration * dt\n\n            # Integrate velocity to get position change\n            position_change = self.velocity * dt\n            self.current_pose[:3, 3] += position_change\n\n        self.prev_imu_time = current_time\n\n    def image_callback(self, msg: Image):\n        \"\"\"Handle camera image with visual feature tracking\"\"\"\n        cv_image = self.cv_bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        # Convert to grayscale\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\n        # Detect and track features\n        keypoints, descriptors = self.feature_detector.detectAndCompute(gray, None)\n\n        # Process visual odometry (simplified)\n        # In a real system, this would track features across frames\n        # and use them to refine the IMU-based estimates\n\n        # Publish integrated pose\n        self.publish_odometry(msg.header.stamp)\n\n    def publish_odometry(self, stamp):\n        \"\"\"Publish odometry message with VIO estimate\"\"\"\n        odom_msg = Odometry()\n        odom_msg.header.stamp = stamp\n        odom_msg.header.frame_id = 'map'\n        odom_msg.child_frame_id = 'base_link'\n\n        # Position\n        odom_msg.pose.pose.position.x = self.current_pose[0, 3]\n        odom_msg.pose.pose.position.y = self.current_pose[1, 3]\n        odom_msg.pose.pose.position.z = self.current_pose[2, 3]\n\n        # Velocity\n        odom_msg.twist.twist.linear.x = self.velocity[0]\n        odom_msg.twist.twist.linear.y = self.velocity[1]\n        odom_msg.twist.twist.linear.z = self.velocity[2]\n\n        # Publish\n        self.odom_pub.publish(odom_msg)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"visual-odometry",children:"Visual Odometry"}),"\n",(0,s.jsx)(n.p,{children:"Visual Odometry (VO) is the process of incrementally estimating the pose of a camera by analyzing the changes in consecutive images. For humanoid robots, VO provides continuous pose estimation that is essential for navigation and mapping."}),"\n",(0,s.jsx)(n.p,{children:"The core algorithm involves:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Feature detection and description"}),"\n",(0,s.jsx)(n.li,{children:"Feature matching between frames"}),"\n",(0,s.jsx)(n.li,{children:"Motion estimation from feature correspondences"}),"\n",(0,s.jsx)(n.li,{children:"Optimization and outlier rejection"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Here's a comprehensive implementation of visual odometry:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport cv2\nfrom typing import List, Tuple, Optional\nimport time\n\nclass VisualOdometry:\n    """\n    Visual Odometry implementation for humanoid robot navigation\n    """\n\n    def __init__(self, camera_matrix: np.ndarray,\n                 feature_detector_type: str = \'orb\',\n                 max_features: int = 1000,\n                 matching_threshold: float = 0.7):\n\n        self.camera_matrix = camera_matrix\n        self.matching_threshold = matching_threshold\n\n        # Initialize feature detector\n        if feature_detector_type == \'orb\':\n            self.detector = cv2.ORB_create(nfeatures=max_features)\n            self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n        elif feature_detector_type == \'sift\':\n            self.detector = cv2.SIFT_create(nfeatures=max_features)\n            self.matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n        elif feature_detector_type == \'akaze\':\n            self.detector = cv2.AKAZE_create()\n            self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n        else:\n            raise ValueError(f"Unsupported feature detector: {feature_detector_type}")\n\n        # Pose estimation\n        self.current_pose = np.eye(4)\n        self.prev_image = None\n        self.prev_kp = None\n        self.prev_desc = None\n\n        # Tracking state\n        self.is_initialized = False\n        self.tracking_quality = 0.0\n        self.feature_matches = 0\n\n    def process_frame(self, image: np.ndarray) -> Tuple[np.ndarray, bool]:\n        """\n        Process a new frame and return the estimated pose and tracking status\n        """\n        # Convert to grayscale if needed\n        if len(image.shape) == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image\n\n        # Detect features\n        current_kp, current_desc = self.detector.detectAndCompute(gray, None)\n\n        if current_kp is None or current_desc is None:\n            return self.current_pose, False\n\n        if not self.is_initialized:\n            # Initialize tracking with first frame\n            self.prev_image = gray\n            self.prev_kp = current_kp\n            self.prev_desc = current_desc\n            self.is_initialized = True\n            return self.current_pose, True\n\n        # Match features between previous and current frame\n        matches = self.match_features(self.prev_desc, current_desc)\n\n        if len(matches) < 10:  # Need minimum matches for robust estimation\n            return self.current_pose, False\n\n        # Extract matched points\n        prev_points = np.float32([self.prev_kp[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n        current_points = np.float32([current_kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n        # Estimate essential matrix\n        E, mask = cv2.findEssentialMat(\n            prev_points,\n            current_points,\n            self.camera_matrix,\n            method=cv2.RANSAC,\n            threshold=1.0,\n            prob=0.999\n        )\n\n        if E is None or E.size == 0:\n            return self.current_pose, False\n\n        # Recover pose\n        _, R, t, mask_pose = cv2.recoverPose(E, prev_points, current_points, self.camera_matrix)\n\n        # Create transformation matrix\n        transformation = np.eye(4)\n        transformation[:3, :3] = R\n        transformation[:3, 3] = t.flatten()\n\n        # Update current pose (relative to first frame)\n        self.current_pose = self.current_pose @ transformation\n\n        # Update tracking quality metrics\n        self.feature_matches = len(matches)\n        self.tracking_quality = min(1.0, len(matches) / 50.0)  # Normalize by expected match count\n\n        # Update previous frame data\n        self.prev_image = gray\n        self.prev_kp = current_kp\n        self.prev_desc = current_desc\n\n        return self.current_pose, True\n\n    def match_features(self, desc1, desc2) -> List[cv2.DMatch]:\n        """Match features between two descriptor sets"""\n        if desc1 is None or desc2 is None or len(desc1) == 0 or len(desc2) == 0:\n            return []\n\n        # Find matches\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n\n        # Apply Lowe\'s ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < self.matching_threshold * n.distance:\n                    good_matches.append(m)\n\n        return good_matches\n\n    def reset(self):\n        """Reset the visual odometry"""\n        self.current_pose = np.eye(4)\n        self.prev_image = None\n        self.prev_kp = None\n        self.prev_desc = None\n        self.is_initialized = False\n        self.tracking_quality = 0.0\n        self.feature_matches = 0\n\n# Integration with Isaac ROS\nclass IsaacROSVOIntegration:\n    """\n    Integration of Visual Odometry with Isaac ROS\n    """\n\n    def __init__(self, node):\n        self.node = node\n        self.vo = None\n        self.camera_matrix = None\n        self.is_active = False\n\n    def initialize_vo(self, camera_matrix: np.ndarray):\n        """Initialize the visual odometry module"""\n        try:\n            self.vo = VisualOdometry(camera_matrix)\n            self.camera_matrix = camera_matrix\n            self.is_active = True\n            self.node.get_logger().info(\'Visual Odometry initialized successfully\')\n        except Exception as e:\n            self.node.get_logger().error(f\'Failed to initialize VO: {str(e)}\')\n\n    def process_image(self, image_msg):\n        """Process image message and update pose estimate"""\n        if not self.is_active or self.vo is None:\n            return\n\n        # Convert ROS image to OpenCV\n        cv_bridge = CvBridge()\n        cv_image = cv_bridge.imgmsg_to_cv2(image_msg, desired_encoding=\'passthrough\')\n\n        # Process with VO\n        pose, success = self.vo.process_frame(cv_image)\n\n        if success:\n            # Publish pose update\n            self.publish_pose_estimate(pose, image_msg.header.stamp)\n\n    def publish_pose_estimate(self, pose: np.ndarray, timestamp):\n        """Publish the estimated pose to ROS topics"""\n        # Create PoseStamped message\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = timestamp\n        pose_msg.header.frame_id = \'map\'\n\n        # Extract position and orientation\n        position = pose[:3, 3]\n        rotation_matrix = pose[:3, :3]\n        qw, qx, qy, qz = self.rotation_matrix_to_quaternion(rotation_matrix)\n\n        pose_msg.pose.position.x = float(position[0])\n        pose_msg.pose.position.y = float(position[1])\n        pose_msg.pose.position.z = float(position[2])\n        pose_msg.pose.orientation.w = qw\n        pose_msg.pose.orientation.x = qx\n        pose_msg.pose.orientation.y = qy\n        pose_msg.pose.orientation.z = qz\n\n        # Publish to VO topic\n        self.node.vo_pose_pub.publish(pose_msg)\n\n    def rotation_matrix_to_quaternion(self, R: np.ndarray) -> Tuple[float, float, float, float]:\n        """Convert rotation matrix to quaternion (same implementation as before)"""\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2\n            qw = 0.25 * s\n            qx = (R[2, 1] - R[1, 2]) / s\n            qy = (R[0, 2] - R[2, 0]) / s\n            qz = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                qw = (R[2, 1] - R[1, 2]) / s\n                qx = 0.25 * s\n                qy = (R[0, 1] + R[1, 0]) / s\n                qz = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                qw = (R[0, 2] - R[2, 0]) / s\n                qx = (R[0, 1] + R[1, 0]) / s\n                qy = 0.25 * s\n                qz = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                qw = (R[1, 0] - R[0, 1]) / s\n                qx = (R[0, 2] + R[2, 0]) / s\n                qy = (R[1, 2] + R[2, 1]) / s\n                qz = 0.25 * s\n\n        return qw, qx, qy, qz\n'})}),"\n",(0,s.jsx)(n.h2,{id:"loop-closure",children:"Loop Closure"}),"\n",(0,s.jsx)(n.p,{children:"Loop closure is a critical component of VSLAM that recognizes when the robot returns to a previously visited location, allowing the system to correct accumulated drift errors. Isaac ROS implements advanced loop closure detection using place recognition and geometric verification."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport cv2\nfrom typing import List, Tuple, Dict\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import normalize\nimport faiss\nimport time\n\nclass LoopClosureDetector:\n    """\n    Loop closure detection for Isaac ROS VSLAM\n    """\n\n    def __init__(self,\n                 vocab_size: int = 1000,\n                 min_loop_matches: int = 20,\n                 loop_threshold: float = 0.6,\n                 max_db_size: int = 10000):\n\n        self.vocab_size = vocab_size\n        self.min_loop_matches = min_loop_matches\n        self.loop_threshold = loop_threshold\n        self.max_db_size = max_db_size\n\n        # Initialize vocabulary for bag-of-words\n        self.vocabulary = None\n        self.vocabulary_trained = False\n\n        # Database of keyframes\n        self.keyframes_db = []\n        self.keyframe_features = []  # BoW representations\n        self.keyframe_poses = []\n\n        # Feature extraction\n        self.feature_extractor = cv2.ORB_create(nfeatures=500)\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n\n        # FAISS index for efficient similarity search\n        self.index = faiss.IndexFlatL2(32)  # Using 32-dimensional descriptors\n\n        # Clustering for vocabulary construction\n        self.kmeans = MiniBatchKMeans(n_clusters=vocab_size, random_state=42)\n\n    def add_keyframe(self, image: np.ndarray, pose: np.ndarray) -> bool:\n        """\n        Add a new keyframe to the database\n        """\n        if not self.vocabulary_trained:\n            # Collect features for vocabulary training\n            keypoints, descriptors = self.feature_extractor.detectAndCompute(image, None)\n            if descriptors is not None:\n                self._collect_vocabulary_features(descriptors)\n            return False\n\n        # Extract features and compute BoW representation\n        keypoints, descriptors = self.feature_extractor.detectAndCompute(image, None)\n        if descriptors is None:\n            return False\n\n        # Compute BoW representation\n        bow_descriptor = self._compute_bow_descriptor(descriptors)\n\n        # Add to database\n        self.keyframes_db.append(image)\n        self.keyframe_features.append(bow_descriptor)\n        self.keyframe_poses.append(pose.copy())\n\n        # Add to FAISS index\n        self.index.add(bow_descriptor.reshape(1, -1).astype(\'float32\'))\n\n        # Limit database size\n        if len(self.keyframes_db) > self.max_db_size:\n            self._remove_oldest_keyframes()\n\n        # Check for loop closure\n        loop_result = self._detect_loop_closure(bow_descriptor, pose)\n\n        return loop_result\n\n    def _collect_vocabulary_features(self, descriptors: np.ndarray):\n        """\n        Collect features for vocabulary training\n        """\n        if not hasattr(self, \'vocab_features\'):\n            self.vocab_features = descriptors\n        else:\n            self.vocab_features = np.vstack([self.vocab_features, descriptors])\n\n        # Train vocabulary when enough features are collected\n        if len(self.vocab_features) >= self.vocab_size * 10:\n            self._train_vocabulary()\n\n    def _train_vocabulary(self):\n        """\n        Train the vocabulary using k-means clustering\n        """\n        if len(self.vocab_features) < self.vocab_size:\n            return\n\n        # Normalize features\n        normalized_features = normalize(self.vocab_features, norm=\'l2\')\n\n        # Train k-means\n        self.kmeans.fit(normalized_features)\n\n        # Get vocabulary\n        self.vocabulary = self.kmeans.cluster_centers_\n        self.vocabulary_trained = True\n\n        # Clear temporary features\n        delattr(self, \'vocab_features\')\n\n        print(f\'Vocabulary trained with {self.vocab_size} words\')\n\n    def _compute_bow_descriptor(self, descriptors: np.ndarray) -> np.ndarray:\n        """\n        Compute bag-of-words descriptor for image\n        """\n        if not self.vocabulary_trained:\n            return np.zeros(self.vocab_size)\n\n        # Normalize descriptors\n        normalized_desc = normalize(descriptors, norm=\'l2\')\n\n        # Assign each descriptor to closest vocabulary word\n        distances = np.linalg.norm(\n            normalized_desc[:, np.newaxis, :] - self.vocabulary[np.newaxis, :, :],\n            axis=2\n        )\n\n        # Get closest vocabulary word for each descriptor\n        assignments = np.argmin(distances, axis=1)\n\n        # Create histogram\n        bow_descriptor = np.bincount(assignments, minlength=self.vocab_size)\n\n        # Normalize histogram\n        bow_descriptor = bow_descriptor.astype(np.float32)\n        if np.sum(bow_descriptor) > 0:\n            bow_descriptor = bow_descriptor / np.sum(bow_descriptor)\n\n        return bow_descriptor\n\n    def _detect_loop_closure(self, current_descriptor: np.ndarray, current_pose: np.ndarray) -> bool:\n        """\n        Detect loop closure by comparing with previous keyframes\n        """\n        if len(self.keyframe_features) < 10:  # Need minimum keyframes\n            return False\n\n        # Search for similar keyframes using FAISS\n        current_descriptor = current_descriptor.astype(\'float32\').reshape(1, -1)\n        scores, indices = self.index.search(current_descriptor, k=min(10, len(self.keyframe_features)))\n\n        # Check top matches\n        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n            if score < self.loop_threshold:\n                # Found potential loop closure\n                # Perform geometric verification\n                is_valid_loop = self._verify_geometric_consistency(idx, current_pose)\n                if is_valid_loop:\n                    # Optimize pose graph to correct drift\n                    self._optimize_poses(idx)\n                    return True\n\n        return False\n\n    def _verify_geometric_consistency(self, old_keyframe_idx: int, current_pose: np.ndarray) -> bool:\n        """\n        Verify geometric consistency of loop closure hypothesis\n        """\n        # This would typically involve:\n        # 1. Re-matching features between current and old keyframe\n        # 2. Computing relative pose using PnP or similar\n        # 3. Comparing with expected relative pose from pose graph\n        # 4. Checking geometric constraints\n\n        # For simplicity, we\'ll just check pose distance\n        old_pose = self.keyframe_poses[old_keyframe_idx]\n\n        # Calculate position distance\n        pos_diff = np.linalg.norm(current_pose[:3, 3] - old_pose[:3, 3])\n\n        # Calculate rotation difference\n        R1 = current_pose[:3, :3]\n        R2 = old_pose[:3, :3]\n        R_diff = R1 @ R2.T\n        trace = np.trace(R_diff)\n        rot_diff = np.arccos(np.clip((trace - 1) / 2, -1, 1))\n\n        # Check if geometrically plausible (thresholds would be tuned based on application)\n        position_threshold = 2.0  # meters\n        rotation_threshold = 0.5  # radians\n\n        return pos_diff < position_threshold and rot_diff < rotation_threshold\n\n    def _optimize_poses(self, loop_idx: int):\n        """\n        Optimize poses to correct drift after loop closure detection\n        """\n        # In a real system, this would involve:\n        # 1. Building a pose graph with loop constraints\n        # 2. Optimizing using bundle adjustment or graph optimization\n        # 3. Updating map points to reflect corrected poses\n\n        print(f\'Loop closure detected! Optimizing poses from index {loop_idx}\')\n\n    def _remove_oldest_keyframes(self):\n        """\n        Remove oldest keyframes to maintain database size\n        """\n        num_to_remove = len(self.keyframes_db) - self.max_db_size + 100  # Remove extra to avoid frequent removals\n\n        # Remove from lists\n        self.keyframes_db = self.keyframes_db[num_to_remove:]\n        self.keyframe_features = self.keyframe_features[num_to_remove:]\n        self.keyframe_poses = self.keyframe_poses[num_to_remove:]\n\n        # Rebuild FAISS index (simplified approach)\n        if self.keyframe_features:\n            all_features = np.array(self.keyframe_features).astype(\'float32\')\n            self.index = faiss.IndexFlatL2(all_features.shape[1])\n            self.index.add(all_features)\n        else:\n            self.index = faiss.IndexFlatL2(32)  # Default size\n'})}),"\n",(0,s.jsx)(n.h2,{id:"mapping",children:"Mapping"}),"\n",(0,s.jsx)(n.p,{children:"The mapping component of VSLAM builds and maintains a representation of the environment. For humanoid robots, this map is essential for navigation, path planning, and task execution."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport open3d as o3d\nfrom typing import List, Dict, Tuple\nimport cv2\nfrom scipy.spatial import cKDTree\n\nclass VSLAMMap:\n    """\n    VSLAM mapping component for humanoid robot environment representation\n    """\n\n    def __init__(self,\n                 resolution: float = 0.1,  # meters\n                 max_points: int = 100000,\n                 update_rate: float = 1.0):  # Hz\n\n        self.resolution = resolution\n        self.max_points = max_points\n        self.update_rate = update_rate\n\n        # Point cloud representation\n        self.points = np.empty((0, 3))  # 3D points [x, y, z]\n        self.colors = np.empty((0, 3))  # RGB colors\n        self.descriptors = {}  # Feature descriptors for map points\n        self.point_ids = []  # Unique IDs for map points\n        self.next_point_id = 0\n\n        # Occupancy grid for navigation\n        self.occupancy_grid = {}\n        self.grid_resolution = 0.5  # meters\n\n        # Tracking\n        self.last_update_time = 0\n        self.active = True\n\n    def add_keyframe_observation(self,\n                                image: np.ndarray,\n                                depth_map: Optional[np.ndarray],\n                                camera_pose: np.ndarray,\n                                camera_matrix: np.ndarray):\n        """\n        Add observations from a keyframe to the map\n        """\n        if not self.active:\n            return\n\n        # Extract features from image\n        keypoints, descriptors = self.extract_features(image)\n\n        if descriptors is None:\n            return\n\n        # Triangulate 3D points if depth is available, otherwise use feature tracking\n        if depth_map is not None:\n            points_3d, colors, valid_mask = self.triangulate_from_depth(\n                keypoints, depth_map, camera_matrix, camera_pose)\n        else:\n            # Use other methods to estimate depth (stereo, motion, etc.)\n            points_3d, colors = self.estimate_points_from_features(\n                keypoints, camera_pose, camera_matrix)\n            valid_mask = np.ones(len(points_3d), dtype=bool)\n\n        if len(points_3d) > 0:\n            # Transform points to world coordinates\n            world_points = self.transform_points_to_world(points_3d, camera_pose)\n            world_colors = colors[valid_mask] if depth_map is not None else colors\n\n            # Add points to map\n            self.add_points_to_map(world_points, world_colors, descriptors[valid_mask] if depth_map is not None else descriptors)\n\n    def extract_features(self, image: np.ndarray):\n        """\n        Extract features from image\n        """\n        # Convert to grayscale if needed\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n\n        # Detect and compute features\n        detector = cv2.ORB_create(nfeatures=500)\n        keypoints, descriptors = detector.detectAndCompute(gray, None)\n\n        if keypoints is not None:\n            # Extract pixel coordinates and colors\n            kp_coords = np.array([kp.pt for kp in keypoints])\n            kp_colors = np.array([image[int(kp.pt[1]), int(kp.pt[0])] for kp in keypoints if\n                                 0 <= int(kp.pt[1]) < image.shape[0] and 0 <= int(kp.pt[0]) < image.shape[1]])\n            return kp_coords, descriptors, kp_colors\n\n        return None, None, None\n\n    def triangulate_from_depth(self,\n                             keypoints: np.ndarray,\n                             depth_map: np.ndarray,\n                             camera_matrix: np.ndarray,\n                             camera_pose: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        """\n        Triangulate 3D points from 2D keypoints and depth map\n        """\n        if keypoints.size == 0:\n            return np.empty((0, 3)), np.empty((0, 3)), np.array([])\n\n        # Get pixel coordinates\n        u = keypoints[:, 0].astype(int)\n        v = keypoints[:, 1].astype(int)\n\n        # Check bounds\n        valid_mask = (u >= 0) & (u < depth_map.shape[1]) & (v >= 0) & (v < depth_map.shape[0])\n\n        if not np.any(valid_mask):\n            return np.empty((0, 3)), np.empty((0, 3)), np.array([])\n\n        # Extract valid coordinates and depths\n        u_valid = u[valid_mask]\n        v_valid = v[valid_mask]\n        depths = depth_map[v_valid, u_valid]\n\n        # Remove invalid depth values\n        depth_valid_mask = (depths > 0) & (depths < 100)  # Valid depth range\n        final_mask = valid_mask.copy()\n        final_mask[valid_mask] = depth_valid_mask\n\n        if not np.any(depth_valid_mask):\n            return np.empty((0, 3)), np.empty((0, 3)), np.array([])\n\n        u_final = u_valid[depth_valid_mask]\n        v_final = v_valid[depth_valid_mask]\n        depths_final = depths[depth_valid_mask]\n\n        # Convert to 3D points in camera frame\n        K_inv = np.linalg.inv(camera_matrix)\n        pixels = np.stack([u_final, v_final, np.ones_like(u_final)], axis=1)\n        rays = (K_inv @ pixels.T).T  # Rays from camera center\n        points_cam = rays * depths_final[:, np.newaxis]  # Scale by depth\n\n        # Transform to world coordinates\n        R = camera_pose[:3, :3]\n        t = camera_pose[:3, 3]\n        points_world = (R @ points_cam.T).T + t\n\n        # Get colors from original image\n        colors = np.zeros((len(points_world), 3))\n        for i, (u, v) in enumerate(zip(u_final, v_final)):\n            if len(image.shape) == 3:\n                colors[i] = image[v, u][::-1] / 255.0  # BGR to RGB and normalize\n            else:\n                colors[i] = np.full(3, image[v, u] / 255.0)\n\n        return points_world, colors, final_mask\n\n    def estimate_points_from_features(self, keypoints: np.ndarray,\n                                    camera_pose: np.ndarray,\n                                    camera_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        """\n        Estimate 3D points from feature matches (simplified approach)\n        """\n        # This is a simplified implementation\n        # In reality, this would use triangulation from multiple views\n        # or other depth estimation techniques\n\n        # For now, create points at a fixed depth\n        points = np.zeros((len(keypoints), 3))\n        colors = np.random.rand(len(keypoints), 3)  # Random colors\n\n        # Convert 2D points to 3D with assumed depth\n        for i, (u, v) in enumerate(keypoints):\n            # Create a ray from camera center through pixel\n            ray = np.linalg.inv(camera_matrix) @ np.array([u, v, 1])\n            ray = ray / np.linalg.norm(ray)  # Normalize\n            points[i] = camera_pose[:3, 3] + ray * 2.0  # Assume 2m depth\n\n        return points, colors\n\n    def transform_points_to_world(self, points: np.ndarray, camera_pose: np.ndarray) -> np.ndarray:\n        """\n        Transform points from camera frame to world frame\n        """\n        R = camera_pose[:3, :3]\n        t = camera_pose[:3, 3]\n        return (R @ points.T).T + t\n\n    def add_points_to_map(self, new_points: np.ndarray,\n                         new_colors: np.ndarray,\n                         descriptors: np.ndarray = None):\n        """\n        Add new points to the map with duplicate removal\n        """\n        if len(new_points) == 0:\n            return\n\n        # Remove duplicates based on resolution\n        unique_points, unique_indices = self.remove_duplicate_points(new_points)\n\n        if len(unique_points) == 0:\n            return\n\n        # Update point cloud\n        new_colors = new_colors[unique_indices]\n\n        # Add to existing points\n        self.points = np.vstack([self.points, unique_points])\n        self.colors = np.vstack([self.colors, new_colors])\n\n        # Add point IDs\n        for _ in range(len(unique_points)):\n            self.point_ids.append(self.next_point_id)\n            self.next_point_id += 1\n\n        # Limit map size\n        if len(self.points) > self.max_points:\n            self._limit_map_size()\n\n        # Update occupancy grid\n        self._update_occupancy_grid(unique_points)\n\n    def remove_duplicate_points(self, new_points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        """\n        Remove duplicate points based on resolution\n        """\n        if len(self.points) == 0:\n            return new_points, np.arange(len(new_points))\n\n        # Use k-d tree for efficient nearest neighbor search\n        if len(self.points) > 100:  # Only build tree if we have enough points\n            tree = cKDTree(self.points)\n\n            # Find nearest neighbors for each new point\n            distances, indices = tree.query(new_points)\n\n            # Keep only points that are farther than resolution threshold\n            keep_mask = distances > self.resolution\n            unique_points = new_points[keep_mask]\n            unique_indices = np.where(keep_mask)[0]\n        else:\n            # Simple approach for small maps\n            keep_mask = np.ones(len(new_points), dtype=bool)\n            unique_points = []\n            unique_indices = []\n\n            for i, point in enumerate(new_points):\n                # Check distance to existing points\n                min_dist = float(\'inf\')\n                for existing_point in self.points:\n                    dist = np.linalg.norm(point - existing_point)\n                    if dist < min_dist:\n                        min_dist = dist\n\n                if min_dist > self.resolution:\n                    unique_points.append(point)\n                    unique_indices.append(i)\n\n            unique_points = np.array(unique_points)\n            unique_indices = np.array(unique_indices)\n\n        return unique_points, unique_indices\n\n    def _limit_map_size(self):\n        """\n        Limit map size by removing oldest points\n        """\n        excess = len(self.points) - self.max_points\n        if excess > 0:\n            # Remove excess points (simple approach - remove first N points)\n            self.points = self.points[excess:]\n            self.colors = self.colors[excess:]\n            self.point_ids = self.point_ids[excess:]\n\n    def _update_occupancy_grid(self, new_points: np.ndarray):\n        """\n        Update occupancy grid with new points\n        """\n        # This would implement a 2D or 3D occupancy grid\n        # For simplicity, we\'ll just store the concept\n\n        # In a real implementation:\n        # 1. Discretize space into grid cells\n        # 2. Update occupancy probability for each cell\n        # 3. Handle sensor uncertainty and multiple observations\n        pass\n\n    def get_map_as_pointcloud(self) -> o3d.geometry.PointCloud:\n        """\n        Get current map as Open3D point cloud\n        """\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(self.points)\n        pcd.colors = o3d.utility.Vector3dVector(self.colors)\n        return pcd\n\n    def save_map(self, filename: str):\n        """\n        Save map to file\n        """\n        np.savez(filename,\n                points=self.points,\n                colors=self.colors,\n                point_ids=np.array(self.point_ids))\n\n    def load_map(self, filename: str):\n        """\n        Load map from file\n        """\n        data = np.load(filename)\n        self.points = data[\'points\']\n        self.colors = data[\'colors\']\n        self.point_ids = data[\'point_ids\'].tolist()\n        self.next_point_id = max(self.point_ids) + 1 if self.point_ids else 0\n'})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-navigation",children:"Integration with Navigation"}),"\n",(0,s.jsx)(n.p,{children:"The VSLAM system must integrate seamlessly with the navigation stack to provide localization and mapping services for the humanoid robot:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom nav_msgs.msg import Path, OccupancyGrid\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom std_msgs.msg import Header\nfrom builtin_interfaces.msg import Time\nfrom tf2_ros import TransformListener, Buffer\nimport tf2_geometry_msgs\nimport numpy as np\nimport threading\nfrom typing import Optional\n\nclass IsaacROSNavIntegration(Node):\n    \"\"\"\n    Integration between Isaac ROS VSLAM and navigation stack\n    \"\"\"\n\n    def __init__(self):\n        super().__init__('isaac_ros_nav_integration')\n\n        # Parameters\n        self.declare_parameter('map_frame', 'map')\n        self.declare_parameter('odom_frame', 'odom')\n        self.declare_parameter('base_frame', 'base_link')\n        self.declare_parameter('update_rate', 10.0)  # Hz\n\n        self.map_frame = self.get_parameter('map_frame').value\n        self.odom_frame = self.get_parameter('odom_frame').value\n        self.base_frame = self.get_parameter('base_frame').value\n        self.update_rate = self.get_parameter('update_rate').value\n\n        # TF setup\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Publishers\n        self.global_path_pub = self.create_publisher(Path, 'global_plan', 10)\n        self.local_path_pub = self.create_publisher(Path, 'local_plan', 10)\n        self.map_pub = self.create_publisher(OccupancyGrid, 'map', 10)\n        self.goal_pub = self.create_publisher(Marker, 'goal_marker', 10)\n        self.robot_pose_pub = self.create_publisher(PoseStamped, 'robot_pose', 10)\n\n        # Timer for periodic updates\n        self.timer = self.create_timer(1.0/self.update_rate, self.update_callback)\n\n        # Navigation state\n        self.current_pose = None\n        self.global_path = []\n        self.local_path = []\n        self.map_data = None\n\n        # Threading for non-blocking operations\n        self.nav_lock = threading.RLock()\n\n        self.get_logger().info('Isaac ROS Navigation Integration initialized')\n\n    def update_callback(self):\n        \"\"\"\n        Periodic update callback\n        \"\"\"\n        # Get current robot pose from TF\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                self.map_frame,\n                self.base_frame,\n                rclpy.time.Time())\n\n            # Convert transform to pose\n            pose = PoseStamped()\n            pose.header.stamp = self.get_clock().now().to_msg()\n            pose.header.frame_id = self.map_frame\n            pose.pose.position.x = transform.transform.translation.x\n            pose.pose.position.y = transform.transform.translation.y\n            pose.pose.position.z = transform.transform.translation.z\n            pose.pose.orientation = transform.transform.rotation\n\n            # Publish robot pose\n            self.robot_pose_pub.publish(pose)\n\n            with self.nav_lock:\n                self.current_pose = pose\n\n        except Exception as e:\n            self.get_logger().warn(f'Could not get transform: {str(e)}')\n\n    def publish_global_path(self, waypoints: List[Tuple[float, float, float]]):\n        \"\"\"\n        Publish global path for navigation\n        \"\"\"\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = self.map_frame\n\n        for x, y, theta in waypoints:\n            pose = PoseStamped()\n            pose.header.frame_id = self.map_frame\n            pose.pose.position.x = x\n            pose.pose.position.y = y\n            pose.pose.position.z = 0.0\n            # Convert theta to quaternion\n            qw = np.cos(theta/2)\n            qz = np.sin(theta/2)\n            pose.pose.orientation.w = qw\n            pose.pose.orientation.z = qz\n\n            path_msg.poses.append(pose)\n\n        self.global_path_pub.publish(path_msg)\n\n    def publish_local_path(self, local_waypoints: List[Tuple[float, float, float]]):\n        \"\"\"\n        Publish local path for obstacle avoidance\n        \"\"\"\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = self.map_frame\n\n        for x, y, theta in local_waypoints:\n            pose = PoseStamped()\n            pose.header.frame_id = self.map_frame\n            pose.pose.position.x = x\n            pose.pose.position.y = y\n            pose.pose.position.z = 0.0\n            # Convert theta to quaternion\n            qw = np.cos(theta/2)\n            qz = np.sin(theta/2)\n            pose.pose.orientation.w = qw\n            pose.pose.orientation.z = qz\n\n            path_msg.poses.append(pose)\n\n        self.local_path_pub.publish(path_msg)\n\n    def publish_map(self, occupancy_grid: OccupancyGrid):\n        \"\"\"\n        Publish occupancy grid map\n        \"\"\"\n        occupancy_grid.header.stamp = self.get_clock().now().to_msg()\n        occupancy_grid.header.frame_id = self.map_frame\n        self.map_pub.publish(occupancy_grid)\n\n    def publish_goal_marker(self, goal_x: float, goal_y: float, goal_z: float = 0.0):\n        \"\"\"\n        Publish goal marker for visualization\n        \"\"\"\n        marker = Marker()\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.header.frame_id = self.map_frame\n        marker.ns = \"goal\"\n        marker.id = 0\n        marker.type = Marker.CYLINDER\n        marker.action = Marker.ADD\n        marker.pose.position.x = goal_x\n        marker.pose.position.y = goal_y\n        marker.pose.position.z = goal_z\n        marker.pose.orientation.w = 1.0\n        marker.scale.x = 0.5  # diameter\n        marker.scale.y = 0.5  # diameter\n        marker.scale.z = 1.0  # height\n        marker.color.a = 0.8  # alpha\n        marker.color.r = 0.0\n        marker.color.g = 1.0\n        marker.color.b = 0.0\n\n        self.goal_pub.publish(marker)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    nav_integration = IsaacROSNavIntegration()\n\n    try:\n        rclpy.spin(nav_integration)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        nav_integration.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"isaac-sim-setup-parameters",children:"Isaac Sim Setup Parameters"}),"\n",(0,s.jsx)(n.p,{children:"Here's a Mermaid.js diagram showing the VSLAM architecture:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[Isaac ROS VSLAM System] --\x3e B[Camera Input]\n    A --\x3e C[IMU Input]\n    A --\x3e D[Feature Extraction]\n\n    B --\x3e E[Stereo Camera]\n    B --\x3e F[Mono Camera]\n    B --\x3e G[RGB-D Camera]\n\n    C --\x3e H[Inertial Measurement]\n    D --\x3e I[ORB Features]\n    D --\x3e J[SIFT Features]\n    D --\x3e K[AKAZE Features]\n\n    E --\x3e L[Stereo VSLAM]\n    F --\x3e M[Mono VSLAM]\n    G --\x3e N[RGB-D VSLAM]\n\n    H --\x3e O[Visual Inertial Odometry]\n    I --\x3e P[Feature Matching]\n    J --\x3e P\n    K --\x3e P\n\n    L --\x3e Q[Depth Estimation]\n    M --\x3e R[Scale Recovery]\n    N --\x3e S[Depth Integration]\n\n    O --\x3e T[State Estimation]\n    P --\x3e T\n    Q --\x3e T\n    R --\x3e T\n    S --\x3e T\n\n    T --\x3e U[Map Building]\n    T --\x3e V[Loop Closure]\n    T --\x3e W[Localization]\n\n    U --\x3e X[Point Cloud Map]\n    V --\x3e Y[Graph Optimization]\n    W --\x3e Z[Pose Estimation]\n\n    X --\x3e AA[Navigation Integration]\n    Y --\x3e AA\n    Z --\x3e AA\n\n    AA --\x3e BB[Path Planning]\n    AA --\x3e CC[Obstacle Detection]\n\n    style A fill:#e1f5fe\n    style T fill:#f3e5f5\n    style U fill:#e8f5e8\n    style V fill:#fff3e0\n    style W fill:#fce4ec\n"})}),"\n",(0,s.jsx)(n.p,{children:"Here are the key Isaac ROS VSLAM configuration parameters:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Isaac ROS VSLAM Configuration\nISAAC_ROS_VSLAM_CONFIG = {\n    # Camera Configuration\n    "camera": {\n        "resolution": [640, 480],\n        "focal_length": 320.0,\n        "principal_point": [320.0, 240.0],\n        "distortion_coefficients": [0.0, 0.0, 0.0, 0.0, 0.0],\n        "frame_rate": 30.0\n    },\n\n    # Feature Detection Parameters\n    "feature_detection": {\n        "detector_type": "orb",  # orb, sift, akaze\n        "max_features": 1000,\n        "scale_factor": 1.2,\n        "levels": 8,\n        "edge_threshold": 31,\n        "patch_size": 31,\n        "fast_threshold": 20\n    },\n\n    # Matching Parameters\n    "matching": {\n        "descriptor_matcher": "brute_force",\n        "distance_metric": "hamming",  # hamming for binary, l2 for float\n        "matching_threshold": 0.7,\n        "max_matches": 100\n    },\n\n    # Pose Estimation Parameters\n    "pose_estimation": {\n        "essential_matrix_threshold": 1.0,\n        "ransac_probability": 0.999,\n        "min_inliers": 10,\n        "max_iterations": 1000\n    },\n\n    # Loop Closure Parameters\n    "loop_closure": {\n        "vocabulary_size": 1000,\n        "loop_detection_threshold": 0.6,\n        "min_loop_matches": 20,\n        "max_database_size": 10000,\n        "loop_closure_frequency": 1.0  # Hz\n    },\n\n    # Mapping Parameters\n    "mapping": {\n        "map_resolution": 0.1,  # meters\n        "max_map_points": 100000,\n        "map_update_rate": 1.0,  # Hz\n        "occupancy_grid_resolution": 0.5  # meters\n    },\n\n    # Tracking Parameters\n    "tracking": {\n        "min_tracking_features": 50,\n        "tracking_quality_threshold": 0.5,\n        "relocalization_threshold": 0.3,\n        "max_tracking_loss": 10  # frames\n    },\n\n    # Performance Parameters\n    "performance": {\n        "max_processing_time": 0.1,  # seconds\n        "use_gpu": True,\n        "gpu_device_id": 0,\n        "thread_count": 4\n    }\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS VSLAM provides a comprehensive solution for visual localization and mapping in humanoid robots. The system combines multiple components including visual odometry, loop closure detection, and map building to create robust and accurate spatial understanding capabilities."}),"\n",(0,s.jsx)(n.p,{children:"The implementation includes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual Odometry"}),": Real-time pose estimation using feature tracking"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loop Closure"}),": Recognition of previously visited locations to correct drift"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mapping"}),": Construction and maintenance of environment representations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigation Integration"}),": Seamless integration with ROS navigation stack"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The key advantages of Isaac ROS VSLAM for humanoid robots include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time performance on NVIDIA hardware"}),"\n",(0,s.jsx)(n.li,{children:"Robust tracking in dynamic environments"}),"\n",(0,s.jsx)(n.li,{children:"Integration with other Isaac ROS components"}),"\n",(0,s.jsx)(n.li,{children:"Support for multiple sensor configurations (mono, stereo, RGB-D)"}),"\n",(0,s.jsx)(n.li,{children:"GPU acceleration for computationally intensive tasks"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The modular architecture allows for customization based on specific robot requirements and sensor configurations. The system is designed to handle the unique challenges of humanoid robot navigation, including dynamic movement, changing viewpoints, and complex indoor environments."})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453(e,n,a){a.d(n,{R:()=>r,x:()=>i});var t=a(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);