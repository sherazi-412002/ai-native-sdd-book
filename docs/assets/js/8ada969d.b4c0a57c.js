"use strict";(globalThis.webpackChunkclassic=globalThis.webpackChunkclassic||[]).push([[2157],{4612(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4/whisper-integration","title":"Whisper Integration","description":"Audio Processing with OpenAI Whisper","source":"@site/docs/module-4/whisper-integration.md","sourceDirName":"module-4","slug":"/module-4/whisper-integration","permalink":"/ai-native-sdd-book/docs/module-4/whisper-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/sherazi-412002/ai-native-sdd-book/tree/main/docs/module-4/whisper-integration.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"curriculumSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/ai-native-sdd-book/docs/category/module-4-vision-language-action-vla"},"next":{"title":"LLM Cognitive Planning","permalink":"/ai-native-sdd-book/docs/module-4/llm-cognitive-planning"}}');var s=t(4848),o=t(8453);const a={sidebar_position:1},r="Whisper Integration",c={},d=[{value:"Audio Processing with OpenAI Whisper",id:"audio-processing-with-openai-whisper",level:2},{value:"Introduction to Whisper",id:"introduction-to-whisper",level:2},{value:"Audio Preprocessing",id:"audio-preprocessing",level:2},{value:"Speech Recognition",id:"speech-recognition",level:2},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Real-time Processing",id:"real-time-processing",level:2},{value:"Summary",id:"summary",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"whisper-integration",children:"Whisper Integration"})}),"\n",(0,s.jsx)(n.h2,{id:"audio-processing-with-openai-whisper",children:"Audio Processing with OpenAI Whisper"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covers integrating OpenAI Whisper for audio processing in humanoid robots. OpenAI Whisper is a state-of-the-art speech recognition model that can transcribe, translate, and understand audio in multiple languages. For humanoid robots, Whisper enables natural language interaction, command recognition, and contextual understanding of spoken instructions."}),"\n",(0,s.jsx)(n.p,{children:"Whisper's architecture is based on a Transformer sequence-to-sequence model that maps audio spectrograms to text tokens. The model has been trained on 680,000 hours of multilingual and multitask supervised data, making it highly robust to accents, background noise, and technical language."}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-whisper",children:"Introduction to Whisper"}),"\n",(0,s.jsx)(n.p,{children:"Whisper models come in different sizes, each optimized for different performance requirements:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tiny"}),": 39M parameters, suitable for edge devices"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"base"}),": 74M parameters, good balance of speed and accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"small"}),": 244M parameters, higher accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"medium"}),": 769M parameters, high accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"large"}),": 1550M parameters, highest accuracy"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"For humanoid robots, the choice depends on computational resources and real-time requirements. The tiny and base models are often sufficient for command recognition, while larger models provide better accuracy for complex conversations."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import whisper\nimport torch\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\nimport librosa\nimport io\nfrom dataclasses import dataclass\nimport threading\nimport queue\nimport time\nfrom scipy import signal\nimport webrtcvad\n\n@dataclass\nclass AudioConfig:\n    """Configuration for audio processing"""\n    sample_rate: int = 16000\n    chunk_duration: float = 1.0  # seconds\n    vad_mode: int = 1  # WebRTC VAD mode (0-3)\n    silence_threshold: float = 0.3  # seconds of silence before stopping\n    min_speech_duration: float = 0.5  # minimum speech duration\n    max_buffer_duration: float = 10.0  # maximum buffer size in seconds\n\nclass AudioPreprocessor:\n    """Preprocess audio for Whisper with noise reduction and VAD"""\n\n    def __init__(self, config: AudioConfig):\n        self.config = config\n        self.vad = webrtcvad.Vad(config.vad_mode)\n\n        # Noise reduction parameters\n        self.noise_floor = 0.01\n        self.snr_threshold = 10.0  # Signal-to-noise ratio threshold\n\n        # Audio buffer for continuous processing\n        self.audio_buffer = np.array([])\n        self.max_buffer_size = int(config.max_buffer_duration * config.sample_rate)\n\n        # Threading for real-time processing\n        self.processing_lock = threading.Lock()\n\n    def preprocess_audio(self, audio_data: np.ndarray) -> np.ndarray:\n        """Preprocess audio data for Whisper"""\n        # Normalize audio\n        audio_data = self._normalize_audio(audio_data)\n\n        # Apply noise reduction\n        audio_data = self._reduce_noise(audio_data)\n\n        # Apply high-pass filter to remove low-frequency noise\n        audio_data = self._high_pass_filter(audio_data)\n\n        return audio_data\n\n    def _normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:\n        """Normalize audio to optimal range for Whisper"""\n        # Normalize to [-1, 1]\n        max_val = np.max(np.abs(audio_data))\n        if max_val > 0:\n            audio_data = audio_data / max_val\n        return audio_data\n\n    def _reduce_noise(self, audio_data: np.ndarray) -> np.ndarray:\n        """Apply basic noise reduction"""\n        # Simple spectral subtraction for noise reduction\n        # Calculate power spectrum\n        fft = np.fft.fft(audio_data)\n        power_spectrum = np.abs(fft) ** 2\n\n        # Estimate noise floor (assuming first 10% is noise)\n        noise_floor_idx = int(len(power_spectrum) * 0.1)\n        if noise_floor_idx > 0:\n            noise_floor = np.mean(power_spectrum[:noise_floor_idx])\n            noise_floor = max(self.noise_floor, noise_floor)\n\n            # Subtract noise from spectrum\n            enhanced_spectrum = np.maximum(power_spectrum - noise_floor, 0)\n\n            # Apply Wiener filtering\n            wiener_gain = enhanced_spectrum / (enhanced_spectrum + noise_floor)\n\n            # Apply gain to original spectrum\n            enhanced_fft = fft * np.sqrt(wiener_gain)\n\n            # Inverse FFT\n            audio_data = np.real(np.fft.ifft(enhanced_fft))\n\n        return audio_data\n\n    def _high_pass_filter(self, audio_data: np.ndarray) -> np.ndarray:\n        """Apply high-pass filter to remove low-frequency noise"""\n        # Design high-pass filter\n        nyquist = self.config.sample_rate / 2\n        cutoff = 100.0  # Hz\n        normalized_cutoff = cutoff / nyquist\n\n        # Create Butterworth filter\n        b, a = signal.butter(4, normalized_cutoff, btype=\'high\', analog=False)\n\n        # Apply filter\n        filtered_audio = signal.filtfilt(b, a, audio_data)\n\n        return filtered_audio\n\n    def detect_voice_activity(self, audio_chunk: np.ndarray) -> bool:\n        """Detect voice activity in audio chunk using WebRTC VAD"""\n        # Convert to 16-bit PCM\n        audio_int16 = (audio_chunk * 32767).astype(np.int16)\n\n        # Ensure proper length (10, 20, or 30 ms frames)\n        frame_duration = 20  # ms\n        frame_size = int(self.config.sample_rate * frame_duration / 1000)\n\n        if len(audio_int16) < frame_size:\n            # Pad with zeros if too short\n            padding = frame_size - len(audio_int16)\n            audio_int16 = np.pad(audio_int16, (0, padding), mode=\'constant\')\n\n        # Process in frames\n        frames = []\n        for i in range(0, len(audio_int16), frame_size):\n            frame = audio_int16[i:i+frame_size]\n            if len(frame) == frame_size:\n                frames.append(self.vad.is_speech(frame.tobytes(), self.config.sample_rate))\n\n        # Voice activity if majority of frames have speech\n        if frames:\n            vad_ratio = sum(frames) / len(frames)\n            return vad_ratio > 0.3  # At least 30% of frames should have speech\n        return False\n\n    def add_audio_chunk(self, audio_chunk: np.ndarray):\n        """Add audio chunk to internal buffer"""\n        with self.processing_lock:\n            self.audio_buffer = np.concatenate([self.audio_buffer, audio_chunk])\n\n            # Limit buffer size\n            max_samples = self.max_buffer_size\n            if len(self.audio_buffer) > max_samples:\n                self.audio_buffer = self.audio_buffer[-max_samples:]\n\n    def get_audio_segment(self, duration: float) -> Optional[np.ndarray]:\n        """Get audio segment of specified duration from buffer"""\n        with self.processing_lock:\n            required_samples = int(duration * self.config.sample_rate)\n            if len(self.audio_buffer) >= required_samples:\n                segment = self.audio_buffer[-required_samples:]\n                return segment\n            return None\n\nclass WhisperProcessor:\n    """Process audio using Whisper model with real-time capabilities"""\n\n    def __init__(self, model_size: str = "base", device: str = "cuda"):\n        self.model_size = model_size\n        self.device = device\n\n        # Load Whisper model\n        self.model = whisper.load_model(model_size, device=device)\n\n        # Audio configuration\n        self.config = AudioConfig()\n\n        # Preprocessor\n        self.preprocessor = AudioPreprocessor(self.config)\n\n        # Threading for continuous processing\n        self.processing_thread = None\n        self.processing_queue = queue.Queue()\n        self.running = False\n\n        # Results queue for processed transcriptions\n        self.results_queue = queue.Queue()\n\n    def transcribe_audio(self, audio_data: np.ndarray, language: str = "en") -> Dict:\n        """Transcribe audio using Whisper"""\n        # Preprocess audio\n        processed_audio = self.preprocessor.preprocess_audio(audio_data)\n\n        # Convert to float32 if needed\n        if processed_audio.dtype != np.float32:\n            processed_audio = processed_audio.astype(np.float32)\n\n        # Transcribe using Whisper\n        result = self.model.transcribe(\n            processed_audio,\n            language=language,\n            fp16=(self.device == "cuda")\n        )\n\n        return result\n\n    def transcribe_with_options(self, audio_data: np.ndarray,\n                              language: str = "en",\n                              task: str = "transcribe",\n                              temperature: float = 0.0,\n                              best_of: int = 5,\n                              beam_size: int = 5) -> Dict:\n        """Transcribe audio with advanced options"""\n        # Preprocess audio\n        processed_audio = self.preprocessor.preprocess_audio(audio_data)\n\n        # Convert to float32 if needed\n        if processed_audio.dtype != np.float32:\n            processed_audio = processed_audio.astype(np.float32)\n\n        # Create options\n        options = whisper.DecodingOptions(\n            language=language,\n            task=task,\n            temperature=temperature,\n            best_of=best_of,\n            beam_size=beam_size,\n            fp16=(self.device == "cuda")\n        )\n\n        # Decode\n        result = self.model.decode(processed_audio, options)\n\n        return {\n            "text": result.text,\n            "segments": result.tokens,\n            "language": result.language\n        }\n\n    def start_continuous_processing(self):\n        """Start continuous audio processing in background thread"""\n        self.running = True\n        self.processing_thread = threading.Thread(target=self._continuous_processing_loop)\n        self.processing_thread.start()\n\n    def stop_continuous_processing(self):\n        """Stop continuous audio processing"""\n        self.running = False\n        if self.processing_thread:\n            self.processing_thread.join()\n\n    def _continuous_processing_loop(self):\n        """Continuous processing loop for real-time transcription"""\n        while self.running:\n            try:\n                # Get audio from queue\n                audio_chunk = self.processing_queue.get(timeout=1.0)\n\n                # Add to buffer\n                self.preprocessor.add_audio_chunk(audio_chunk)\n\n                # Check for voice activity\n                vad_active = self.preprocessor.detect_voice_activity(audio_chunk)\n\n                if vad_active:\n                    # Get audio segment for processing\n                    segment = self.preprocessor.get_audio_segment(duration=5.0)  # 5 seconds\n                    if segment is not None:\n                        # Transcribe the segment\n                        result = self.transcribe_audio(segment)\n\n                        # Add result to results queue\n                        self.results_queue.put(result)\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                print(f"Error in continuous processing: {e}")\n                continue\n\nclass WhisperROSInterface:\n    """ROS 2 interface for Whisper integration"""\n\n    def __init__(self):\n        import rclpy\n        from rclpy.node import Node\n        from std_msgs.msg import String\n        from sensor_msgs.msg import AudioData\n        from audio_common_msgs.msg import AudioData as AudioDataMsg\n\n        # Initialize ROS node\n        self.node = Node(\'whisper_audio_processor\')\n\n        # Parameters\n        self.node.declare_parameter(\'model_size\', \'base\')\n        self.node.declare_parameter(\'device\', \'cuda\')\n        self.node.declare_parameter(\'sample_rate\', 16000)\n\n        self.model_size = self.node.get_parameter(\'model_size\').value\n        self.device = self.node.get_parameter(\'device\').value\n        self.sample_rate = self.node.get_parameter(\'sample_rate\').value\n\n        # Initialize Whisper processor\n        self.whisper_processor = WhisperProcessor(self.model_size, self.device)\n\n        # Publishers and subscribers\n        self.audio_sub = self.node.create_subscription(\n            AudioDataMsg, \'audio_input\', self.audio_callback, 10)\n        self.transcription_pub = self.node.create_publisher(\n            String, \'transcription\', 10)\n\n        # Start continuous processing\n        self.whisper_processor.start_continuous_processing()\n\n        self.node.get_logger().info(\'Whisper ROS interface initialized\')\n\n    def audio_callback(self, msg: AudioDataMsg):\n        """Handle incoming audio data"""\n        # Convert audio data to numpy array\n        audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n        # Add to processing queue\n        self.whisper_processor.processing_queue.put(audio_data)\n\n        # Check for results\n        try:\n            while True:\n                result = self.whisper_processor.results_queue.get_nowait()\n\n                # Publish transcription\n                transcription_msg = String()\n                transcription_msg.data = result[\'text\']\n                self.transcription_pub.publish(transcription_msg)\n\n        except queue.Empty:\n            pass  # No results available yet\n\n    def shutdown(self):\n        """Shutdown the ROS interface"""\n        self.whisper_processor.stop_continuous_processing()\n\ndef main():\n    """Main function for standalone operation"""\n    import rclpy\n\n    rclpy.init()\n\n    # Create Whisper ROS interface\n    whisper_interface = WhisperROSInterface()\n\n    try:\n        rclpy.spin(whisper_interface.node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        whisper_interface.shutdown()\n        whisper_interface.node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"audio-preprocessing",children:"Audio Preprocessing"}),"\n",(0,s.jsx)(n.p,{children:"Effective audio preprocessing is crucial for optimal Whisper performance, especially in robotic environments where background noise, reverberation, and varying acoustic conditions can significantly impact recognition accuracy."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy import signal\nfrom scipy.fft import fft, ifft\nimport librosa\nfrom typing import Tuple, Optional\nimport webrtcvad\nimport pyaudio\nimport threading\n\nclass AdvancedAudioPreprocessor:\n    """Advanced audio preprocessing for Whisper with multiple noise reduction techniques"""\n\n    def __init__(self, sample_rate: int = 16000, frame_duration: float = 0.03):\n        self.sample_rate = sample_rate\n        self.frame_duration = frame_duration\n        self.frame_size = int(sample_rate * frame_duration)\n\n        # WebRTC VAD for voice activity detection\n        self.vad = webrtcvad.Vad(2)  # Aggressive mode\n\n        # Spectral subtraction parameters\n        self.noise_floor = 0.01\n        self.alpha = 0.9  # Noise estimation smoothing factor\n        self.beta = 0.05  # Over-subtraction factor\n        self.smoothing_factor = 0.8\n\n        # Noise estimate (will be updated during processing)\n        self.noise_estimate = None\n        self.initialized = False\n\n        # Audio buffer for continuous processing\n        self.audio_buffer = np.array([])\n\n    def preprocess_audio_batch(self, audio_data: np.ndarray) -> np.ndarray:\n        """Preprocess audio in batch mode"""\n        # Resample if needed\n        if self.sample_rate != 16000:\n            audio_data = librosa.resample(audio_data, orig_sr=self.sample_rate, target_sr=16000)\n            self.sample_rate = 16000\n\n        # Normalize audio\n        audio_data = self._normalize_audio(audio_data)\n\n        # Apply noise reduction\n        audio_data = self._spectral_subtraction(audio_data)\n\n        # Apply Wiener filtering\n        audio_data = self._wiener_filter(audio_data)\n\n        # Apply high-pass filter\n        audio_data = self._high_pass_filter(audio_data)\n\n        return audio_data\n\n    def preprocess_audio_stream(self, audio_chunk: np.ndarray) -> Tuple[np.ndarray, bool]:\n        """Preprocess audio chunk in streaming mode with VAD"""\n        # Normalize\n        audio_chunk = self._normalize_audio(audio_chunk)\n\n        # Apply noise reduction\n        audio_chunk = self._spectral_subtraction_stream(audio_chunk)\n\n        # Apply high-pass filter\n        audio_chunk = self._high_pass_filter(audio_chunk)\n\n        # Detect voice activity\n        vad_active = self._detect_voice_activity(audio_chunk)\n\n        return audio_chunk, vad_active\n\n    def _normalize_audio(self, audio_data: np.ndarray) -> np.ndarray:\n        """Normalize audio to optimal range"""\n        # Peak normalization\n        max_val = np.max(np.abs(audio_data))\n        if max_val > 0:\n            audio_data = audio_data / max_val\n            # Scale to optimal range for Whisper (-0.8 to 0.8)\n            audio_data = audio_data * 0.8\n\n        return audio_data\n\n    def _spectral_subtraction(self, audio_data: np.ndarray) -> np.ndarray:\n        """Apply spectral subtraction noise reduction"""\n        # Convert to frequency domain\n        fft_data = fft(audio_data)\n        power_spectrum = np.abs(fft_data) ** 2\n\n        # Estimate noise floor using minimum statistics\n        if self.noise_estimate is None:\n            self.noise_estimate = power_spectrum.copy()\n\n        # Update noise estimate\n        self.noise_estimate = self.alpha * self.noise_estimate + (1 - self.alpha) * power_spectrum\n\n        # Apply spectral subtraction\n        enhanced_spectrum = np.maximum(power_spectrum - self.beta * self.noise_estimate, 0)\n\n        # Reconstruct signal\n        magnitude = np.sqrt(enhanced_spectrum)\n        enhanced_fft = magnitude * np.exp(1j * np.angle(fft_data))\n\n        # Convert back to time domain\n        enhanced_audio = np.real(ifft(enhanced_fft))\n\n        return enhanced_audio.astype(np.float32)\n\n    def _spectral_subtraction_stream(self, audio_chunk: np.ndarray) -> np.ndarray:\n        """Apply spectral subtraction for streaming audio"""\n        # Convert to frequency domain\n        fft_chunk = fft(audio_chunk)\n        power_spectrum = np.abs(fft_chunk) ** 2\n\n        if self.noise_estimate is None:\n            # Initialize noise estimate with first chunk\n            self.noise_estimate = power_spectrum.copy()\n            self.initialized = True\n\n        # Update noise estimate (only during silence periods for streaming)\n        if not self.initialized:\n            self.noise_estimate = self.alpha * self.noise_estimate + (1 - self.alpha) * power_spectrum\n        else:\n            # Use more conservative update during speech\n            self.noise_estimate = 0.99 * self.noise_estimate + 0.01 * power_spectrum\n\n        # Apply spectral subtraction\n        enhanced_spectrum = np.maximum(power_spectrum - self.beta * self.noise_estimate, 0)\n\n        # Reconstruct signal\n        magnitude = np.sqrt(enhanced_spectrum)\n        enhanced_fft = magnitude * np.exp(1j * np.angle(fft_chunk))\n\n        # Convert back to time domain\n        enhanced_audio = np.real(ifft(enhanced_fft))\n\n        return enhanced_audio.astype(np.float32)\n\n    def _wiener_filter(self, audio_data: np.ndarray) -> np.ndarray:\n        """Apply Wiener filtering for noise reduction"""\n        # Compute STFT\n        f, t, Zxx = signal.stft(audio_data, fs=self.sample_rate, nperseg=512)\n\n        # Estimate noise PSD (assuming first portion is noise)\n        noise_frames = min(10, Zxx.shape[1])\n        noise_psd = np.mean(np.abs(Zxx[:, :noise_frames]) ** 2, axis=1, keepdims=True)\n\n        # Compute Wiener gain\n        signal_psd = np.maximum(np.abs(Zxx) ** 2 - noise_psd, 0)\n        wiener_gain = signal_psd / (signal_psd + noise_psd)\n\n        # Apply gain\n        enhanced_Zxx = Zxx * wiener_gain\n\n        # Inverse STFT\n        _, enhanced_audio = signal.istft(enhanced_Zxx, fs=self.sample_rate, nperseg=512)\n\n        return enhanced_audio.astype(np.float32)\n\n    def _high_pass_filter(self, audio_data: np.ndarray) -> np.ndarray:\n        """Apply high-pass filter to remove low-frequency noise"""\n        # Design high-pass filter\n        nyquist = self.sample_rate / 2\n        cutoff = 80.0  # Hz (remove very low frequencies that don\'t contain speech)\n        normalized_cutoff = cutoff / nyquist\n\n        # Create Butterworth filter\n        b, a = signal.butter(4, normalized_cutoff, btype=\'high\', analog=False)\n\n        # Apply filter\n        filtered_audio = signal.filtfilt(b, a, audio_data)\n\n        return filtered_audio\n\n    def _detect_voice_activity(self, audio_chunk: np.ndarray) -> bool:\n        """Detect voice activity using WebRTC VAD"""\n        # Convert to 16-bit PCM\n        audio_int16 = (audio_chunk * 32767).astype(np.int16)\n\n        # Ensure proper frame size (10, 20, or 30 ms)\n        frame_size = int(self.sample_rate * self.frame_duration)\n        if len(audio_int16) < frame_size:\n            # Pad with zeros\n            padding = frame_size - len(audio_int16)\n            audio_int16 = np.pad(audio_int16, (0, padding), mode=\'constant\')\n\n        # Process in frames\n        frames = []\n        for i in range(0, len(audio_int16), frame_size):\n            frame = audio_int16[i:i+frame_size]\n            if len(frame) == frame_size:\n                try:\n                    is_speech = self.vad.is_speech(frame.tobytes(), self.sample_rate)\n                    frames.append(is_speech)\n                except:\n                    # Skip problematic frames\n                    continue\n\n        # Voice activity if majority of frames have speech\n        if frames:\n            vad_ratio = sum(frames) / len(frames)\n            return vad_ratio > 0.2  # At least 20% of frames should have speech\n        return False\n\n    def adaptive_noise_estimation(self, audio_data: np.ndarray, vad_active: bool):\n        """Adaptively estimate noise based on VAD results"""\n        if not vad_active and len(audio_data) > 0:\n            # This is likely a noise segment, update noise estimate\n            fft_data = fft(audio_data)\n            power_spectrum = np.abs(fft_data) ** 2\n\n            if self.noise_estimate is None:\n                self.noise_estimate = power_spectrum.copy()\n            else:\n                # Update noise estimate with slower adaptation\n                self.noise_estimate = 0.95 * self.noise_estimate + 0.05 * power_spectrum\n\nclass RealTimeAudioProcessor:\n    """Real-time audio processor for continuous Whisper integration"""\n\n    def __init__(self, sample_rate: int = 16000, chunk_duration: float = 0.5):\n        self.sample_rate = sample_rate\n        self.chunk_duration = chunk_duration\n        self.chunk_size = int(sample_rate * chunk_duration)\n\n        # Preprocessor\n        self.preprocessor = AdvancedAudioPreprocessor(sample_rate)\n\n        # Audio buffer\n        self.audio_buffer = np.array([])\n        self.max_buffer_duration = 10.0  # 10 seconds max\n        self.max_buffer_size = int(sample_rate * self.max_buffer_duration)\n\n        # Voice activity detection\n        self.vad_active = False\n        self.speech_start_time = None\n        self.silence_start_time = None\n\n        # Threading\n        self.processing_lock = threading.Lock()\n        self.audio_queue = queue.Queue()\n\n    def process_audio_chunk(self, audio_chunk: np.ndarray) -> Optional[np.ndarray]:\n        """Process incoming audio chunk and return speech segments for transcription"""\n        with self.processing_lock:\n            # Preprocess chunk\n            processed_chunk, vad_active = self.preprocessor.preprocess_audio_stream(audio_chunk)\n\n            # Add to buffer\n            self.audio_buffer = np.concatenate([self.audio_buffer, processed_chunk])\n\n            # Limit buffer size\n            if len(self.audio_buffer) > self.max_buffer_size:\n                self.audio_buffer = self.audio_buffer[-self.max_buffer_size:]\n\n            # Update VAD state\n            current_time = time.time()\n            if vad_active:\n                # Speech detected\n                if not self.vad_active:\n                    # Just started speaking\n                    self.speech_start_time = current_time\n                    self.silence_start_time = None\n\n                self.vad_active = True\n            else:\n                # Silence detected\n                if self.vad_active:\n                    # Just stopped speaking\n                    self.silence_start_time = current_time\n\n                self.vad_active = False\n\n            # Update noise estimate\n            self.preprocessor.adaptive_noise_estimation(processed_chunk, vad_active)\n\n            # Check if we have a complete speech segment to return\n            if (not self.vad_active and\n                self.silence_start_time and\n                (current_time - self.silence_start_time) > 0.5 and  # 0.5s silence threshold\n                self.speech_start_time):\n\n                # Extract speech segment\n                speech_duration = self.silence_start_time - self.speech_start_time\n                required_samples = int(speech_duration * self.sample_rate)\n\n                if len(self.audio_buffer) >= required_samples:\n                    speech_segment = self.audio_buffer[-required_samples:]\n\n                    # Clear buffer after extracting speech\n                    self.audio_buffer = np.array([])\n                    self.speech_start_time = None\n                    self.silence_start_time = None\n\n                    return speech_segment\n\n            return None\n\n# Example usage of audio preprocessing\ndef example_audio_preprocessing():\n    """Example of using the audio preprocessing pipeline"""\n    # Create preprocessor\n    preprocessor = AdvancedAudioPreprocessor()\n\n    # Load example audio file\n    # audio, sr = librosa.load("example_audio.wav", sr=16000)\n\n    # For this example, create synthetic audio with noise\n    duration = 5.0  # seconds\n    t = np.linspace(0, duration, int(16000 * duration))\n    # Create a simple sine wave with noise\n    clean_audio = 0.5 * np.sin(2 * np.pi * 440 * t)  # 440 Hz tone\n    noise = np.random.normal(0, 0.1, len(clean_audio))\n    noisy_audio = clean_audio + noise\n\n    # Preprocess the audio\n    processed_audio = preprocessor.preprocess_audio_batch(noisy_audio)\n\n    print(f"Original audio shape: {noisy_audio.shape}")\n    print(f"Processed audio shape: {processed_audio.shape}")\n\n    return processed_audio\n\ndef example_real_time_processing():\n    """Example of real-time audio processing"""\n    processor = RealTimeAudioProcessor()\n\n    # Simulate processing of audio chunks\n    chunk_duration = 0.1  # 100ms chunks\n    chunk_size = int(16000 * chunk_duration)\n\n    for i in range(100):  # Simulate 10 seconds of audio\n        # Generate synthetic audio chunk\n        t = np.linspace(i * chunk_duration, (i + 1) * chunk_duration, chunk_size)\n        chunk = 0.3 * np.sin(2 * np.pi * 880 * t)  # 880 Hz tone\n        chunk += np.random.normal(0, 0.05, chunk_size)  # Add noise\n\n        # Process chunk\n        speech_segment = processor.process_audio_chunk(chunk)\n\n        if speech_segment is not None:\n            print(f"Detected speech segment of {len(speech_segment) / 16000:.2f} seconds")\n            # Here you would send the segment to Whisper for transcription\n'})}),"\n",(0,s.jsx)(n.h2,{id:"speech-recognition",children:"Speech Recognition"}),"\n",(0,s.jsx)(n.p,{children:"Speech recognition with Whisper involves not just transcription but also language identification, translation, and understanding of context. For humanoid robots, this extends to command interpretation and intent recognition."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import whisper\nimport torch\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple, Any\nimport json\nimport asyncio\nfrom dataclasses import dataclass\nimport re\nfrom transformers import pipeline\nimport spacy\n\n@dataclass\nclass RecognitionResult:\n    \"\"\"Result from speech recognition\"\"\"\n    text: str\n    language: str\n    confidence: float\n    segments: List[Dict]\n    timestamp: float\n    command_intent: Optional[str] = None\n    entities: Optional[List[Dict]] = None\n\nclass CommandInterpreter:\n    \"\"\"Interpret recognized speech as commands for humanoid robots\"\"\"\n\n    def __init__(self):\n        # Load spaCy model for NLP\n        try:\n            self.nlp = spacy.load(\"en_core_web_sm\")\n        except OSError:\n            print(\"spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n            self.nlp = None\n\n        # Define command patterns\n        self.command_patterns = {\n            'move': [\n                r'go to (.+)',\n                r'move to (.+)',\n                r'walk to (.+)',\n                r'go (.+)',\n                r'move (.+)',\n                r'walk (.+)'\n            ],\n            'grasp': [\n                r'pick up (.+)',\n                r'grab (.+)',\n                r'get (.+)',\n                r'pick (.+)',\n                r'take (.+)'\n            ],\n            'navigation': [\n                r'find (.+)',\n                r'locate (.+)',\n                r'look for (.+)',\n                r'search for (.+)'\n            ],\n            'interaction': [\n                r'talk to (.+)',\n                r'speak to (.+)',\n                r'hello (.+)',\n                r'hi (.+)'\n            ],\n            'action': [\n                r'wave',\n                r'nod',\n                r'shake',\n                r'dance',\n                r'sit',\n                r'stand',\n                r'jump'\n            ]\n        }\n\n        # Location keywords\n        self.location_keywords = {\n            'kitchen', 'living room', 'bedroom', 'bathroom', 'office',\n            'dining room', 'hallway', 'garage', 'garden', 'entrance'\n        }\n\n        # Object keywords\n        self.object_keywords = {\n            'cup', 'bottle', 'book', 'phone', 'keys', 'wallet',\n            'apple', 'banana', 'water', 'milk', 'bread', 'chair'\n        }\n\n    def extract_command_intent(self, text: str) -> Tuple[Optional[str], List[Dict]]:\n        \"\"\"Extract command intent and entities from text\"\"\"\n        if self.nlp:\n            doc = self.nlp(text.lower())\n        else:\n            doc = None\n\n        # Find command type\n        command_type = None\n        entities = []\n\n        for cmd_type, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text.lower())\n                if match:\n                    command_type = cmd_type\n                    # Extract the argument\n                    arg = match.group(1).strip()\n                    entities.append({\n                        'type': 'argument',\n                        'value': arg,\n                        'start': match.start(1),\n                        'end': match.end(1)\n                    })\n                    break\n            if command_type:\n                break\n\n        # If no pattern matched, try NLP approach\n        if not command_type and doc:\n            # Look for verbs that indicate actions\n            for token in doc:\n                if token.pos_ == 'VERB':\n                    # Check if this verb is in our command patterns\n                    for cmd_type, patterns in self.command_patterns.items():\n                        for pattern in patterns:\n                            if token.lemma_ in pattern:\n                                command_type = cmd_type\n                                break\n                        if command_type:\n                            break\n                    if command_type:\n                        break\n\n        # Extract additional entities using NLP\n        if doc:\n            for ent in doc.ents:\n                if ent.label_ in ['PERSON', 'GPE', 'ORG', 'MONEY', 'TIME', 'DATE']:\n                    entities.append({\n                        'type': ent.label_,\n                        'value': ent.text,\n                        'start': ent.start_char,\n                        'end': ent.end_char\n                    })\n\n            # Extract location and object entities\n            for token in doc:\n                if token.text in self.location_keywords:\n                    entities.append({\n                        'type': 'location',\n                        'value': token.text,\n                        'start': token.idx,\n                        'end': token.idx + len(token.text)\n                    })\n                elif token.text in self.object_keywords:\n                    entities.append({\n                        'type': 'object',\n                        'value': token.text,\n                        'start': token.idx,\n                        'end': token.idx + len(token.text)\n                    })\n\n        return command_type, entities\n\nclass WhisperSpeechRecognizer:\n    \"\"\"Advanced speech recognition using Whisper with command interpretation\"\"\"\n\n    def __init__(self, model_size: str = \"base\", device: str = \"cuda\"):\n        self.model_size = model_size\n        self.device = device\n\n        # Load Whisper model\n        self.model = whisper.load_model(model_size, device=device)\n\n        # Command interpreter\n        self.command_interpreter = CommandInterpreter()\n\n        # Audio preprocessing\n        self.preprocessor = AdvancedAudioPreprocessor()\n\n        # Language identification (if needed)\n        self.language_identification = pipeline(\n            \"language-detection\",\n            model=\"papluca/xlm-roberta-base-language-detection\"\n        ) if model_size != \"large\" else None\n\n    def recognize_speech(self, audio_data: np.ndarray,\n                        language: Optional[str] = None,\n                        temperature: float = 0.0,\n                        best_of: int = 5,\n                        beam_size: int = 5) -> RecognitionResult:\n        \"\"\"Recognize speech and interpret as command\"\"\"\n        # Preprocess audio\n        processed_audio = self.preprocessor.preprocess_audio_batch(audio_data)\n\n        # Convert to float32\n        if processed_audio.dtype != np.float32:\n            processed_audio = processed_audio.astype(np.float32)\n\n        # Transcribe with Whisper\n        if language:\n            result = self.model.transcribe(\n                processed_audio,\n                language=language,\n                temperature=temperature,\n                best_of=best_of,\n                beam_size=beam_size,\n                fp16=(self.device == \"cuda\")\n            )\n        else:\n            # Let Whisper detect language automatically\n            result = self.model.transcribe(\n                processed_audio,\n                temperature=temperature,\n                best_of=best_of,\n                beam_size=beam_size,\n                fp16=(self.device == \"cuda\")\n            )\n\n        # Extract text and language\n        text = result.get('text', '').strip()\n        detected_language = result.get('language', 'unknown')\n\n        # Calculate confidence (simplified approach)\n        confidence = self._estimate_confidence(result, text)\n\n        # Extract segments\n        segments = result.get('segments', [])\n\n        # Interpret command\n        command_intent, entities = self.command_interpreter.extract_command_intent(text)\n\n        # Create result\n        recognition_result = RecognitionResult(\n            text=text,\n            language=detected_language,\n            confidence=confidence,\n            segments=segments,\n            timestamp=time.time(),\n            command_intent=command_intent,\n            entities=entities\n        )\n\n        return recognition_result\n\n    def recognize_batch(self, audio_segments: List[np.ndarray]) -> List[RecognitionResult]:\n        \"\"\"Recognize multiple audio segments in batch\"\"\"\n        results = []\n\n        for audio_segment in audio_segments:\n            result = self.recognize_speech(audio_segment)\n            results.append(result)\n\n        return results\n\n    def _estimate_confidence(self, whisper_result: Dict, text: str) -> float:\n        \"\"\"Estimate confidence of transcription\"\"\"\n        if not text.strip():\n            return 0.0\n\n        # Simple confidence estimation based on various factors\n        confidence = 1.0\n\n        # Length-based confidence (very short transcriptions might be unreliable)\n        if len(text.strip()) < 3:\n            confidence *= 0.5\n        elif len(text.strip()) > 100:\n            confidence *= 0.9  # Longer texts might have more errors\n\n        # Check for common error patterns\n        if any(word in text.lower() for word in ['you know', 'um', 'uh', 'like']):\n            confidence *= 0.8  # Filler words might indicate uncertainty\n\n        # Check for repeated words (might indicate decoding errors)\n        words = text.lower().split()\n        if len(words) > 1 and len(set(words)) / len(words) < 0.7:  # Too many repetitions\n            confidence *= 0.6\n\n        return max(0.0, min(1.0, confidence))\n\n    def translate_speech(self, audio_data: np.ndarray, target_language: str = \"en\") -> RecognitionResult:\n        \"\"\"Translate speech to target language\"\"\"\n        # Preprocess audio\n        processed_audio = self.preprocessor.preprocess_audio_batch(audio_data)\n\n        # Convert to float32\n        if processed_audio.dtype != np.float32:\n            processed_audio = processed_audio.astype(np.float32)\n\n        # Translate with Whisper\n        result = self.model.transcribe(\n            processed_audio,\n            task=\"translate\",\n            language=\"auto\",  # Detect source language automatically\n            temperature=0.0,\n            best_of=5,\n            beam_size=5,\n            fp16=(self.device == \"cuda\")\n        )\n\n        # Interpret the translated result\n        text = result.get('text', '').strip()\n        source_language = result.get('language', 'unknown')\n\n        # Calculate confidence\n        confidence = self._estimate_confidence(result, text)\n\n        # Extract segments\n        segments = result.get('segments', [])\n\n        # Interpret command\n        command_intent, entities = self.command_interpreter.extract_command_intent(text)\n\n        # Create result\n        recognition_result = RecognitionResult(\n            text=text,\n            language=target_language,  # Target language\n            confidence=confidence,\n            segments=segments,\n            timestamp=time.time(),\n            command_intent=command_intent,\n            entities=entities\n        )\n\n        return recognition_result\n\n    def detect_language(self, audio_data: np.ndarray) -> str:\n        \"\"\"Detect language of audio (alternative method)\"\"\"\n        if self.language_identification:\n            # Preprocess and convert to text first\n            processed_audio = self.preprocessor.preprocess_audio_batch(audio_data)\n            # This is a simplified approach - in practice, you'd use audio-based language detection\n            pass\n\n        # For now, return result from Whisper\n        result = self.model.transcribe(processed_audio, fp16=(self.device == \"cuda\"))\n        return result.get('language', 'unknown')\n\nclass ContextAwareRecognizer:\n    \"\"\"Context-aware speech recognition that considers conversation history\"\"\"\n\n    def __init__(self, whisper_recognizer: WhisperSpeechRecognizer):\n        self.whisper_recognizer = whisper_recognizer\n        self.conversation_history = []\n        self.context_window = 5  # Keep last 5 interactions\n\n    def recognize_with_context(self, audio_data: np.ndarray,\n                             current_context: Optional[Dict] = None) -> RecognitionResult:\n        \"\"\"Recognize speech with context awareness\"\"\"\n        # Perform initial recognition\n        result = self.whisper_recognizer.recognize_speech(audio_data)\n\n        # Apply context-aware corrections if needed\n        if current_context:\n            result = self._apply_context_corrections(result, current_context)\n\n        # Update conversation history\n        self.conversation_history.append({\n            'timestamp': result.timestamp,\n            'text': result.text,\n            'intent': result.command_intent,\n            'entities': result.entities\n        })\n\n        # Limit history size\n        if len(self.conversation_history) > self.context_window:\n            self.conversation_history = self.conversation_history[-self.context_window:]\n\n        return result\n\n    def _apply_context_corrections(self, result: RecognitionResult, context: Dict) -> RecognitionResult:\n        \"\"\"Apply context-aware corrections to recognition result\"\"\"\n        # This is a simplified implementation\n        # In a real system, you would use more sophisticated context modeling\n\n        text = result.text\n\n        # Example: If context indicates we're in a kitchen, correct common misrecognitions\n        if context.get('location') == 'kitchen':\n            # Correct common kitchen-related misrecognitions\n            corrections = {\n                'water': ['water', 'what are', 'wonder'],\n                'milk': ['milk', 'mill', 'make'],\n                'bread': ['bread', 'bred', 'read']\n            }\n\n            for correct_word, possible_mistakes in corrections.items():\n                for mistake in possible_mistakes:\n                    if mistake in text.lower() and correct_word in context.get('available_objects', []):\n                        text = re.sub(r'\\b' + mistake + r'\\b', correct_word, text, flags=re.IGNORECASE)\n\n        # Update result with corrected text\n        result.text = text\n\n        # Re-interpret command with corrected text\n        command_intent, entities = self.whisper_recognizer.command_interpreter.extract_command_intent(text)\n        result.command_intent = command_intent\n        result.entities = entities\n\n        return result\n\n    def get_conversation_context(self) -> Dict:\n        \"\"\"Get current conversation context\"\"\"\n        return {\n            'history': self.conversation_history[-3:],  # Last 3 interactions\n            'common_entities': self._extract_common_entities(),\n            'topic': self._infer_current_topic()\n        }\n\n    def _extract_common_entities(self) -> List[str]:\n        \"\"\"Extract commonly mentioned entities from conversation history\"\"\"\n        entities = []\n        for interaction in self.conversation_history:\n            if interaction.get('entities'):\n                for entity in interaction['entities']:\n                    entities.append(entity['value'])\n\n        # Return most common entities\n        from collections import Counter\n        entity_counts = Counter(entities)\n        return [entity for entity, count in entity_counts.most_common(5)]\n\n    def _infer_current_topic(self) -> str:\n        \"\"\"Infer current conversation topic\"\"\"\n        if not self.conversation_history:\n            return \"general\"\n\n        # Simple topic inference based on recent entities and intents\n        recent_intents = [h.get('intent', 'unknown') for h in self.conversation_history[-3:]]\n        intent_counts = Counter(recent_intents)\n\n        # Return most common intent as topic\n        most_common_intent = intent_counts.most_common(1)\n        if most_common_intent:\n            return most_common_intent[0][0]\n\n        return \"general\"\n\n# Example usage of speech recognition\ndef example_speech_recognition():\n    \"\"\"Example of using the speech recognition system\"\"\"\n    # Initialize recognizer\n    recognizer = WhisperSpeechRecognizer(model_size=\"base\")\n\n    # For this example, we'll create synthetic audio\n    # In practice, you would load actual audio data\n    duration = 3.0  # seconds\n    sample_rate = 16000\n    t = np.linspace(0, duration, int(sample_rate * duration))\n\n    # Create synthetic \"speech\" (in practice, this would be actual recorded audio)\n    synthetic_audio = 0.3 * np.sin(2 * np.pi * 440 * t)  # Base tone\n    synthetic_audio += 0.2 * np.sin(2 * np.pi * 660 * t)  # Harmonic\n    synthetic_audio += np.random.normal(0, 0.05, len(synthetic_audio))  # Noise\n\n    # Recognize speech\n    result = recognizer.recognize_speech(synthetic_audio)\n\n    print(f\"Recognized text: {result.text}\")\n    print(f\"Language: {result.language}\")\n    print(f\"Confidence: {result.confidence:.2f}\")\n    print(f\"Command intent: {result.command_intent}\")\n    print(f\"Entities: {result.entities}\")\n\n    return result\n\ndef example_context_aware_recognition():\n    \"\"\"Example of context-aware speech recognition\"\"\"\n    # Initialize recognizer\n    whisper_rec = WhisperSpeechRecognizer(model_size=\"base\")\n    context_rec = ContextAwareRecognizer(whisper_rec)\n\n    # Simulate a conversation in a kitchen context\n    context = {\n        'location': 'kitchen',\n        'available_objects': ['water', 'milk', 'bread', 'apple', 'banana']\n    }\n\n    # For this example, create synthetic audio\n    duration = 2.0\n    sample_rate = 16000\n    t = np.linspace(0, duration, int(sample_rate * duration))\n    synthetic_audio = 0.3 * np.sin(2 * np.pi * 523 * t)  # C note\n    synthetic_audio += np.random.normal(0, 0.05, len(synthetic_audio))\n\n    # Recognize with context\n    result = context_rec.recognize_with_context(synthetic_audio, context)\n\n    print(f\"Context-aware recognition result:\")\n    print(f\"Text: {result.text}\")\n    print(f\"Intent: {result.command_intent}\")\n    print(f\"Context: {context_rec.get_conversation_context()}\")\n\n    return result\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,s.jsx)(n.p,{children:"Integrating Whisper with ROS 2 enables humanoid robots to process audio in real-time and respond to voice commands. This integration involves creating ROS 2 nodes for audio capture, preprocessing, transcription, and command execution."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, DurabilityPolicy\nfrom std_msgs.msg import String, Bool, Float32\nfrom sensor_msgs.msg import AudioData\nfrom geometry_msgs.msg import PoseStamped\nfrom action_msgs.msg import GoalStatus\nfrom audio_common_msgs.msg import AudioData as AudioDataMsg\nimport whisper\nimport numpy as np\nfrom typing import Dict, List, Optional\nimport threading\nimport queue\nimport time\nfrom dataclasses import dataclass\n\n@dataclass\nclass RobotCommand:\n    \"\"\"Command for humanoid robot\"\"\"\n    command_type: str\n    arguments: Dict\n    confidence: float\n    timestamp: float\n\nclass AudioInputNode(Node):\n    \"\"\"ROS 2 node for audio input capture\"\"\"\n\n    def __init__(self):\n        super().__init__('audio_input_node')\n\n        # Parameters\n        self.declare_parameter('sample_rate', 16000)\n        self.declare_parameter('chunk_duration', 0.5)\n        self.declare_parameter('device_index', -1)  # Use default device\n\n        self.sample_rate = self.get_parameter('sample_rate').value\n        self.chunk_duration = self.get_parameter('chunk_duration').value\n        self.device_index = self.get_parameter('device_index').value\n\n        # Publisher for audio data\n        qos_profile = QoSProfile(depth=10)\n        qos_profile.durability = DurabilityPolicy.TRANSIENT_LOCAL\n        self.audio_pub = self.create_publisher(AudioDataMsg, 'audio_input', qos_profile)\n\n        # Initialize audio input\n        try:\n            import pyaudio\n            self.pyaudio = pyaudio\n            self.audio = pyaudio.PyAudio()\n\n            # Open audio stream\n            self.stream = self.audio.open(\n                format=pyaudio.paInt16,\n                channels=1,\n                rate=self.sample_rate,\n                input=True,\n                frames_per_buffer=int(self.sample_rate * self.chunk_duration),\n                input_device_index=self.device_index if self.device_index >= 0 else None\n            )\n\n            # Start audio capture timer\n            self.timer = self.create_timer(self.chunk_duration, self.capture_audio)\n\n            self.get_logger().info('Audio input node initialized')\n\n        except ImportError:\n            self.get_logger().error('PyAudio not available. Install with: pip install pyaudio')\n            self.stream = None\n            self.audio = None\n\n    def capture_audio(self):\n        \"\"\"Capture audio chunk and publish to ROS\"\"\"\n        if self.stream is None:\n            return\n\n        try:\n            # Read audio data\n            data = self.stream.read(int(self.sample_rate * self.chunk_duration), exception_on_overflow=False)\n\n            # Create and publish audio message\n            audio_msg = AudioDataMsg()\n            audio_msg.data = data\n            audio_msg.info.channels = 1\n            audio_msg.info.sample_rate = self.sample_rate\n            audio_msg.info.encoding = 'PCM_16'\n            audio_msg.info.step = 2  # 16-bit = 2 bytes\n\n            self.audio_pub.publish(audio_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error capturing audio: {e}')\n\nclass WhisperTranscriptionNode(Node):\n    \"\"\"ROS 2 node for Whisper-based speech transcription\"\"\"\n\n    def __init__(self):\n        super().__init__('whisper_transcription_node')\n\n        # Parameters\n        self.declare_parameter('model_size', 'base')\n        self.declare_parameter('device', 'cuda' if torch.cuda.is_available() else 'cpu')\n        self.declare_parameter('language', 'en')\n        self.declare_parameter('enable_translation', False)\n\n        self.model_size = self.get_parameter('model_size').value\n        self.device = self.get_parameter('device').value\n        self.language = self.get_parameter('language').value\n        self.enable_translation = self.get_parameter('enable_translation').value\n\n        # Initialize Whisper\n        try:\n            self.whisper_model = whisper.load_model(self.model_size, device=self.device)\n            self.get_logger().info(f'Whisper model {self.model_size} loaded on {self.device}')\n        except Exception as e:\n            self.get_logger().error(f'Failed to load Whisper model: {e}')\n            self.whisper_model = None\n            return\n\n        # Initialize audio preprocessor\n        self.preprocessor = AdvancedAudioPreprocessor(sample_rate=16000)\n\n        # Subscribers\n        self.audio_sub = self.create_subscription(\n            AudioDataMsg, 'audio_input', self.audio_callback, 10)\n\n        # Publishers\n        self.transcription_pub = self.create_publisher(String, 'transcription', 10)\n        self.command_pub = self.create_publisher(String, 'robot_command', 10)\n        self.confidence_pub = self.create_publisher(Float32, 'transcription_confidence', 10)\n\n        # Internal state\n        self.audio_buffer = np.array([])\n        self.speech_segmenter = RealTimeAudioProcessor()\n\n        self.get_logger().info('Whisper transcription node initialized')\n\n    def audio_callback(self, msg: AudioDataMsg):\n        \"\"\"Handle incoming audio data\"\"\"\n        if self.whisper_model is None:\n            return\n\n        try:\n            # Convert audio data to numpy array\n            audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Process with speech segmenter to detect complete utterances\n            speech_segment = self.speech_segmenter.process_audio_chunk(audio_data)\n\n            if speech_segment is not None:\n                # Transcribe the speech segment\n                self.transcribe_audio(speech_segment)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing audio: {e}')\n\n    def transcribe_audio(self, audio_segment: np.ndarray):\n        \"\"\"Transcribe audio segment using Whisper\"\"\"\n        try:\n            # Preprocess audio\n            processed_audio = self.preprocessor.preprocess_audio_batch(audio_segment)\n\n            # Ensure correct format\n            if processed_audio.dtype != np.float32:\n                processed_audio = processed_audio.astype(np.float32)\n\n            # Transcribe with Whisper\n            if self.enable_translation:\n                result = self.whisper_model.transcribe(\n                    processed_audio,\n                    task=\"translate\",\n                    language=self.language,\n                    fp16=(self.device == \"cuda\")\n                )\n            else:\n                result = self.whisper_model.transcribe(\n                    processed_audio,\n                    language=self.language,\n                    fp16=(self.device == \"cuda\")\n                )\n\n            # Extract text\n            text = result.get('text', '').strip()\n\n            if text:\n                # Publish transcription\n                transcription_msg = String()\n                transcription_msg.data = text\n                self.transcription_pub.publish(transcription_msg)\n\n                # Estimate and publish confidence\n                confidence = self._estimate_transcription_confidence(result, text)\n                confidence_msg = Float32()\n                confidence_msg.data = confidence\n                self.confidence_pub.publish(confidence_msg)\n\n                # Interpret as command and publish if high confidence\n                if confidence > 0.7:  # Only publish commands with high confidence\n                    command_intent, entities = self.interpret_command(text)\n                    if command_intent:\n                        command_msg = String()\n                        command_msg.data = f\"{command_intent}:{text}\"\n                        self.command_pub.publish(command_msg)\n\n                self.get_logger().info(f'Transcribed: \"{text}\" (confidence: {confidence:.2f})')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in transcription: {e}')\n\n    def _estimate_transcription_confidence(self, result: Dict, text: str) -> float:\n        \"\"\"Estimate confidence of transcription\"\"\"\n        if not text.strip():\n            return 0.0\n\n        # Simple confidence estimation\n        confidence = 1.0\n\n        # Length-based adjustment\n        if len(text.strip()) < 3:\n            confidence *= 0.3\n        elif len(text.strip()) > 100:\n            confidence *= 0.9\n\n        # Check for common filler words\n        if any(word in text.lower() for word in ['you know', 'um', 'uh', 'like', 'so']):\n            confidence *= 0.7\n\n        return max(0.0, min(1.0, confidence))\n\n    def interpret_command(self, text: str) -> Tuple[Optional[str], List[Dict]]:\n        \"\"\"Interpret transcribed text as robot command\"\"\"\n        # This would use the CommandInterpreter from the previous section\n        interpreter = CommandInterpreter()\n        return interpreter.extract_command_intent(text)\n\nclass CommandExecutionNode(Node):\n    \"\"\"ROS 2 node for executing robot commands from speech\"\"\"\n\n    def __init__(self):\n        super().__init__('command_execution_node')\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String, 'robot_command', self.command_callback, 10)\n\n        # Publishers for robot control\n        self.navigation_pub = self.create_publisher(PoseStamped, 'navigation_goal', 10)\n        self.action_pub = self.create_publisher(String, 'robot_action', 10)\n\n        # Internal state\n        self.command_queue = queue.Queue()\n        self.command_thread = threading.Thread(target=self.command_worker)\n        self.command_thread.daemon = True\n        self.command_thread.start()\n\n        self.get_logger().info('Command execution node initialized')\n\n    def command_callback(self, msg: String):\n        \"\"\"Handle incoming robot command\"\"\"\n        try:\n            # Parse command\n            command_parts = msg.data.split(':', 1)\n            if len(command_parts) >= 2:\n                command_type = command_parts[0]\n                command_text = command_parts[1]\n\n                # Create robot command\n                robot_cmd = RobotCommand(\n                    command_type=command_type,\n                    arguments={'text': command_text},\n                    confidence=1.0,  # This would come from transcription confidence\n                    timestamp=time.time()\n                )\n\n                # Add to queue for processing\n                self.command_queue.put(robot_cmd)\n\n                self.get_logger().info(f'Command received: {command_type} - {command_text}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error parsing command: {e}')\n\n    def command_worker(self):\n        \"\"\"Worker thread for processing commands\"\"\"\n        while rclpy.ok():\n            try:\n                # Get command from queue (with timeout to allow graceful shutdown)\n                command = self.command_queue.get(timeout=1.0)\n\n                # Execute command\n                self.execute_command(command)\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                self.get_logger().error(f'Error in command worker: {e}')\n\n    def execute_command(self, command: RobotCommand):\n        \"\"\"Execute robot command based on type\"\"\"\n        if command.command_type == 'move':\n            self.execute_navigation_command(command)\n        elif command.command_type == 'grasp':\n            self.execute_manipulation_command(command)\n        elif command.command_type == 'action':\n            self.execute_action_command(command)\n        else:\n            self.get_logger().info(f'Unknown command type: {command.command_type}')\n\n    def execute_navigation_command(self, command: RobotCommand):\n        \"\"\"Execute navigation command\"\"\"\n        target_location = command.arguments.get('text', '').lower()\n\n        # Simple location mapping (in a real system, this would use a map)\n        location_map = {\n            'kitchen': (2.0, 1.0, 0.0),\n            'living room': (0.0, 0.0, 0.0),\n            'bedroom': (-2.0, 1.0, 0.0),\n            'bathroom': (-1.0, -1.0, 0.0)\n        }\n\n        if target_location in location_map:\n            x, y, theta = location_map[target_location]\n\n            # Create navigation goal\n            goal = PoseStamped()\n            goal.header.stamp = self.get_clock().now().to_msg()\n            goal.header.frame_id = 'map'\n            goal.pose.position.x = x\n            goal.pose.position.y = y\n            goal.pose.position.z = 0.0\n\n            # Convert theta to quaternion\n            import math\n            goal.pose.orientation.z = math.sin(theta / 2.0)\n            goal.pose.orientation.w = math.cos(theta / 2.0)\n\n            # Publish navigation goal\n            self.navigation_pub.publish(goal)\n            self.get_logger().info(f'Navigating to {target_location} at ({x}, {y})')\n        else:\n            self.get_logger().info(f'Unknown location: {target_location}')\n\n    def execute_manipulation_command(self, command: RobotCommand):\n        \"\"\"Execute manipulation command\"\"\"\n        object_name = command.arguments.get('text', '').lower()\n\n        # In a real system, this would trigger arm/leg movements\n        action_msg = String()\n        action_msg.data = f'grasping:{object_name}'\n        self.action_pub.publish(action_msg)\n\n        self.get_logger().info(f'Attempting to grasp {object_name}')\n\n    def execute_action_command(self, command: RobotCommand):\n        \"\"\"Execute action command\"\"\"\n        action_name = command.arguments.get('text', '').lower()\n\n        action_msg = String()\n        action_msg.data = f'action:{action_name}'\n        self.action_pub.publish(action_msg)\n\n        self.get_logger().info(f'Performing action: {action_name}')\n\ndef main(args=None):\n    \"\"\"Main function to run the Whisper ROS integration\"\"\"\n    rclpy.init(args=args)\n\n    # Create nodes\n    audio_node = AudioInputNode()\n    whisper_node = WhisperTranscriptionNode()\n    command_node = CommandExecutionNode()\n\n    # Create executor and add nodes\n    executor = rclpy.executors.MultiThreadedExecutor()\n    executor.add_node(audio_node)\n    executor.add_node(whisper_node)\n    executor.add_node(command_node)\n\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        # Cleanup\n        if hasattr(audio_node, 'stream') and audio_node.stream:\n            audio_node.stream.stop_stream()\n            audio_node.stream.close()\n        if hasattr(audio_node, 'audio'):\n            audio_node.audio.terminate()\n\n        audio_node.destroy_node()\n        whisper_node.destroy_node()\n        command_node.destroy_node()\n\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"real-time-processing",children:"Real-time Processing"}),"\n",(0,s.jsx)(n.p,{children:"Real-time processing of audio for Whisper integration requires careful consideration of latency, computational efficiency, and accuracy. The following implementation demonstrates how to achieve real-time performance while maintaining transcription quality."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport threading\nimport time\nfrom collections import deque\nimport numpy as np\nfrom typing import Deque, Optional, Callable\nimport queue\n\nclass RealTimeWhisperProcessor:\n    """Real-time Whisper processor with streaming audio support"""\n\n    def __init__(self,\n                 model_size: str = "base",\n                 device: str = "cuda",\n                 buffer_duration: float = 5.0,  # seconds\n                 hop_length: float = 0.5):      # seconds\n\n        self.model_size = model_size\n        self.device = device\n        self.buffer_duration = buffer_duration\n        self.hop_length = hop_length\n\n        # Load model\n        self.model = whisper.load_model(model_size, device=device)\n\n        # Audio processing\n        self.sample_rate = 16000\n        self.buffer_size = int(buffer_duration * self.sample_rate)\n        self.hop_size = int(hop_length * self.sample_rate)\n\n        # Audio buffer for streaming\n        self.audio_buffer = np.zeros(self.buffer_size, dtype=np.float32)\n        self.buffer_ptr = 0\n        self.audio_available = threading.Event()\n\n        # Processing queue\n        self.process_queue = queue.Queue()\n        self.result_queue = queue.Queue()\n\n        # Threading\n        self.processing_thread = None\n        self.running = False\n\n        # Callbacks\n        self.transcription_callback: Optional[Callable] = None\n        self.partial_result_callback: Optional[Callable] = None\n\n        # Preprocessing\n        self.preprocessor = AdvancedAudioPreprocessor(sample_rate=self.sample_rate)\n\n    def start_processing(self):\n        """Start real-time processing"""\n        self.running = True\n        self.processing_thread = threading.Thread(target=self._processing_loop)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n    def stop_processing(self):\n        """Stop real-time processing"""\n        self.running = False\n        if self.processing_thread:\n            self.processing_thread.join()\n\n    def add_audio_chunk(self, audio_chunk: np.ndarray):\n        """Add audio chunk to processing buffer"""\n        # Convert to float32 if needed\n        if audio_chunk.dtype != np.float32:\n            audio_chunk = audio_chunk.astype(np.float32)\n\n        # Preprocess chunk\n        processed_chunk = self.preprocessor.preprocess_audio_batch(audio_chunk)\n\n        # Add to circular buffer\n        chunk_len = len(processed_chunk)\n\n        if self.buffer_ptr + chunk_len <= self.buffer_size:\n            self.audio_buffer[self.buffer_ptr:self.buffer_ptr + chunk_len] = processed_chunk\n            self.buffer_ptr += chunk_len\n        else:\n            # Wrap around\n            first_part = self.buffer_size - self.buffer_ptr\n            self.audio_buffer[self.buffer_ptr:] = processed_chunk[:first_part]\n            self.audio_buffer[:chunk_len - first_part] = processed_chunk[first_part:]\n            self.buffer_ptr = (self.buffer_ptr + chunk_len) % self.buffer_size\n\n    def _processing_loop(self):\n        """Main processing loop for real-time transcription"""\n        last_hop = 0\n\n        while self.running:\n            try:\n                current_pos = self.buffer_ptr\n\n                # Check if we have enough audio to process\n                if current_pos >= self.hop_size:\n                    # Extract audio segment for processing\n                    start_idx = (current_pos - self.hop_size) % self.buffer_size\n                    end_idx = current_pos\n\n                    if start_idx < end_idx:\n                        audio_segment = self.audio_buffer[start_idx:end_idx]\n                    else:\n                        # Handle wrap-around\n                        audio_segment = np.concatenate([\n                            self.audio_buffer[start_idx:],\n                            self.audio_buffer[:end_idx]\n                        ])\n\n                    # Process the segment\n                    self._process_audio_segment(audio_segment)\n\n                    # Move hop forward\n                    last_hop = current_pos\n\n                    # Small delay to prevent busy waiting\n                    time.sleep(0.01)\n                else:\n                    # Not enough audio, wait a bit\n                    time.sleep(0.1)\n\n            except Exception as e:\n                print(f"Error in processing loop: {e}")\n                time.sleep(0.1)\n\n    def _process_audio_segment(self, audio_segment: np.ndarray):\n        """Process a single audio segment with Whisper"""\n        try:\n            # Ensure proper format\n            if len(audio_segment) < 100:  # Too short to process\n                return\n\n            # Transcribe the segment\n            result = self.model.transcribe(\n                audio_segment,\n                fp16=(self.device == "cuda")\n            )\n\n            text = result.get(\'text\', \'\').strip()\n\n            if text and len(text) > 2:  # Only process non-empty, meaningful text\n                # Calculate confidence\n                confidence = self._estimate_confidence(result, text)\n\n                # Create result\n                transcription_result = {\n                    \'text\': text,\n                    \'language\': result.get(\'language\', \'unknown\'),\n                    \'confidence\': confidence,\n                    \'timestamp\': time.time(),\n                    \'segments\': result.get(\'segments\', [])\n                }\n\n                # Call callback if available\n                if self.transcription_callback:\n                    self.transcription_callback(transcription_result)\n\n                # Add to result queue\n                self.result_queue.put(transcription_result)\n\n        except Exception as e:\n            print(f"Error processing audio segment: {e}")\n\n    def _estimate_confidence(self, result: Dict, text: str) -> float:\n        """Estimate confidence of transcription"""\n        if not text.strip():\n            return 0.0\n\n        confidence = 1.0\n\n        # Length-based confidence\n        if len(text.strip()) < 3:\n            confidence *= 0.3\n        elif len(text.strip()) > 100:\n            confidence *= 0.9\n\n        # Check for common filler words\n        if any(word in text.lower() for word in [\'you know\', \'um\', \'uh\', \'like\', \'so\']):\n            confidence *= 0.7\n\n        return max(0.0, min(1.0, confidence))\n\n    def set_transcription_callback(self, callback: Callable):\n        """Set callback for completed transcriptions"""\n        self.transcription_callback = callback\n\n    def set_partial_result_callback(self, callback: Callable):\n        """Set callback for partial results (not implemented in this basic version)"""\n        self.partial_result_callback = callback\n\n    def get_results(self) -> List[Dict]:\n        """Get available transcription results"""\n        results = []\n        try:\n            while True:\n                result = self.result_queue.get_nowait()\n                results.append(result)\n        except queue.Empty:\n            pass\n        return results\n\nclass StreamingAudioProcessor:\n    """Advanced streaming audio processor with VAD and real-time Whisper"""\n\n    def __init__(self, model_size: str = "base", device: str = "cuda"):\n        self.model_size = model_size\n        self.device = device\n\n        # Real-time processor\n        self.rt_processor = RealTimeWhisperProcessor(model_size, device)\n\n        # Voice activity detection\n        self.vad_processor = RealTimeAudioProcessor()\n\n        # Conversation context\n        self.context_aware = ContextAwareRecognizer(\n            WhisperSpeechRecognizer(model_size, device)\n        )\n\n        # Results buffer\n        self.results_buffer = deque(maxlen=10)  # Keep last 10 results\n\n    def process_streaming_audio(self, audio_chunk: np.ndarray) -> Optional[Dict]:\n        """Process streaming audio chunk with VAD and transcription"""\n        # First, use VAD to detect speech segments\n        speech_segment = self.vad_processor.process_audio_chunk(audio_chunk)\n\n        if speech_segment is not None:\n            # We have a complete speech segment, process with Whisper\n            result = self.context_aware.recognize_with_context(\n                speech_segment,\n                current_context=self.context_aware.get_conversation_context()\n            )\n\n            # Add to results buffer\n            result_dict = {\n                \'text\': result.text,\n                \'language\': result.language,\n                \'confidence\': result.confidence,\n                \'command_intent\': result.command_intent,\n                \'entities\': result.entities,\n                \'timestamp\': result.timestamp\n            }\n\n            self.results_buffer.append(result_dict)\n\n            return result_dict\n\n        return None\n\n    def start_streaming(self):\n        """Start streaming processing"""\n        self.rt_processor.start_processing()\n\n    def stop_streaming(self):\n        """Stop streaming processing"""\n        self.rt_processor.stop_processing()\n\n    def get_recent_results(self, n: int = 5) -> List[Dict]:\n        """Get recent transcription results"""\n        return list(self.results_buffer)[-n:]\n\n# Example of real-time processing\ndef example_real_time_whisper():\n    """Example of real-time Whisper processing"""\n    import pyaudio\n    import wave\n\n    # Initialize real-time processor\n    processor = StreamingAudioProcessor(model_size="base")\n    processor.start_streaming()\n\n    # Setup audio input (simplified example)\n    chunk_duration = 0.5  # 500ms chunks\n    sample_rate = 16000\n    chunk_size = int(sample_rate * chunk_duration)\n\n    # Simulate audio streaming\n    print("Starting real-time processing... (Press Ctrl+C to stop)")\n\n    try:\n        for i in range(100):  # Simulate 50 seconds of audio\n            # In a real application, this would come from microphone input\n            # For this example, we\'ll generate synthetic audio\n            t = np.linspace(i * chunk_duration, (i + 1) * chunk_duration, chunk_size)\n            # Create audio with some speech-like characteristics\n            audio_chunk = 0.2 * np.sin(2 * np.pi * 440 * t)  # 440 Hz tone\n            audio_chunk += 0.1 * np.sin(2 * np.pi * 660 * t)  # Harmonic\n            audio_chunk += np.random.normal(0, 0.05, chunk_size)  # Noise\n\n            # Process the chunk\n            result = processor.process_streaming_audio(audio_chunk)\n\n            if result and result[\'confidence\'] > 0.5:\n                print(f"Transcription: \'{result[\'text\']}\' (Conf: {result[\'confidence\']:.2f})")\n\n            time.sleep(chunk_duration)  # Simulate real-time pacing\n\n    except KeyboardInterrupt:\n        print("\\nStopping real-time processing...")\n\n    finally:\n        processor.stop_streaming()\n        recent_results = processor.get_recent_results()\n        print(f"\\nRecent results: {len(recent_results)} transcriptions processed")\n\ndef example_ros_integration():\n    """Example of integrating real-time processing with ROS 2"""\n    # This would be part of a ROS 2 node\n    processor = RealTimeWhisperProcessor(model_size="base")\n\n    def transcription_handler(result):\n        """Handle completed transcriptions"""\n        print(f"Real-time transcription: {result[\'text\']}")\n        # In ROS context, this would publish to a topic\n        # e.g., self.transcription_publisher.publish(String(data=result[\'text\']))\n\n    # Set up callback\n    processor.set_transcription_callback(transcription_handler)\n\n    # Start processing\n    processor.start_processing()\n\n    # Simulate adding audio chunks (in ROS, this would come from audio subscriber)\n    for i in range(20):\n        # Simulate audio chunk\n        chunk_size = int(16000 * 0.5)  # 500ms chunk\n        t = np.linspace(i * 0.5, (i + 1) * 0.5, chunk_size)\n        audio_chunk = 0.3 * np.sin(2 * np.pi * 523 * t)  # Musical note\n        audio_chunk += np.random.normal(0, 0.05, chunk_size)\n\n        processor.add_audio_chunk(audio_chunk)\n        time.sleep(0.1)  # Small delay\n\n    # Get results\n    results = processor.get_results()\n    print(f"Processed {len(results)} segments")\n\n    processor.stop_processing()\n\n# Run examples if this file is executed directly\nif __name__ == "__main__":\n    print("Running real-time Whisper examples...")\n    example_real_time_whisper()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Whisper integration enables humanoid robots to understand and respond to spoken commands through sophisticated audio processing, speech recognition, and natural language understanding. The implementation includes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Audio Preprocessing"}),": Advanced noise reduction, voice activity detection, and signal enhancement techniques to improve transcription accuracy in robotic environments."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition"}),": Real-time transcription using OpenAI Whisper models with confidence estimation and command interpretation capabilities."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Integration"}),": Complete ROS 2 nodes for audio capture, transcription, and command execution that can be integrated into humanoid robot systems."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real-time Processing"}),": Efficient streaming audio processing with minimal latency while maintaining high transcription accuracy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The system is designed to be robust in real-world environments with background noise, reverberation, and varying acoustic conditions. The modular architecture allows for customization based on specific robot platforms and application requirements, making it suitable for a wide range of humanoid robot applications requiring natural language interaction."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);