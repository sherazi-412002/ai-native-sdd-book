"use strict";(globalThis.webpackChunkclassic=globalThis.webpackChunkclassic||[]).push([[8902],{5049(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-4/llm-cognitive-planning","title":"LLM Cognitive Planning","description":"Cognitive Planning with Large Language Models","source":"@site/docs/module-4/llm-cognitive-planning.md","sourceDirName":"module-4","slug":"/module-4/llm-cognitive-planning","permalink":"/ai-native-sdd-book/docs/module-4/llm-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/sherazi-412002/ai-native-sdd-book/tree/main/docs/module-4/llm-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"curriculumSidebar","previous":{"title":"Whisper Integration","permalink":"/ai-native-sdd-book/docs/module-4/whisper-integration"},"next":{"title":"Capstone Project","permalink":"/ai-native-sdd-book/docs/module-4/capstone-project"}}');var s=t(4848),o=t(8453);const i={sidebar_position:2},r="LLM Cognitive Planning",l={},c=[{value:"Cognitive Planning with Large Language Models",id:"cognitive-planning-with-large-language-models",level:2},{value:"Introduction to Cognitive Planning",id:"introduction-to-cognitive-planning",level:2},{value:"LLM Integration",id:"llm-integration",level:2},{value:"Planning Algorithms",id:"planning-algorithms",level:2},{value:"Context Understanding",id:"context-understanding",level:2},{value:"Task Decomposition",id:"task-decomposition",level:2},{value:"Execution Monitoring",id:"execution-monitoring",level:2},{value:"Summary",id:"summary",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"llm-cognitive-planning",children:"LLM Cognitive Planning"})}),"\n",(0,s.jsx)(n.h2,{id:"cognitive-planning-with-large-language-models",children:"Cognitive Planning with Large Language Models"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covers using Large Language Models (LLMs) for cognitive planning in humanoid robots. Cognitive planning refers to the ability of an AI system to understand high-level goals, decompose them into executable tasks, reason about the world state, and generate appropriate action sequences to achieve objectives. For humanoid robots, this involves processing natural language commands and translating them into complex behavioral sequences."}),"\n",(0,s.jsx)(n.p,{children:"LLM-based cognitive planning leverages the reasoning capabilities, world knowledge, and language understanding of large language models to create more flexible and adaptive robotic systems. Unlike traditional planning approaches that rely on pre-defined rules and symbolic representations, LLMs can handle ambiguous, natural language instructions and generate plans for novel situations."}),"\n",(0,s.jsx)(n.h2,{id:"introduction-to-cognitive-planning",children:"Introduction to Cognitive Planning"}),"\n",(0,s.jsx)(n.p,{children:"Cognitive planning in robotics involves several key components:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal Understanding"}),": Interpreting high-level goals or commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"World Modeling"}),": Maintaining an internal representation of the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan Generation"}),": Creating sequences of actions to achieve goals"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Plan Execution"}),": Executing actions while monitoring progress"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adaptation"}),": Adjusting plans based on feedback and changing conditions"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nimport numpy as np\nimport json\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom dataclasses import dataclass\nimport asyncio\nimport time\nfrom enum import Enum\n\n@dataclass\nclass WorldState:\n    """Represents the current state of the world"""\n    objects: Dict[str, Dict]  # Object name to properties\n    robot_state: Dict[str, Any]  # Robot\'s current state\n    environment: Dict[str, Any]  # Environmental conditions\n    time: float  # Current simulation time\n    location: str  # Current location of robot\n\n@dataclass\nclass Action:\n    """Represents an action that can be executed"""\n    name: str\n    parameters: Dict[str, Any]\n    duration: float  # Expected duration in seconds\n    preconditions: List[str]  # Conditions that must be true before execution\n    effects: List[str]  # Effects of the action on the world state\n\n@dataclass\nclass PlanStep:\n    """A step in a plan"""\n    action: Action\n    expected_outcome: str\n    confidence: float\n    dependencies: List[int]  # Indices of steps that must be completed first\n\n@dataclass\nclass CognitivePlan:\n    """A complete cognitive plan"""\n    steps: List[PlanStep]\n    goal: str\n    context: Dict[str, Any]\n    creation_time: float\n    estimated_duration: float\n\nclass PlanningState(Enum):\n    """State of the planning process"""\n    IDLE = 0\n    UNDERSTANDING_GOAL = 1\n    ANALYZING_WORLD = 2\n    GENERATING_PLAN = 3\n    VALIDATING_PLAN = 4\n    EXECUTING = 5\n    ADAPTING = 6\n    COMPLETED = 7\n    FAILED = 8\n\nclass CognitivePlanner:\n    """Main cognitive planning system using LLMs"""\n\n    def __init__(self, api_key: str, model: str = "gpt-4"):\n        self.api_key = api_key\n        self.model = model\n        self.current_state = WorldState(\n            objects={},\n            robot_state={},\n            environment={},\n            time=time.time(),\n            location="unknown"\n        )\n        self.current_plan = None\n        self.planning_state = PlanningState.IDLE\n        self.action_repository = self._initialize_action_repository()\n\n    def _initialize_action_repository(self) -> Dict[str, Dict]:\n        """Initialize available actions for the robot"""\n        return {\n            "move_to": {\n                "description": "Move the robot to a specific location",\n                "parameters": {\n                    "target_location": "str - The destination location",\n                    "speed": "float - Movement speed (0.1-1.0)"\n                },\n                "preconditions": ["robot_is_active", "target_location_is_known"],\n                "effects": ["robot_location_changes"]\n            },\n            "pick_up": {\n                "description": "Pick up an object with the robot\'s manipulator",\n                "parameters": {\n                    "object_name": "str - Name of the object to pick up",\n                    "arm": "str - Which arm to use (left, right)"\n                },\n                "preconditions": ["object_is_reachable", "object_is_graspable", "arm_is_free"],\n                "effects": ["object_is_held", "arm_is_occupied"]\n            },\n            "place": {\n                "description": "Place an object at a specific location",\n                "parameters": {\n                    "object_name": "str - Name of the object to place",\n                    "target_location": "str - Where to place the object",\n                    "arm": "str - Which arm is holding the object"\n                },\n                "preconditions": ["object_is_held", "target_location_is_reachable"],\n                "effects": ["object_is_placed", "arm_is_free"]\n            },\n            "navigate_to_object": {\n                "description": "Navigate to an object\'s location",\n                "parameters": {\n                    "object_name": "str - Name of the target object"\n                },\n                "preconditions": ["object_is_visible", "object_location_is_known"],\n                "effects": ["robot_is_near_object"]\n            },\n            "detect_object": {\n                "description": "Detect and identify objects in the environment",\n                "parameters": {\n                    "object_type": "str - Type of object to detect (optional)"\n                },\n                "preconditions": ["camera_is_active"],\n                "effects": ["object_locations_updated"]\n            },\n            "ask_human": {\n                "description": "Ask for clarification or information from a human",\n                "parameters": {\n                    "question": "str - The question to ask",\n                    "target_person": "str - Who to ask (optional)"\n                },\n                "preconditions": ["human_is_present"],\n                "effects": ["information_acquired"]\n            }\n        }\n\n    async def understand_goal(self, goal_description: str) -> Dict[str, Any]:\n        """Use LLM to understand and decompose a high-level goal"""\n        self.planning_state = PlanningState.UNDERSTANDING_GOAL\n\n        prompt = f"""\n        Analyze the following goal and break it down into components:\n        Goal: "{goal_description}"\n\n        Provide a structured analysis with:\n        1. Main objective\n        2. Sub-goals or required steps\n        3. Necessary conditions or prerequisites\n        4. Potential obstacles or challenges\n        5. Success criteria\n\n        Respond in JSON format with keys: main_objective, sub_goals, prerequisites, challenges, success_criteria\n        """\n\n        response = await openai.ChatCompletion.acreate(\n            model=self.model,\n            messages=[{"role": "user", "content": prompt}],\n            temperature=0.3\n        )\n\n        analysis = json.loads(response.choices[0].message.content)\n        return analysis\n\n    async def analyze_world_state(self, goal_analysis: Dict[str, Any]) -> WorldState:\n        """Analyze the current world state in relation to the goal"""\n        self.planning_state = PlanningState.ANALYZING_WORLD\n\n        # In a real system, this would interface with perception systems\n        # For this example, we\'ll simulate world state analysis\n        current_state = self.current_state\n\n        # Update state based on goal requirements\n        required_objects = self._extract_required_objects(goal_analysis)\n        detected_objects = await self._detect_relevant_objects(required_objects)\n\n        current_state.objects.update(detected_objects)\n        current_state.time = time.time()\n\n        return current_state\n\n    def _extract_required_objects(self, goal_analysis: Dict[str, Any]) -> List[str]:\n        """Extract object names that might be needed for the goal"""\n        # This would use NLP to extract object references from the goal analysis\n        text_context = f"{goal_analysis.get(\'main_objective\', \'\')} {goal_analysis.get(\'sub_goals\', \'\')}"\n        # Simple keyword extraction (in practice, use proper NLP)\n        import re\n        words = re.findall(r\'\\b\\w+\\b\', text_context.lower())\n        potential_objects = [word for word in words if len(word) > 2]  # Filter short words\n        return list(set(potential_objects))  # Remove duplicates\n\n    async def _detect_relevant_objects(self, object_names: List[str]) -> Dict[str, Dict]:\n        """Simulate object detection (in real system, this would use perception)"""\n        # Simulate detection results\n        detected_objects = {}\n        for obj_name in object_names[:5]:  # Limit for simulation\n            detected_objects[obj_name] = {\n                "location": f"{obj_name}_location",\n                "pose": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # x, y, z, roll, pitch, yaw\n                "properties": {\n                    "graspable": True,\n                    "movable": True,\n                    "size": "medium"\n                },\n                "confidence": 0.8 + np.random.random() * 0.2  # 0.8-1.0\n            }\n        return detected_objects\n\n    async def generate_plan(self, goal_analysis: Dict[str, Any], world_state: WorldState) -> CognitivePlan:\n        """Generate a cognitive plan using LLM"""\n        self.planning_state = PlanningState.GENERATING_PLAN\n\n        # Create prompt for plan generation\n        prompt = f"""\n        Generate a detailed plan to achieve the following goal:\n        Main Objective: {goal_analysis[\'main_objective\']}\n        Sub-goals: {goal_analysis[\'sub_goals\']}\n        Prerequisites: {goal_analysis[\'prerequisites\']}\n\n        Current World State:\n        - Robot Location: {world_state.location}\n        - Available Objects: {list(world_state.objects.keys())}\n        - Robot Capabilities: {list(self.action_repository.keys())}\n\n        Create a step-by-step plan with:\n        1. Specific actions from the available action set\n        2. Required parameters for each action\n        3. Estimated confidence for each step\n        4. Dependencies between steps\n\n        Available actions: {list(self.action_repository.keys())}\n        Each action has: {self.action_repository}\n\n        Respond in JSON format with structure:\n        {{\n            "steps": [\n                {{\n                    "action": "action_name",\n                    "parameters": {{"param1": "value1", ...}},\n                    "expected_outcome": "description",\n                    "confidence": 0.0-1.0,\n                    "dependencies": [step_indices]\n                }}\n            ],\n            "estimated_duration": total_seconds\n        }}\n        """\n\n        response = await openai.ChatCompletion.acreate(\n            model=self.model,\n            messages=[{"role": "user", "content": prompt}],\n            temperature=0.4\n        )\n\n        plan_data = json.loads(response.choices[0].message.content)\n\n        # Convert to CognitivePlan object\n        steps = []\n        for step_data in plan_data[\'steps\']:\n            action = Action(\n                name=step_data[\'action\'],\n                parameters=step_data[\'parameters\'],\n                duration=1.0,  # Default duration\n                preconditions=self.action_repository[step_data[\'action\']].get(\'preconditions\', []),\n                effects=self.action_repository[step_data[\'action\']].get(\'effects\', [])\n            )\n            step = PlanStep(\n                action=action,\n                expected_outcome=step_data[\'expected_outcome\'],\n                confidence=step_data[\'confidence\'],\n                dependencies=step_data[\'dependencies\']\n            )\n            steps.append(step)\n\n        plan = CognitivePlan(\n            steps=steps,\n            goal=goal_analysis[\'main_objective\'],\n            context=goal_analysis,\n            creation_time=time.time(),\n            estimated_duration=plan_data[\'estimated_duration\']\n        )\n\n        self.current_plan = plan\n        return plan\n\n    async def validate_plan(self, plan: CognitivePlan, world_state: WorldState) -> Tuple[bool, List[str]]:\n        """Validate the generated plan for feasibility"""\n        self.planning_state = PlanningState.VALIDATING_PLAN\n\n        issues = []\n\n        # Check preconditions for each step\n        for i, step in enumerate(plan.steps):\n            for precondition in step.action.preconditions:\n                # In a real system, this would check actual world state\n                # For simulation, we\'ll assume some basic validation\n                if "robot_is_active" in precondition and not world_state.robot_state.get("active", False):\n                    issues.append(f"Step {i}: Robot is not active, required for {step.action.name}")\n\n        # Check for conflicts between steps\n        for i in range(len(plan.steps)):\n            for j in range(i + 1, len(plan.steps)):\n                if (i in plan.steps[j].dependencies and\n                    j in plan.steps[i].dependencies):\n                    issues.append(f"Circular dependency between steps {i} and {j}")\n\n        is_valid = len(issues) == 0\n        return is_valid, issues\n\n    def execute_plan(self, plan: CognitivePlan):\n        """Execute the cognitive plan"""\n        self.planning_state = PlanningState.EXECUTING\n\n        # This would interface with the robot\'s execution system\n        # For this example, we\'ll simulate execution\n        for i, step in enumerate(plan.steps):\n            print(f"Executing step {i}: {step.action.name} with params {step.action.parameters}")\n            # Simulate execution time\n            time.sleep(0.1)\n\n            # Check if step succeeded\n            success = np.random.random() > 0.1  # 90% success rate for simulation\n            if not success:\n                print(f"Step {i} failed, adapting plan...")\n                self.planning_state = PlanningState.ADAPTING\n                # In a real system, this would trigger replanning\n                break\n\n        if self.planning_state == PlanningState.EXECUTING:\n            self.planning_state = PlanningState.COMPLETED\n\n    async def process_goal(self, goal_description: str) -> Optional[CognitivePlan]:\n        """Complete process from goal to execution-ready plan"""\n        try:\n            # Step 1: Understand the goal\n            goal_analysis = await self.understand_goal(goal_description)\n\n            # Step 2: Analyze world state\n            world_state = await self.analyze_world_state(goal_analysis)\n\n            # Step 3: Generate plan\n            plan = await self.generate_plan(goal_analysis, world_state)\n\n            # Step 4: Validate plan\n            is_valid, issues = await self.validate_plan(plan, world_state)\n\n            if not is_valid:\n                print(f"Plan validation failed with issues: {issues}")\n                # In a real system, this might trigger plan refinement\n                return None\n\n            return plan\n\n        except Exception as e:\n            print(f"Error in goal processing: {e}")\n            self.planning_state = PlanningState.FAILED\n            return None\n\n# Example usage\nasync def example_cognitive_planning():\n    """Example of using the cognitive planning system"""\n    # This would require an actual OpenAI API key\n    # For demonstration purposes, we\'ll show the structure\n    planner = CognitivePlanner(api_key="your-api-key-here")\n\n    goal = "Please bring me a cup of coffee from the kitchen"\n    plan = await planner.process_goal(goal)\n\n    if plan:\n        print(f"Generated plan for: {plan.goal}")\n        print(f"Plan has {len(plan.steps)} steps")\n        for i, step in enumerate(plan.steps):\n            print(f"  {i+1}. {step.action.name} - {step.expected_outcome} (confidence: {step.confidence:.2f})")\n\n    return plan\n'})}),"\n",(0,s.jsx)(n.h2,{id:"llm-integration",children:"LLM Integration"}),"\n",(0,s.jsx)(n.p,{children:"Integrating LLMs into cognitive planning systems requires careful consideration of prompt engineering, response parsing, and system integration. The following implementation demonstrates how to effectively use LLMs for various planning tasks."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nimport json\nimport asyncio\nfrom typing import Dict, List, Any, Optional, Callable\nimport re\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom enum import Enum\n\nclass TaskType(Enum):\n    """Types of planning tasks"""\n    GOAL_DECOMPOSITION = "goal_decomposition"\n    ACTION_PLANNING = "action_planning"\n    CONTEXT_ANALYSIS = "context_analysis"\n    TASK_REFINEMENT = "task_refinement"\n    EXECUTION_MONITORING = "execution_monitoring"\n\nclass LLMInterface:\n    """Interface for LLM integration in cognitive planning"""\n\n    def __init__(self, api_key: str, model: str = "gpt-4"):\n        openai.api_key = api_key\n        self.model = model\n        self.client = instructor.patch(openai.ChatCompletion)\n\n    async def generate_with_schema(self, prompt: str, response_model: BaseModel) -> BaseModel:\n        """Generate response with structured schema"""\n        try:\n            response = await self.client.acreate(\n                model=self.model,\n                messages=[{"role": "user", "content": prompt}],\n                response_model=response_model,\n                temperature=0.3\n            )\n            return response\n        except Exception as e:\n            print(f"Error generating with schema: {e}")\n            return None\n\n    async def generate_text(self, prompt: str, temperature: float = 0.3) -> str:\n        """Generate text response from LLM"""\n        try:\n            response = await openai.ChatCompletion.acreate(\n                model=self.model,\n                messages=[{"role": "user", "content": prompt}],\n                temperature=temperature\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f"Error generating text: {e}")\n            return ""\n\nclass GoalDecomposition(BaseModel):\n    """Schema for goal decomposition response"""\n    main_objective: str = Field(description="The main goal to be achieved")\n    sub_goals: List[str] = Field(description="List of sub-goals that need to be achieved")\n    priority_order: List[int] = Field(description="Order of sub-goal priority (indices of sub_goals)")\n    dependencies: Dict[str, List[str]] = Field(description="Dependencies between sub-goals")\n    success_criteria: List[str] = Field(description="Criteria for determining success")\n\nclass ActionPlan(BaseModel):\n    """Schema for action planning response"""\n    steps: List[Dict] = Field(description="List of steps to execute")\n    action_name: str = Field(description="Name of the action")\n    parameters: Dict[str, Any] = Field(description="Parameters for the action")\n    preconditions: List[str] = Field(description="Conditions that must be true before action")\n    expected_effects: List[str] = Field(description="Expected effects of the action")\n    confidence: float = Field(description="Confidence in the plan (0.0-1.0)")\n\nclass ContextAnalysis(BaseModel):\n    """Schema for context analysis response"""\n    current_situation: str = Field(description="Current situation analysis")\n    relevant_objects: List[str] = Field(description="Objects relevant to the task")\n    environmental_factors: List[str] = Field(description="Environmental factors to consider")\n    constraints: List[str] = Field(description="Constraints on the solution")\n    opportunities: List[str] = Field(description="Opportunities for optimization")\n\nclass TaskRefinement(BaseModel):\n    """Schema for task refinement response"""\n    refined_task: str = Field(description="More specific version of the task")\n    assumptions: List[str] = Field(description="Assumptions made during refinement")\n    clarifications_needed: List[str] = Field(description="Information needed for better execution")\n    alternative_approaches: List[str] = Field(description="Alternative approaches to consider")\n\nclass ExecutionMonitoring(BaseModel):\n    """Schema for execution monitoring response"""\n    progress: float = Field(description="Progress toward goal (0.0-1.0)")\n    obstacles_encountered: List[str] = Field(description="Obstacles encountered")\n    plan_adaptations: List[str] = Field(description="Suggested adaptations to the plan")\n    confidence_level: float = Field(description="Confidence in continued success (0.0-1.0)")\n\nclass LLMCognitivePlanner:\n    """Advanced cognitive planner using LLM integration"""\n\n    def __init__(self, llm_interface: LLMInterface):\n        self.llm = llm_interface\n        self.task_history = []\n        self.context_memory = {}\n\n    async def decompose_goal(self, goal: str, context: Dict[str, Any] = None) -> Optional[GoalDecomposition]:\n        """Decompose a high-level goal using LLM"""\n        prompt = f"""\n        Decompose the following goal into manageable sub-goals:\n\n        Goal: {goal}\n\n        Context: {context or \'No additional context provided\'}\n\n        Consider:\n        1. What are the main components of this goal?\n        2. What needs to happen first, second, etc.?\n        3. Are there dependencies between different parts?\n        4. How will we know when each part is complete?\n\n        Provide your analysis in the required JSON format.\n        """\n\n        return await self.llm.generate_with_schema(prompt, GoalDecomposition)\n\n    async def generate_action_plan(self, sub_goal: str, world_state: Dict[str, Any]) -> Optional[ActionPlan]:\n        """Generate an action plan for a sub-goal using LLM"""\n        prompt = f"""\n        Generate a detailed action plan for the following sub-goal:\n\n        Sub-goal: {sub_goal}\n\n        Current World State:\n        {json.dumps(world_state, indent=2)}\n\n        Available Actions: navigate_to_object, pick_up, place, detect_object, ask_human, move_to\n\n        Create a step-by-step plan that:\n        1. Uses available actions\n        2. Specifies necessary parameters\n        3. Considers preconditions\n        4. Estimates expected outcomes\n\n        Provide your plan in the required JSON format.\n        """\n\n        return await self.llm.generate_with_schema(prompt, ActionPlan)\n\n    async def analyze_context(self, situation: str, goal: str) -> Optional[ContextAnalysis]:\n        """Analyze the current context for planning"""\n        prompt = f"""\n        Analyze the following situation in the context of achieving the specified goal:\n\n        Current Situation: {situation}\n        Goal: {goal}\n\n        Provide analysis considering:\n        1. What is the current state of affairs?\n        2. What objects or entities are relevant?\n        3. What environmental factors should be considered?\n        4. What constraints limit the possible approaches?\n        5. Are there opportunities for optimization or improvement?\n\n        Format your response in the required JSON structure.\n        """\n\n        return await self.llm.generate_with_schema(prompt, ContextAnalysis)\n\n    async def refine_task(self, task_description: str, execution_history: List[Dict] = None) -> Optional[TaskRefinement]:\n        """Refine a task based on execution history or current understanding"""\n        history_context = execution_history or []\n        prompt = f"""\n        Refine the following task description based on execution history:\n\n        Task: {task_description}\n        Execution History: {history_context}\n\n        Provide a more specific and actionable version of the task that:\n        1. Clarifies ambiguous elements\n        2. Identifies assumptions being made\n        3. Highlights information needed for better execution\n        4. Suggests alternative approaches if the original seems problematic\n\n        Return in the specified JSON format.\n        """\n\n        return await self.llm.generate_with_schema(prompt, TaskRefinement)\n\n    async def monitor_execution(self, current_state: Dict[str, Any],\n                              original_plan: List[Dict],\n                              executed_steps: List[Dict]) -> Optional[ExecutionMonitoring]:\n        """Monitor execution progress and suggest adaptations"""\n        prompt = f"""\n        Monitor the execution progress for the following plan:\n\n        Original Plan: {original_plan}\n        Executed Steps: {executed_steps}\n        Current State: {current_state}\n\n        Analyze:\n        1. What progress has been made?\n        2. What obstacles have been encountered?\n        3. What adaptations to the plan are needed?\n        4. How confident are we in achieving the goal?\n\n        Provide monitoring results in the required JSON format.\n        """\n\n        return await self.llm.generate_with_schema(prompt, ExecutionMonitoring)\n\n    async def create_comprehensive_plan(self, goal: str, world_state: Dict[str, Any]) -> Dict[str, Any]:\n        """Create a comprehensive plan using multiple LLM calls"""\n        # Step 1: Analyze context\n        context_analysis = await self.analyze_context(\n            f"Robot in {world_state.get(\'location\', \'unknown location\')} with objects {list(world_state.get(\'objects\', {}).keys())}",\n            goal\n        )\n\n        # Step 2: Decompose goal\n        goal_decomp = await self.decompose_goal(goal, {\n            "context_analysis": context_analysis.dict() if context_analysis else {},\n            "world_state": world_state\n        })\n\n        if not goal_decomp:\n            return {"success": False, "error": "Failed to decompose goal"}\n\n        # Step 3: Generate action plans for each sub-goal\n        action_plans = []\n        for sub_goal in goal_decomp.sub_goals:\n            action_plan = await self.generate_action_plan(sub_goal, world_state)\n            if action_plan:\n                action_plans.append(action_plan.dict())\n\n        # Step 4: Create comprehensive plan\n        comprehensive_plan = {\n            "main_goal": goal,\n            "decomposition": goal_decomp.dict(),\n            "action_plans": action_plans,\n            "context_analysis": context_analysis.dict() if context_analysis else {},\n            "created_at": time.time()\n        }\n\n        return {"success": True, "plan": comprehensive_plan}\n\n# Advanced planning utilities\nclass PlanningUtilities:\n    """Utility functions for cognitive planning"""\n\n    @staticmethod\n    def calculate_plan_confidence(steps: List[Dict]) -> float:\n        """Calculate overall confidence in a plan based on step confidences"""\n        if not steps:\n            return 0.0\n\n        confidences = [step.get(\'confidence\', 0.5) for step in steps]\n        # Use geometric mean for more conservative estimate\n        product = 1.0\n        for conf in confidences:\n            product *= conf\n        return product ** (1.0 / len(confidences))\n\n    @staticmethod\n    def detect_conflicts_in_plan(plan: Dict[str, Any]) -> List[str]:\n        """Detect potential conflicts in a plan"""\n        conflicts = []\n        steps = plan.get(\'action_plans\', [])\n\n        for i, step1 in enumerate(steps):\n            for j, step2 in enumerate(steps[i+1:], i+1):\n                # Check for resource conflicts\n                step1_resources = step1.get(\'parameters\', {}).get(\'arm\', \'default\')\n                step2_resources = step2.get(\'parameters\', {}).get(\'arm\', \'default\')\n\n                if step1_resources == step2_resources and step1_resources != \'default\':\n                    conflicts.append(f"Resource conflict: steps {i} and {j} both need {step1_resources}")\n\n        return conflicts\n\n    @staticmethod\n    def optimize_plan_order(plan: Dict[str, Any]) -> Dict[str, Any]:\n        """Optimize the order of plan steps"""\n        # This is a simplified optimization\n        # In practice, this would use more sophisticated algorithms\n        optimized_plan = plan.copy()\n        steps = optimized_plan.get(\'action_plans\', [])\n\n        # Sort by priority (if available) or by estimated duration\n        # For now, just return the plan as is\n        return optimized_plan\n\n# Example of LLM integration\nasync def example_llm_integration():\n    """Example of using LLM for cognitive planning"""\n    # Initialize LLM interface (requires actual API key)\n    llm_interface = LLMInterface(api_key="your-api-key-here")\n    planner = LLMCognitivePlanner(llm_interface)\n\n    # Example goal\n    goal = "Bring a red cup from the kitchen table to the living room couch"\n\n    # Example world state\n    world_state = {\n        "location": "kitchen",\n        "objects": {\n            "red_cup": {"location": "kitchen_table", "properties": {"color": "red", "graspable": True}},\n            "kitchen_table": {"location": "kitchen_center", "properties": {"surface": True}},\n            "living_room_couch": {"location": "living_room_center", "properties": {"furniture": True}}\n        },\n        "robot_state": {\n            "location": "kitchen_entrance",\n            "available_arms": ["left", "right"],\n            "battery_level": 0.8\n        }\n    }\n\n    # Create comprehensive plan\n    result = await planner.create_comprehensive_plan(goal, world_state)\n\n    if result["success"]:\n        plan = result["plan"]\n        print(f"Created plan for: {plan[\'main_goal\']}")\n        print(f"Decomposed into {len(plan[\'decomposition\'][\'sub_goals\'])} sub-goals")\n        print(f"Generated {len(plan[\'action_plans\'])} action plans")\n\n        # Calculate plan confidence\n        confidence = PlanningUtilities.calculate_plan_confidence(plan[\'action_plans\'])\n        print(f"Plan confidence: {confidence:.2f}")\n\n        # Check for conflicts\n        conflicts = PlanningUtilities.detect_conflicts_in_plan(plan)\n        if conflicts:\n            print(f"Detected conflicts: {conflicts}")\n        else:\n            print("No conflicts detected in plan")\n\n        return plan\n    else:\n        print(f"Failed to create plan: {result[\'error\']}")\n        return None\n'})}),"\n",(0,s.jsx)(n.h2,{id:"planning-algorithms",children:"Planning Algorithms"}),"\n",(0,s.jsx)(n.p,{children:"Planning algorithms for LLM-based cognitive planning must balance the reasoning capabilities of language models with the practical constraints of robotic execution. The following implementation demonstrates various planning approaches."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import heapq\nfrom typing import List, Dict, Any, Tuple, Optional, Set\nimport networkx as nx\nfrom dataclasses import dataclass\nimport numpy as np\n\n@dataclass\nclass PlanNode:\n    """Node in a planning graph"""\n    state: Dict[str, Any]\n    actions: List[str]\n    cost: float\n    heuristic: float\n    parent: Optional[\'PlanNode\'] = None\n\n    def __lt__(self, other):\n        return (self.cost + self.heuristic) < (other.cost + other.heuristic)\n\nclass HierarchicalTaskNetworkPlanner:\n    """Hierarchical Task Network (HTN) planner using LLM guidance"""\n\n    def __init__(self, llm_planner: LLMCognitivePlanner):\n        self.llm_planner = llm_planner\n        self.task_networks = {}  # Store predefined task networks\n\n    def add_task_network(self, task_name: str, subtasks: List[Dict[str, Any]]):\n        """Add a predefined task network"""\n        self.task_networks[task_name] = subtasks\n\n    async def plan_with_hierarchical_decomposition(self, goal: str, world_state: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n        """Plan using hierarchical decomposition guided by LLM"""\n        # Use LLM to decompose the high-level goal\n        goal_decomp = await self.llm_planner.decompose_goal(goal, world_state)\n\n        if not goal_decomp:\n            return None\n\n        # Build hierarchical plan\n        plan = []\n        for i, sub_goal in enumerate(goal_decomp.sub_goals):\n            # Generate action plan for each sub-goal\n            action_plan = await self.llm_planner.generate_action_plan(sub_goal, world_state)\n\n            if action_plan:\n                # Add to overall plan\n                for step in action_plan.steps:\n                    step[\'sub_goal_idx\'] = i\n                    step[\'original_sub_goal\'] = sub_goal\n                    plan.append(step)\n\n        return plan\n\nclass GraphBasedPlanner:\n    """Graph-based planner for complex cognitive tasks"""\n\n    def __init__(self):\n        self.graph = nx.DiGraph()\n\n    def add_state_transition(self, from_state: str, to_state: str, action: str, cost: float = 1.0):\n        """Add a state transition to the planning graph"""\n        self.graph.add_edge(from_state, to_state, action=action, cost=cost)\n\n    def find_optimal_path(self, start_state: str, goal_state: str) -> Optional[List[Tuple[str, str, Dict]]]:\n        """Find optimal path using Dijkstra\'s algorithm"""\n        try:\n            path = nx.shortest_path(self.graph, start_state, goal_state, weight=\'cost\')\n            edges = []\n            for i in range(len(path) - 1):\n                edge_data = self.graph[path[i]][path[i+1]]\n                edges.append((path[i], path[i+1], edge_data))\n            return edges\n        except nx.NetworkXNoPath:\n            return None\n\nclass AStarPlanner:\n    """A* planner with LLM-heuristic guidance"""\n\n    def __init__(self, llm_planner: LLMCognitivePlanner):\n        self.llm_planner = llm_planner\n\n    async def heuristic_estimate(self, current_state: Dict[str, Any], goal: str) -> float:\n        """Use LLM to estimate heuristic distance to goal"""\n        prompt = f"""\n        Estimate how close the following state is to achieving the goal.\n        State: {current_state}\n        Goal: {goal}\n\n        Return a number between 0 and 1, where:\n        - 0 means the goal is achieved\n        - 1 means very far from the goal\n        - Numbers in between represent relative distance\n        """\n\n        response = await self.llm_planner.llm.generate_text(prompt, temperature=0.1)\n\n        try:\n            # Extract number from response\n            match = re.search(r\'(\\d+\\.?\\d*)\', response)\n            if match:\n                value = float(match.group(1))\n                return min(1.0, max(0.0, value))  # Clamp between 0 and 1\n        except:\n            pass\n\n        # Default fallback\n        return 0.5\n\n    async def plan(self, start_state: Dict[str, Any], goal: str,\n                   max_steps: int = 100) -> Optional[List[Dict[str, Any]]]:\n        """Plan using A* algorithm with LLM heuristic"""\n        start_node = PlanNode(\n            state=start_state,\n            actions=[],\n            cost=0.0,\n            heuristic=await self.heuristic_estimate(start_state, goal)\n        )\n\n        open_set = [start_node]\n        closed_set: Set[str] = set()\n\n        step_count = 0\n        while open_set and step_count < max_steps:\n            current = heapq.heappop(open_set)\n            state_key = str(current.state)  # Simple state keying\n\n            if state_key in closed_set:\n                continue\n\n            closed_set.add(state_key)\n\n            # Check if goal is reached (simplified)\n            if await self._is_goal_reached(current.state, goal):\n                return current.actions\n\n            # Generate successor states\n            successors = await self._generate_successors(current.state, current.actions)\n\n            for next_state, action_taken in successors:\n                next_state_key = str(next_state)\n                if next_state_key in closed_set:\n                    continue\n\n                new_cost = current.cost + 1.0  # Uniform cost for simplicity\n                heuristic = await self.heuristic_estimate(next_state, goal)\n\n                next_node = PlanNode(\n                    state=next_state,\n                    actions=current.actions + [action_taken],\n                    cost=new_cost,\n                    heuristic=heuristic,\n                    parent=current\n                )\n\n                heapq.heappush(open_set, next_node)\n\n            step_count += 1\n\n        return None  # No path found\n\n    async def _is_goal_reached(self, state: Dict[str, Any], goal: str) -> bool:\n        """Check if the goal has been reached"""\n        # This would be more sophisticated in practice\n        # For now, we\'ll use a simple check\n        prompt = f"""\n        Determine if the following state represents achievement of the goal.\n        State: {state}\n        Goal: {goal}\n\n        Respond with \'true\' if goal is achieved, \'false\' otherwise.\n        """\n\n        response = await self.llm_planner.llm.generate_text(prompt, temperature=0.0)\n        return \'true\' in response.lower()\n\n    async def _generate_successors(self, state: Dict[str, Any],\n                                 current_actions: List[str]) -> List[Tuple[Dict[str, Any], str]]:\n        """Generate successor states for planning"""\n        # In a real system, this would interface with action execution\n        # For simulation, we\'ll generate some possible state transitions\n        successors = []\n\n        # Simulate possible actions\n        possible_actions = ["move_forward", "turn_left", "turn_right", "pick_up", "place"]\n\n        for action in possible_actions:\n            # Simulate state change based on action\n            new_state = state.copy()\n            new_state[f"last_action"] = action\n            new_state[f"action_count"] = len(current_actions) + 1\n\n            successors.append((new_state, action))\n\n        return successors\n\nclass ReactivePlanner:\n    """Reactive planner for handling unexpected situations"""\n\n    def __init__(self):\n        self.reactive_rules = []\n\n    def add_reactive_rule(self, condition: Callable[[Dict], bool],\n                         action: Callable[[Dict], Dict]):\n        """Add a reactive rule: if condition, then action"""\n        self.reactive_rules.append((condition, action))\n\n    def execute_with_reactivity(self, plan: List[Dict[str, Any]],\n                              initial_state: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n        """Execute plan with reactive behavior"""\n        current_state = initial_state.copy()\n        executed_actions = []\n        step = 0\n\n        for action in plan:\n            # Check reactive rules before executing each action\n            for condition, rule_action in self.reactive_rules:\n                if condition(current_state):\n                    # Execute reactive action\n                    current_state = rule_action(current_state)\n                    executed_actions.append({\n                        "type": "reactive",\n                        "action": rule_action.__name__,\n                        "step": step,\n                        "state_before": current_state.copy()\n                    })\n\n            # Execute planned action\n            # In a real system, this would interface with robot execution\n            current_state["last_executed"] = action\n            executed_actions.append({\n                "type": "planned",\n                "action": action,\n                "step": step,\n                "state_after": current_state.copy()\n            })\n            step += 1\n\n        return executed_actions, current_state\n\nclass MultiModalPlanner:\n    """Multi-modal planner combining symbolic and neural approaches"""\n\n    def __init__(self, llm_planner: LLMCognitivePlanner):\n        self.llm_planner = llm_planner\n        self.symbolic_planner = GraphBasedPlanner()\n        self.neural_components = {}  # Would include neural networks for perception, etc.\n\n    async def create_hybrid_plan(self, goal: str, world_state: Dict[str, Any]) -> Dict[str, Any]:\n        """Create a hybrid plan combining different planning approaches"""\n        # Use LLM for high-level reasoning\n        goal_analysis = await self.llm_planner.decompose_goal(goal, world_state)\n\n        if not goal_analysis:\n            return {"success": False, "error": "LLM goal analysis failed"}\n\n        # Use graph-based planner for detailed path planning\n        # This is a simplified example\n        self.symbolic_planner.add_state_transition("start", "approach_object", "navigate", 1.0)\n        self.symbolic_planner.add_state_transition("approach_object", "grasp_object", "pick_up", 1.0)\n        self.symbolic_planner.add_state_transition("grasp_object", "move_to_destination", "navigate", 1.0)\n        self.symbolic_planner.add_state_transition("move_to_destination", "place_object", "place", 1.0)\n\n        path = self.symbolic_planner.find_optimal_path("start", "place_object")\n\n        # Combine LLM analysis with symbolic plan\n        hybrid_plan = {\n            "llm_analysis": goal_analysis.dict(),\n            "symbolic_path": path,\n            "execution_strategy": "interleaved",\n            "monitoring_plan": True\n        }\n\n        return {"success": True, "plan": hybrid_plan}\n\n# Example of using different planning algorithms\nasync def example_planning_algorithms():\n    """Example of using different planning algorithms"""\n    # Initialize LLM interface\n    llm_interface = LLMInterface(api_key="your-api-key-here")\n    llm_planner = LLMCognitivePlanner(llm_interface)\n\n    # Example goal and world state\n    goal = "Navigate to kitchen, pick up a cup, and bring it to the table"\n    world_state = {\n        "robot_location": "living_room",\n        "objects": {\n            "cup": {"location": "kitchen_counter", "graspable": True},\n            "kitchen_counter": {"location": "kitchen", "surface": True},\n            "table": {"location": "dining_area", "surface": True}\n        }\n    }\n\n    # HTN Planner\n    htn_planner = HierarchicalTaskNetworkPlanner(llm_planner)\n    htn_plan = await htn_planner.plan_with_hierarchical_decomposition(goal, world_state)\n    print(f"HTN Plan: {len(htn_plan) if htn_plan else 0} steps")\n\n    # A* Planner\n    astar_planner = AStarPlanner(llm_planner)\n    astar_plan = await astar_planner.plan(world_state, goal)\n    print(f"A* Plan: {len(astar_plan) if astar_plan else 0} steps")\n\n    # Multi-modal Planner\n    multi_planner = MultiModalPlanner(llm_planner)\n    multi_plan = await multi_planner.create_hybrid_plan(goal, world_state)\n    print(f"Multi-modal plan created: {multi_plan[\'success\']}")\n\n    # Reactive planner example\n    reactive_planner = ReactivePlanner()\n\n    # Add a simple reactive rule\n    def obstacle_detected(state):\n        return state.get("obstacle_ahead", False)\n\n    def avoid_obstacle(state):\n        state["navigation_strategy"] = "avoidance_mode"\n        state["obstacle_avoided"] = True\n        return state\n\n    reactive_planner.add_reactive_rule(obstacle_detected, avoid_obstacle)\n\n    # Simulate execution with reactivity\n    simple_plan = [{"action": "move_forward"}, {"action": "turn_right"}, {"action": "move_forward"}]\n    initial_state = {"obstacle_ahead": True, "position": [0, 0]}\n    executed, final_state = reactive_planner.execute_with_reactivity(simple_plan, initial_state)\n\n    print(f"Reactive execution: {len(executed)} actions executed")\n    print(f"Final state: {final_state}")\n\n    return {\n        "htn_plan": htn_plan,\n        "astar_plan": astar_plan,\n        "multi_plan": multi_plan,\n        "reactive_execution": (executed, final_state)\n    }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"context-understanding",children:"Context Understanding"}),"\n",(0,s.jsx)(n.p,{children:"Context understanding is crucial for effective cognitive planning, as it allows the system to interpret goals and generate appropriate plans based on the current situation, environment, and constraints."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from typing import Dict, List, Any, Optional, Set\nimport datetime\nfrom dataclasses import dataclass\nimport spacy\nfrom transformers import pipeline\n\n@dataclass\nclass ContextFrame:\n    """A frame representing context at a specific time"""\n    timestamp: datetime.datetime\n    spatial_context: Dict[str, Any]  # Location, objects, layout\n    temporal_context: Dict[str, Any]  # Time of day, schedule, deadlines\n    social_context: Dict[str, Any]  # People present, social norms, relationships\n    task_context: Dict[str, Any]  # Current task, subtasks, progress\n    resource_context: Dict[str, Any]  # Available resources, constraints, capabilities\n\nclass ContextExtractor:\n    """Extract context from various sources"""\n\n    def __init__(self):\n        # Load NLP models\n        try:\n            self.nlp = spacy.load("en_core_web_sm")\n        except OSError:\n            print("spaCy model not found. Install with: python -m spacy download en_core_web_sm")\n            self.nlp = None\n\n        # Initialize other models\n        self.entailment_model = pipeline(\n            "text-classification",\n            model="cross-encoder/nli-deberta-base"\n        ) if False else None  # Disabled to avoid dependency issues\n\n    def extract_from_text(self, text: str) -> Dict[str, Any]:\n        """Extract context information from text"""\n        if not self.nlp:\n            return self._simple_text_extraction(text)\n\n        doc = self.nlp(text)\n\n        context = {\n            "entities": [],\n            "relations": [],\n            "temporal_expressions": [],\n            "spatial_expressions": [],\n            "intent": self._extract_intent(doc)\n        }\n\n        # Extract named entities\n        for ent in doc.ents:\n            context["entities"].append({\n                "text": ent.text,\n                "label": ent.label_,\n                "start": ent.start_char,\n                "end": ent.end_char\n            })\n\n        # Extract temporal expressions\n        for token in doc:\n            if token.ent_type_ in ["TIME", "DATE"]:\n                context["temporal_expressions"].append({\n                    "text": token.text,\n                    "type": token.ent_type_\n                })\n\n        # Extract spatial expressions\n        spatial_keywords = ["kitchen", "living room", "bedroom", "bathroom", "table", "chair", "couch", "door", "window"]\n        for token in doc:\n            if token.lemma_.lower() in spatial_keywords:\n                context["spatial_expressions"].append(token.text)\n\n        return context\n\n    def _simple_text_extraction(self, text: str) -> Dict[str, Any]:\n        """Simple text extraction without NLP models"""\n        import re\n\n        # Extract time expressions\n        time_patterns = [\n            r\'\\d{1,2}:\\d{2}\',  # HH:MM\n            r\'(morning|afternoon|evening|night)\',  # Time of day\n            r\'(today|tomorrow|yesterday|now)\'  # Relative time\n        ]\n\n        times = []\n        for pattern in time_patterns:\n            matches = re.findall(pattern, text, re.IGNORECASE)\n            times.extend(matches)\n\n        # Extract spatial expressions\n        spatial_keywords = ["kitchen", "living room", "bedroom", "bathroom", "table", "chair", "couch", "door", "window"]\n        spaces = [word for word in text.lower().split() if word in spatial_keywords]\n\n        # Extract potential objects\n        object_keywords = ["cup", "bottle", "book", "phone", "keys", "water", "milk", "bread", "apple"]\n        objects = [word for word in text.lower().split() if word in object_keywords]\n\n        return {\n            "entities": objects,\n            "relations": [],\n            "temporal_expressions": times,\n            "spatial_expressions": spaces,\n            "intent": self._simple_intent_extraction(text)\n        }\n\n    def _simple_intent_extraction(self, text: str) -> str:\n        """Simple intent extraction from text"""\n        text_lower = text.lower()\n\n        if any(word in text_lower for word in ["go", "move", "navigate", "walk", "come"]):\n            return "navigation"\n        elif any(word in text_lower for word in ["pick", "get", "take", "grasp", "hold"]):\n            return "manipulation"\n        elif any(word in text_lower for word in ["bring", "deliver", "carry", "transport"]):\n            return "transportation"\n        elif any(word in text_lower for word in ["find", "look", "search", "locate"]):\n            return "search"\n        else:\n            return "unknown"\n\n    def extract_from_world_state(self, world_state: Dict[str, Any]) -> Dict[str, Any]:\n        """Extract context from world state"""\n        context = {\n            "spatial": {\n                "robot_location": world_state.get("robot_location", "unknown"),\n                "objects": list(world_state.get("objects", {}).keys()),\n                "rooms": world_state.get("rooms", []),\n                "navigation_map": world_state.get("navigation_map", {})\n            },\n            "temporal": {\n                "current_time": world_state.get("current_time", time.time()),\n                "battery_level": world_state.get("robot_state", {}).get("battery_level", 1.0),\n                "last_action_time": world_state.get("robot_state", {}).get("last_action_time", 0)\n            },\n            "social": {\n                "people_present": world_state.get("people", []),\n                "interaction_history": world_state.get("interaction_history", [])\n            },\n            "task": {\n                "current_task": world_state.get("current_task", {}),\n                "task_progress": world_state.get("task_progress", 0.0),\n                "subtasks": world_state.get("subtasks", [])\n            }\n        }\n        return context\n\nclass ContextIntegrator:\n    """Integrate multiple context sources"""\n\n    def __init__(self):\n        self.context_extractor = ContextExtractor()\n        self.context_frames = []  # Store recent context frames\n        self.max_context_history = 10\n\n    def integrate_context(self, goal: str, world_state: Dict[str, Any],\n                        user_input: str = None) -> ContextFrame:\n        """Integrate context from multiple sources"""\n        timestamp = datetime.datetime.now()\n\n        # Extract context from different sources\n        goal_context = self.context_extractor.extract_from_text(goal)\n        world_context = self.context_extractor.extract_from_world_state(world_state)\n        user_context = self.context_extractor.extract_from_text(user_input) if user_input else {}\n\n        # Combine contexts\n        spatial_context = {**world_context["spatial"], "goal_relevant_spaces": goal_context.get("spatial_expressions", [])}\n        temporal_context = {**world_context["temporal"], "goal_temporal": goal_context.get("temporal_expressions", [])}\n        social_context = world_context["social"]\n        task_context = {**world_context["task"], "goal_intent": goal_context.get("intent", "unknown")}\n        resource_context = {\n            "capabilities": world_state.get("robot_state", {}).get("capabilities", []),\n            "constraints": world_state.get("constraints", []),\n            "available_objects": list(world_state.get("objects", {}).keys())\n        }\n\n        # Create context frame\n        context_frame = ContextFrame(\n            timestamp=timestamp,\n            spatial_context=spatial_context,\n            temporal_context=temporal_context,\n            social_context=social_context,\n            task_context=task_context,\n            resource_context=resource_context\n        )\n\n        # Store in history\n        self.context_frames.append(context_frame)\n        if len(self.context_frames) > self.max_context_history:\n            self.context_frames.pop(0)\n\n        return context_frame\n\n    def get_relevant_context(self, goal: str, time_window: float = 300) -> ContextFrame:\n        """Get context relevant to a specific goal within a time window"""\n        current_time = datetime.datetime.now()\n        recent_frames = [\n            frame for frame in self.context_frames\n            if (current_time - frame.timestamp).total_seconds() < time_window\n        ]\n\n        if not recent_frames:\n            # Return empty context frame if no recent frames\n            return ContextFrame(\n                timestamp=current_time,\n                spatial_context={},\n                temporal_context={},\n                social_context={},\n                task_context={},\n                resource_context={}\n            )\n\n        # Merge context frames (simplified approach)\n        merged_spatial = {}\n        merged_temporal = {}\n        merged_social = {}\n        merged_task = {}\n        merged_resources = {}\n\n        for frame in recent_frames:\n            merged_spatial.update(frame.spatial_context)\n            merged_temporal.update(frame.temporal_context)\n            merged_social.update(frame.social_context)\n            merged_task.update(frame.task_context)\n            merged_resources.update(frame.resource_context)\n\n        return ContextFrame(\n            timestamp=current_time,\n            spatial_context=merged_spatial,\n            temporal_context=merged_temporal,\n            social_context=merged_social,\n            task_context=merged_task,\n            resource_context=merged_resources\n        )\n\n    def update_context_with_execution(self, action_result: Dict[str, Any]):\n        """Update context based on action execution results"""\n        current_frame = self.context_frames[-1] if self.context_frames else None\n        if not current_frame:\n            return\n\n        # Update based on action result\n        if action_result.get("success", False):\n            # Update world state based on successful action\n            action_type = action_result.get("action", {}).get("name", "")\n            if action_type == "pick_up":\n                obj_name = action_result.get("action", {}).get("parameters", {}).get("object_name")\n                if obj_name and obj_name in current_frame.resource_context.get("available_objects", []):\n                    current_frame.resource_context["available_objects"].remove(obj_name)\n                    current_frame.resource_context["held_objects"] = current_frame.resource_context.get("held_objects", [])\n                    current_frame.resource_context["held_objects"].append(obj_name)\n\n        # Update timestamp\n        current_frame.timestamp = datetime.datetime.now()\n\nclass ContextAwarePlanner:\n    """Planning system that uses context information"""\n\n    def __init__(self, llm_planner: LLMCognitivePlanner):\n        self.llm_planner = llm_planner\n        self.context_integrator = ContextIntegrator()\n\n    async def plan_with_context(self, goal: str, world_state: Dict[str, Any],\n                              user_input: str = None) -> Dict[str, Any]:\n        """Plan using integrated context information"""\n        # Integrate context\n        context_frame = self.context_integrator.integrate_context(goal, world_state, user_input)\n\n        # Create context-aware prompt\n        context_description = self._describe_context(context_frame)\n\n        prompt = f"""\n        Plan to achieve the following goal considering the provided context:\n\n        Goal: {goal}\n\n        Context:\n        {context_description}\n\n        World State:\n        {json.dumps(world_state, indent=2)}\n\n        Generate a detailed plan that:\n        1. Takes into account the current context\n        2. Adapts to the environment and constraints\n        3. Considers the social situation\n        4. Uses available resources effectively\n\n        Provide your plan in the standard format.\n        """\n\n        # Use LLM to generate context-aware plan\n        response = await self.llm_planner.llm.generate_text(prompt)\n\n        # Parse the response (simplified)\n        plan = {\n            "goal": goal,\n            "context_aware_plan": response,\n            "context_used": context_description,\n            "timestamp": time.time()\n        }\n\n        return plan\n\n    def _describe_context(self, context_frame: ContextFrame) -> str:\n        """Create a textual description of the context"""\n        description = f"""\n        Spatial Context:\n        - Robot Location: {context_frame.spatial_context.get(\'robot_location\', \'unknown\')}\n        - Available Objects: {context_frame.spatial_context.get(\'objects\', [])}\n        - Rooms: {context_frame.spatial_context.get(\'rooms\', [])}\n\n        Temporal Context:\n        - Current Time: {context_frame.temporal_context.get(\'current_time\', \'unknown\')}\n        - Battery Level: {context_frame.temporal_context.get(\'battery_level\', \'unknown\')}\n\n        Social Context:\n        - People Present: {context_frame.social_context.get(\'people_present\', [])}\n        - Interaction History: {len(context_frame.social_context.get(\'interaction_history\', []))} interactions\n\n        Task Context:\n        - Current Task: {context_frame.task_context.get(\'current_task\', {})}\n        - Task Progress: {context_frame.task_context.get(\'task_progress\', 0.0)}\n\n        Resource Context:\n        - Capabilities: {context_frame.resource_context.get(\'capabilities\', [])}\n        - Constraints: {context_frame.resource_context.get(\'constraints\', [])}\n        - Available Objects: {context_frame.resource_context.get(\'available_objects\', [])}\n        """\n\n        return description\n\n    def adapt_plan_to_context(self, original_plan: Dict[str, Any],\n                            new_context: ContextFrame) -> Dict[str, Any]:\n        """Adapt an existing plan based on new context"""\n        # This would involve more sophisticated adaptation logic\n        # For now, we\'ll just return the original plan with context info\n        adapted_plan = original_plan.copy()\n        adapted_plan["adaptation_context"] = self._describe_context(new_context)\n        adapted_plan["adaptation_timestamp"] = time.time()\n\n        return adapted_plan\n\n# Example of context understanding\nasync def example_context_understanding():\n    """Example of context understanding in cognitive planning"""\n    # Initialize systems\n    llm_interface = LLMInterface(api_key="your-api-key-here")\n    llm_planner = LLMCognitivePlanner(llm_interface)\n    context_planner = ContextAwarePlanner(llm_planner)\n\n    # Example scenario\n    goal = "Bring coffee to John in the living room"\n    world_state = {\n        "robot_location": "kitchen",\n        "objects": {\n            "coffee": {"location": "kitchen_counter", "properties": {"hot": True, "graspable": True}},\n            "living_room": {"location": "living_room", "properties": {"accessible": True}}\n        },\n        "people": ["John", "Mary"],\n        "current_time": time.time(),\n        "robot_state": {\n            "battery_level": 0.7,\n            "capabilities": ["navigation", "manipulation"],\n            "location": "kitchen"\n        }\n    }\n    user_input = "John is waiting in the living room for his coffee"\n\n    # Plan with context\n    plan = await context_planner.plan_with_context(goal, world_state, user_input)\n    print(f"Context-aware plan generated")\n    print(f"Context considered: {len(plan.get(\'context_aware_plan\', \'\'))} characters of reasoning")\n\n    # Simulate context change\n    new_context = context_planner.context_integrator.get_relevant_context(goal)\n\n    # Adapt plan to new context\n    adapted_plan = context_planner.adapt_plan_to_context(plan, new_context)\n    print(f"Plan adapted to new context")\n\n    return {\n        "original_plan": plan,\n        "adapted_plan": adapted_plan\n    }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"task-decomposition",children:"Task Decomposition"}),"\n",(0,s.jsx)(n.p,{children:"Task decomposition is a critical component of cognitive planning, breaking down complex goals into manageable subtasks that can be executed sequentially or in parallel."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from typing import Dict, List, Any, Optional, Union\nimport asyncio\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport networkx as nx\n\nclass TaskStatus(Enum):\n    """Status of a task in the decomposition"""\n    PENDING = "pending"\n    IN_PROGRESS = "in_progress"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    SKIPPED = "skipped"\n\n@dataclass\nclass TaskNode:\n    """A node representing a task in the decomposition tree"""\n    id: str\n    description: str\n    subtasks: List[\'TaskNode\']\n    dependencies: List[str]  # IDs of tasks that must be completed first\n    status: TaskStatus = TaskStatus.PENDING\n    priority: int = 0  # Lower number means higher priority\n    estimated_duration: float = 1.0  # In seconds\n    confidence: float = 0.5  # Confidence in successful completion\n    resources_required: List[str] = None  # Resources needed for this task\n    effects: List[str] = None  # Effects of completing this task\n\nclass TaskDecomposer:\n    """System for decomposing tasks into subtasks using LLM guidance"""\n\n    def __init__(self, llm_planner: LLMCognitivePlanner):\n        self.llm_planner = llm_planner\n\n    async def decompose_task(self, task_description: str,\n                           context: Dict[str, Any] = None) -> TaskNode:\n        """Decompose a task into subtasks using LLM"""\n        prompt = f"""\n        Decompose the following task into smaller, manageable subtasks:\n\n        Task: {task_description}\n\n        Context: {context or \'No additional context provided\'}\n\n        Consider:\n        1. What are the logical steps required?\n        2. Which steps depend on others?\n        3. What resources are needed for each step?\n        4. What are the expected outcomes of each step?\n        5. How long might each step take?\n\n        Provide the decomposition as a hierarchical structure with:\n        - Task descriptions\n        - Dependencies between tasks\n        - Estimated duration for each task\n        - Required resources\n        - Expected effects of completing each task\n        - Priority ranking (1-5, where 1 is highest priority)\n\n        Format as JSON with structure:\n        {{\n            "task_id": "unique_id",\n            "description": "task description",\n            "subtasks": [subtask_objects],\n            "dependencies": ["dependency_task_ids"],\n            "estimated_duration": number_in_seconds,\n            "resources_required": ["resource1", "resource2"],\n            "effects": ["effect1", "effect2"],\n            "priority": 1-5\n        }}\n        """\n\n        response = await self.llm_planner.llm.generate_text(prompt)\n\n        # Parse the response (simplified - in practice, use structured output)\n        try:\n            task_data = json.loads(response)\n            return self._create_task_node(task_data)\n        except json.JSONDecodeError:\n            # If parsing fails, create a simple task node\n            return TaskNode(\n                id="task_0",\n                description=task_description,\n                subtasks=[],\n                dependencies=[],\n                priority=1,\n                estimated_duration=1.0,\n                confidence=0.5,\n                resources_required=[],\n                effects=[]\n            )\n\n    def _create_task_node(self, task_data: Dict[str, Any]) -> TaskNode:\n        """Create a TaskNode from data dictionary"""\n        subtasks = [self._create_task_node(sub) for sub in task_data.get(\'subtasks\', [])]\n\n        return TaskNode(\n            id=task_data.get(\'task_id\', f"task_{hash(task_data.get(\'description\', \'\')) % 10000}"),\n            description=task_data.get(\'description\', \'\'),\n            subtasks=subtasks,\n            dependencies=task_data.get(\'dependencies\', []),\n            priority=task_data.get(\'priority\', 3),\n            estimated_duration=task_data.get(\'estimated_duration\', 1.0),\n            confidence=task_data.get(\'confidence\', 0.5),\n            resources_required=task_data.get(\'resources_required\', []),\n            effects=task_data.get(\'effects\', [])\n        )\n\n    def build_task_graph(self, root_task: TaskNode) -> nx.DiGraph:\n        """Build a dependency graph from the task decomposition"""\n        graph = nx.DiGraph()\n\n        def add_task_to_graph(task: TaskNode):\n            graph.add_node(task.id, task=task, description=task.description)\n\n            # Add dependency edges\n            for dep_id in task.dependencies:\n                graph.add_edge(dep_id, task.id, type=\'dependency\')\n\n            # Recursively add subtasks\n            for subtask in task.subtasks:\n                add_task_to_graph(subtask)\n                graph.add_edge(task.id, subtask.id, type=\'subtask\')\n\n        add_task_to_graph(root_task)\n        return graph\n\n    def get_execution_order(self, task_graph: nx.DiGraph) -> List[str]:\n        """Get the order in which tasks should be executed"""\n        try:\n            # Use topological sort to get execution order\n            execution_order = list(nx.topological_sort(task_graph))\n            return execution_order\n        except nx.NetworkXUnfeasible:\n            # If there are cycles, return an approximate order\n            # In practice, you\'d want to resolve the cycles first\n            return list(task_graph.nodes())\n\n    def prioritize_tasks(self, tasks: List[TaskNode],\n                        resource_availability: Dict[str, int]) -> List[TaskNode]:\n        """Prioritize tasks based on dependencies, resources, and urgency"""\n        # Sort by priority (lower number = higher priority) and dependencies\n        def task_priority(task: TaskNode):\n            # Higher priority score for higher priority level\n            priority_score = -task.priority\n\n            # Adjust based on resource availability\n            resource_penalty = 0\n            for resource in task.resources_required or []:\n                available = resource_availability.get(resource, 0)\n                if available <= 0:\n                    resource_penalty += 10  # Heavy penalty for unavailable resources\n\n            # Adjust based on dependencies (tasks with fewer unmet dependencies get higher priority)\n            # This is a simplified version\n            return priority_score + resource_penalty\n\n        return sorted(tasks, key=task_priority)\n\nclass HierarchicalTaskDecomposer(TaskDecomposer):\n    """Advanced task decomposer with hierarchical capabilities"""\n\n    def __init__(self, llm_planner: LLMCognitivePlanner):\n        super().__init__(llm_planner)\n        self.decomposition_cache = {}\n\n    async def decompose_with_context(self, task_description: str,\n                                   context_frame: ContextFrame) -> TaskNode:\n        """Decompose task considering the current context"""\n        context_info = self._extract_context_info(context_frame)\n\n        prompt = f"""\n        Decompose the following task considering the provided context:\n\n        Task: {task_description}\n\n        Context Information:\n        - Spatial Context: {context_info[\'spatial\']}\n        - Temporal Context: {context_info[\'temporal\']}\n        - Social Context: {context_info[\'social\']}\n        - Resource Context: {context_info[\'resources\']}\n\n        Create a detailed decomposition that:\n        1. Takes spatial layout into account\n        2. Considers time constraints\n        3. Respects social norms and people present\n        4. Uses available resources efficiently\n        5. Minimizes travel and maximizes efficiency\n\n        Provide the decomposition in the same JSON format as before.\n        """\n\n        response = await self.llm_planner.llm.generate_text(prompt)\n\n        try:\n            task_data = json.loads(response)\n            return self._create_task_node(task_data)\n        except json.JSONDecodeError:\n            # Fallback to simple decomposition\n            return await self.decompose_task(task_description)\n\n    def _extract_context_info(self, context_frame: ContextFrame) -> Dict[str, Any]:\n        """Extract relevant information from context frame"""\n        return {\n            "spatial": {\n                "robot_location": context_frame.spatial_context.get(\'robot_location\'),\n                "available_objects": context_frame.spatial_context.get(\'objects\', []),\n                "navigation_map": context_frame.spatial_context.get(\'navigation_map\', {})\n            },\n            "temporal": {\n                "current_time": context_frame.temporal_context.get(\'current_time\'),\n                "battery_level": context_frame.temporal_context.get(\'battery_level\'),\n                "time_deadline": context_frame.temporal_context.get(\'deadline\', None)\n            },\n            "social": {\n                "people_present": context_frame.social_context.get(\'people_present\', []),\n                "social_norms": context_frame.social_context.get(\'norms\', [])\n            },\n            "resources": {\n                "available_capabilities": context_frame.resource_context.get(\'capabilities\', []),\n                "resource_constraints": context_frame.resource_context.get(\'constraints\', []),\n                "available_objects": context_frame.resource_context.get(\'available_objects\', [])\n            }\n        }\n\n    async def decompose_with_learning(self, task_description: str,\n                                    context_frame: ContextFrame) -> TaskNode:\n        """Decompose task with learning from previous similar tasks"""\n        # Check if we have similar tasks in cache\n        cache_key = self._create_cache_key(task_description, context_frame)\n\n        if cache_key in self.decomposition_cache:\n            cached_decomp = self.decomposition_cache[cache_key]\n            # Adapt cached decomposition to current context\n            return await self._adapt_cached_decomposition(cached_decomp, context_frame)\n\n        # If not in cache, decompose normally\n        decomposition = await self.decompose_with_context(task_description, context_frame)\n\n        # Store in cache\n        self.decomposition_cache[cache_key] = decomposition\n\n        return decomposition\n\n    def _create_cache_key(self, task_description: str, context_frame: ContextFrame) -> str:\n        """Create a cache key for the task decomposition"""\n        import hashlib\n        context_summary = (\n            f"loc:{context_frame.spatial_context.get(\'robot_location\', \'unknown\')}_"\n            f"obj:{len(context_frame.spatial_context.get(\'objects\', []))}_"\n            f"ppl:{len(context_frame.social_context.get(\'people_present\', []))}"\n        )\n        full_key = f"{task_description}_{context_summary}"\n        return hashlib.md5(full_key.encode()).hexdigest()\n\n    async def _adapt_cached_decomposition(self, cached_decomp: TaskNode,\n                                       context_frame: ContextFrame) -> TaskNode:\n        """Adapt a cached decomposition to current context"""\n        # This would involve more sophisticated adaptation\n        # For now, we\'ll just return the cached decomposition\n        return cached_decomp\n\nclass ParallelTaskScheduler:\n    """Scheduler for executing tasks in parallel when possible"""\n\n    def __init__(self):\n        self.resource_manager = ResourceManager()\n\n    def schedule_parallel_tasks(self, task_graph: nx.DiGraph,\n                              available_resources: Dict[str, int]) -> List[List[str]]:\n        """Schedule tasks for parallel execution"""\n        execution_levels = []\n        remaining_tasks = set(task_graph.nodes())\n\n        while remaining_tasks:\n            # Find tasks that can be executed at this level\n            current_level = []\n\n            for task_id in list(remaining_tasks):\n                task_node = task_graph.nodes[task_id][\'task\']\n\n                # Check if all dependencies are satisfied\n                dependencies_met = all(\n                    dep not in remaining_tasks\n                    for dep in task_node.dependencies\n                )\n\n                # Check if resources are available\n                resources_available = self.resource_manager.check_resources_available(\n                    task_node.resources_required, available_resources\n                )\n\n                if dependencies_met and resources_available:\n                    current_level.append(task_id)\n\n            if not current_level:\n                # No tasks can be scheduled, which might indicate a problem\n                print(f"Warning: Could not schedule any tasks, remaining: {remaining_tasks}")\n                break\n\n            # Mark these tasks as scheduled\n            for task_id in current_level:\n                remaining_tasks.remove(task_id)\n\n            execution_levels.append(current_level)\n\n        return execution_levels\n\nclass ResourceManager:\n    """Manage resources for task execution"""\n\n    def __init__(self):\n        self.resource_allocations = {}\n\n    def check_resources_available(self, required_resources: List[str],\n                               available_resources: Dict[str, int]) -> bool:\n        """Check if required resources are available"""\n        if not required_resources:\n            return True\n\n        for resource in required_resources:\n            available = available_resources.get(resource, 0)\n            allocated = self.resource_allocations.get(resource, 0)\n            if (available - allocated) <= 0:\n                return False\n\n        return True\n\n    def allocate_resources(self, task_node: TaskNode):\n        """Allocate resources for a task"""\n        for resource in task_node.resources_required or []:\n            self.resource_allocations[resource] = self.resource_allocations.get(resource, 0) + 1\n\n    def release_resources(self, task_node: TaskNode):\n        """Release resources after task completion"""\n        for resource in task_node.resources_required or []:\n            current = self.resource_allocations.get(resource, 0)\n            if current > 0:\n                self.resource_allocations[resource] = current - 1\n\n# Example of task decomposition\nasync def example_task_decomposition():\n    """Example of task decomposition in cognitive planning"""\n    # Initialize systems\n    llm_interface = LLMInterface(api_key="your-api-key-here")\n    llm_planner = LLMCognitivePlanner(llm_interface)\n\n    # Create decomposer\n    decomposer = HierarchicalTaskDecomposer(llm_planner)\n\n    # Example task\n    task = "Prepare a simple meal consisting of sandwich and drink, then serve to the person in the living room"\n\n    # Create a context frame\n    context_frame = ContextFrame(\n        timestamp=datetime.datetime.now(),\n        spatial_context={\n            "robot_location": "kitchen",\n            "objects": ["bread", "cheese", "ham", "bottle_water", "plate", "cup"],\n            "rooms": ["kitchen", "living_room"]\n        },\n        temporal_context={\n            "current_time": time.time(),\n            "battery_level": 0.8\n        },\n        social_context={\n            "people_present": ["John"],\n            "location": "living_room"\n        },\n        task_context={},\n        resource_context={\n            "capabilities": ["navigation", "manipulation", "detection"],\n            "available_objects": ["bread", "cheese", "ham", "bottle_water"]\n        }\n    )\n\n    # Decompose task with context\n    task_decomposition = await decomposer.decompose_with_context(task, context_frame)\n    print(f"Task decomposed into {len(task_decomposition.subtasks) if task_decomposition.subtasks else 0} subtasks")\n\n    # Build task graph\n    task_graph = decomposer.build_task_graph(task_decomposition)\n    print(f"Task dependency graph has {len(task_graph.nodes())} nodes and {len(task_graph.edges())} edges")\n\n    # Get execution order\n    execution_order = decomposer.get_execution_order(task_graph)\n    print(f"Execution order: {execution_order[:5]}...")  # Show first 5 tasks\n\n    # Schedule for parallel execution\n    scheduler = ParallelTaskScheduler()\n    available_resources = {"left_arm": 1, "right_arm": 1, "navigation_system": 1}\n    execution_levels = scheduler.schedule_parallel_tasks(task_graph, available_resources)\n    print(f"Tasks scheduled in {len(execution_levels)} parallel levels")\n\n    for i, level in enumerate(execution_levels):\n        print(f"  Level {i+1}: {len(level)} tasks")\n\n    return {\n        "decomposition": task_decomposition,\n        "graph": task_graph,\n        "execution_order": execution_order,\n        "execution_levels": execution_levels\n    }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"execution-monitoring",children:"Execution Monitoring"}),"\n",(0,s.jsx)(n.p,{children:"Execution monitoring is critical for cognitive planning systems, allowing them to track progress, detect failures, and adapt plans in real-time."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport time\nfrom typing import Dict, List, Any, Optional, Callable, Awaitable\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport threading\nfrom datetime import datetime\n\nclass ExecutionStatus(Enum):\n    """Status of plan execution"""\n    NOT_STARTED = "not_started"\n    RUNNING = "running"\n    PAUSED = "paused"\n    COMPLETED = "completed"\n    FAILED = "failed"\n    CANCELLED = "cancelled"\n\nclass MonitorEvent(Enum):\n    """Types of monitoring events"""\n    STEP_STARTED = "step_started"\n    STEP_COMPLETED = "step_completed"\n    STEP_FAILED = "step_failed"\n    PLAN_ADAPTED = "plan_adapted"\n    RESOURCE_CHANGED = "resource_changed"\n    CONTEXT_CHANGED = "context_changed"\n    GOAL_ACHIEVED = "goal_achieved"\n    GOAL_FAILED = "goal_failed"\n\n@dataclass\nclass ExecutionStep:\n    """Represents a single step in plan execution"""\n    id: str\n    action: Dict[str, Any]\n    status: ExecutionStatus\n    start_time: Optional[datetime] = None\n    end_time: Optional[datetime] = None\n    result: Optional[Dict[str, Any]] = None\n    error: Optional[str] = None\n    confidence: float = 0.5\n\n@dataclass\nclass ExecutionState:\n    """Current state of plan execution"""\n    status: ExecutionStatus\n    current_step: int\n    completed_steps: List[ExecutionStep]\n    failed_steps: List[ExecutionStep]\n    remaining_steps: List[Dict[str, Any]]\n    progress: float\n    start_time: datetime\n    last_update: datetime\n\nclass ExecutionMonitor:\n    """Monitor for plan execution with real-time feedback"""\n\n    def __init__(self, llm_planner: LLMCognitivePlanner):\n        self.llm_planner = llm_planner\n        self.execution_state = None\n        self.event_handlers = {}\n        self.is_monitoring = False\n        self.monitoring_thread = None\n        self.lock = threading.Lock()\n\n    def register_event_handler(self, event_type: MonitorEvent, handler: Callable):\n        """Register a handler for specific monitoring events"""\n        if event_type not in self.event_handlers:\n            self.event_handlers[event_type] = []\n        self.event_handlers[event_type].append(handler)\n\n    async def trigger_event(self, event_type: MonitorEvent, data: Dict[str, Any]):\n        """Trigger an event and notify all handlers"""\n        if event_type in self.event_handlers:\n            for handler in self.event_handlers[event_type]:\n                try:\n                    if asyncio.iscoroutinefunction(handler):\n                        await handler(event_type, data)\n                    else:\n                        handler(event_type, data)\n                except Exception as e:\n                    print(f"Error in event handler: {e}")\n\n    def start_monitoring(self, plan: List[Dict[str, Any]], initial_state: Dict[str, Any]):\n        """Start monitoring execution of a plan"""\n        with self.lock:\n            self.execution_state = ExecutionState(\n                status=ExecutionStatus.NOT_STARTED,\n                current_step=0,\n                completed_steps=[],\n                failed_steps=[],\n                remaining_steps=plan.copy(),\n                progress=0.0,\n                start_time=datetime.now(),\n                last_update=datetime.now()\n            )\n            self.is_monitoring = True\n\n    def stop_monitoring(self):\n        """Stop monitoring execution"""\n        with self.lock:\n            self.is_monitoring = False\n            if self.execution_state:\n                self.execution_state.status = ExecutionStatus.CANCELLED\n\n    async def monitor_step_execution(self, step_index: int, step: Dict[str, Any],\n                                  execute_callback: Callable) -> ExecutionStep:\n        """Monitor execution of a single step"""\n        step_monitor = ExecutionStep(\n            id=f"step_{step_index}",\n            action=step,\n            status=ExecutionStatus.NOT_STARTED\n        )\n\n        # Mark step as started\n        step_monitor.status = ExecutionStatus.RUNNING\n        step_monitor.start_time = datetime.now()\n\n        await self.trigger_event(MonitorEvent.STEP_STARTED, {\n            "step": step_monitor,\n            "execution_state": self.execution_state\n        })\n\n        try:\n            # Execute the step\n            result = await execute_callback(step)\n\n            # Mark as completed\n            step_monitor.status = ExecutionStatus.COMPLETED\n            step_monitor.end_time = datetime.now()\n            step_monitor.result = result\n\n            # Update execution state\n            with self.lock:\n                if self.execution_state:\n                    self.execution_state.completed_steps.append(step_monitor)\n                    self.execution_state.current_step += 1\n                    self.execution_state.progress = len(self.execution_state.completed_steps) / len(self.execution_state.completed_steps + self.execution_state.failed_steps + self.execution_state.remaining_steps)\n                    self.execution_state.last_update = datetime.now()\n\n            await self.trigger_event(MonitorEvent.STEP_COMPLETED, {\n                "step": step_monitor,\n                "execution_state": self.execution_state\n            })\n\n        except Exception as e:\n            # Mark as failed\n            step_monitor.status = ExecutionStatus.FAILED\n            step_monitor.end_time = datetime.now()\n            step_monitor.error = str(e)\n\n            # Update execution state\n            with self.lock:\n                if self.execution_state:\n                    self.execution_state.failed_steps.append(step_monitor)\n                    self.execution_state.current_step += 1\n                    self.execution_state.last_update = datetime.now()\n\n            await self.trigger_event(MonitorEvent.STEP_FAILED, {\n                "step": step_monitor,\n                "execution_state": self.execution_state,\n                "error": str(e)\n            })\n\n        return step_monitor\n\n    async def adapt_plan_during_execution(self, current_state: Dict[str, Any],\n                                       failed_step: ExecutionStep) -> Optional[List[Dict[str, Any]]]:\n        """Adapt the plan when a step fails"""\n        prompt = f"""\n        A step in the plan has failed. The failed step was:\n        {failed_step.action}\n\n        Error: {failed_step.error}\n\n        Current world state: {current_state}\n\n        Remaining steps in the plan: {self.execution_state.remaining_steps if self.execution_state else []}\n\n        Suggest adaptations to the plan to handle this failure. Consider:\n        1. Can the failed step be retried with different parameters?\n        2. Should the step be skipped if possible?\n        3. Does the failure require modifying subsequent steps?\n        4. Is there an alternative approach to achieve the same goal?\n\n        Provide an adapted plan in JSON format.\n        """\n\n        response = await self.llm_planner.llm.generate_text(prompt)\n\n        try:\n            adapted_plan = json.loads(response)\n            await self.trigger_event(MonitorEvent.PLAN_ADAPTED, {\n                "original_step": failed_step,\n                "adaptation": adapted_plan,\n                "execution_state": self.execution_state\n            })\n            return adapted_plan\n        except json.JSONDecodeError:\n            print(f"Failed to parse plan adaptation: {response}")\n            return None\n\n    def get_execution_progress(self) -> Dict[str, Any]:\n        """Get current execution progress"""\n        if not self.execution_state:\n            return {"status": "no_execution_active"}\n\n        with self.lock:\n            return {\n                "status": self.execution_state.status.value,\n                "current_step": self.execution_state.current_step,\n                "completed_steps": len(self.execution_state.completed_steps),\n                "failed_steps": len(self.execution_state.failed_steps),\n                "remaining_steps": len(self.execution_state.remaining_steps),\n                "progress_percentage": self.execution_state.progress * 100,\n                "execution_time": (datetime.now() - self.execution_state.start_time).total_seconds()\n            }\n\n    async def monitor_resource_usage(self, resource_callback: Callable):\n        """Monitor resource usage during execution"""\n        while self.is_monitoring:\n            try:\n                resource_status = await resource_callback()\n\n                await self.trigger_event(MonitorEvent.RESOURCE_CHANGED, {\n                    "resource_status": resource_status,\n                    "execution_state": self.execution_state\n                })\n\n                # Check if resources are critically low\n                if resource_status.get(\'battery_level\', 1.0) < 0.1:\n                    print("Warning: Battery level critically low")\n\n                await asyncio.sleep(1)  # Check every second\n\n            except Exception as e:\n                print(f"Error monitoring resources: {e}")\n                await asyncio.sleep(1)\n\nclass AdaptiveExecutionManager:\n    """Manager for adaptive plan execution with monitoring"""\n\n    def __init__(self, llm_planner: LLMCognitivePlanner):\n        self.llm_planner = llm_planner\n        self.monitor = ExecutionMonitor(llm_planner)\n        self.context_integrator = ContextIntegrator()\n\n    async def execute_plan_with_monitoring(self, plan: List[Dict[str, Any]],\n                                        world_state: Dict[str, Any],\n                                        execute_step_callback: Callable) -> Dict[str, Any]:\n        """Execute a plan with full monitoring and adaptation capabilities"""\n        # Start monitoring\n        self.monitor.start_monitoring(plan, world_state)\n\n        # Set up event handlers\n        self.monitor.register_event_handler(MonitorEvent.STEP_FAILED, self._handle_step_failure)\n        self.monitor.register_event_handler(MonitorEvent.CONTEXT_CHANGED, self._handle_context_change)\n\n        results = []\n        current_state = world_state.copy()\n\n        for i, step in enumerate(plan):\n            # Check if monitoring should continue\n            if not self.monitor.is_monitoring:\n                break\n\n            # Monitor step execution\n            step_result = await self.monitor.monitor_step_execution(i, step, execute_step_callback)\n            results.append(step_result)\n\n            # Update world state based on step result\n            if step_result.status == ExecutionStatus.COMPLETED and step_result.result:\n                current_state = self._update_world_state(current_state, step_result.result, step)\n\n            # Check for context changes\n            if i % 5 == 0:  # Check every 5 steps\n                await self._check_context_changes(current_state)\n\n        # Stop monitoring\n        self.monitor.stop_monitoring()\n\n        # Return execution summary\n        return {\n            "final_state": self.monitor.get_execution_progress(),\n            "results": results,\n            "success": all(r.status == ExecutionStatus.COMPLETED for r in results)\n        }\n\n    async def _handle_step_failure(self, event_type: MonitorEvent, data: Dict[str, Any]):\n        """Handle when a step fails during execution"""\n        failed_step = data[\'step\']\n        current_state = data.get(\'execution_state\')\n\n        print(f"Step {failed_step.id} failed: {failed_step.error}")\n\n        # Try to adapt the plan\n        adapted_plan = await self.monitor.adapt_plan_during_execution(current_state, failed_step)\n\n        if adapted_plan:\n            print(f"Plan adapted successfully")\n        else:\n            print(f"Could not adapt plan for failure")\n\n    async def _handle_context_change(self, event_type: MonitorEvent, data: Dict[str, Any]):\n        """Handle when context changes during execution"""\n        context_change = data.get(\'context_change\')\n        print(f"Context changed: {context_change}")\n\n    async def _check_context_changes(self, current_state: Dict[str, Any]):\n        """Check for context changes that might affect execution"""\n        # This would interface with perception systems in a real implementation\n        # For simulation, we\'ll just return\n        pass\n\n    def _update_world_state(self, current_state: Dict[str, Any],\n                           step_result: Dict[str, Any], action_taken: Dict[str, Any]) -> Dict[str, Any]:\n        """Update world state based on step execution result"""\n        new_state = current_state.copy()\n\n        # Update based on action effects\n        action_name = action_taken.get(\'name\', \'\')\n        if action_name == \'pick_up\':\n            obj_name = action_taken.get(\'parameters\', {}).get(\'object_name\')\n            if obj_name and obj_name in new_state.get(\'objects\', {}):\n                del new_state[\'objects\'][obj_name]\n                new_state[\'held_objects\'] = new_state.get(\'held_objects\', [])\n                new_state[\'held_objects\'].append(obj_name)\n\n        elif action_name == \'place\':\n            obj_name = action_taken.get(\'parameters\', {}).get(\'object_name\')\n            target_location = action_taken.get(\'parameters\', {}).get(\'target_location\')\n            if obj_name and obj_name in new_state.get(\'held_objects\', []):\n                new_state[\'held_objects\'].remove(obj_name)\n                new_state[\'objects\'][obj_name] = {\'location\': target_location}\n\n        # Update robot location if navigation occurred\n        if action_name in [\'move_to\', \'navigate_to_object\']:\n            target = action_taken.get(\'parameters\', {}).get(\'target_location\')\n            if target:\n                new_state[\'robot_location\'] = target\n\n        return new_state\n\n    async def execute_with_context_awareness(self, plan: List[Dict[str, Any]],\n                                           initial_world_state: Dict[str, Any],\n                                           execute_step_callback: Callable) -> Dict[str, Any]:\n        """Execute plan with context awareness and adaptation"""\n        # Integrate context\n        context_frame = self.context_integrator.integrate_context(\n            "execution_task", initial_world_state\n        )\n\n        # Execute with monitoring\n        result = await self.execute_plan_with_monitoring(\n            plan, initial_world_state, execute_step_callback\n        )\n\n        # Update context with execution results\n        self.context_integrator.update_context_with_execution({\n            "success": result["success"],\n            "steps_completed": len([r for r in result["results"] if r.status == ExecutionStatus.COMPLETED]),\n            "steps_failed": len([r for r in result["results"] if r.status == ExecutionStatus.FAILED])\n        })\n\n        return result\n\n# Example execution monitoring\nasync def example_execution_monitoring():\n    """Example of execution monitoring in cognitive planning"""\n    # Initialize systems\n    llm_interface = LLMInterface(api_key="your-api-key-here")\n    llm_planner = LLMCognitivePlanner(llm_interface)\n    execution_manager = AdaptiveExecutionManager(llm_planner)\n\n    # Example plan (simplified)\n    plan = [\n        {"name": "navigate_to", "parameters": {"location": "kitchen"}, "type": "navigation"},\n        {"name": "detect_object", "parameters": {"object": "cup"}, "type": "perception"},\n        {"name": "pick_up", "parameters": {"object": "cup"}, "type": "manipulation"},\n        {"name": "navigate_to", "parameters": {"location": "table"}, "type": "navigation"},\n        {"name": "place", "parameters": {"object": "cup", "location": "table"}, "type": "manipulation"}\n    ]\n\n    # Initial world state\n    world_state = {\n        "robot_location": "start",\n        "objects": {\n            "cup": {"location": "kitchen_counter", "graspable": True},\n            "kitchen_counter": {"location": "kitchen", "surface": True},\n            "table": {"location": "dining_area", "surface": True}\n        },\n        "robot_state": {\n            "battery_level": 0.8,\n            "arm_status": "free"\n        }\n    }\n\n    # Define step execution callback\n    async def execute_step(step: Dict[str, Any]) -> Dict[str, Any]:\n        """Simulate step execution"""\n        print(f"Executing: {step[\'name\']} with {step.get(\'parameters\', {})}")\n\n        # Simulate different execution times based on action type\n        action_times = {\n            "navigation": 2.0,\n            "perception": 1.0,\n            "manipulation": 1.5\n        }\n\n        sleep_time = action_times.get(step.get(\'type\', \'navigation\'), 1.0)\n        await asyncio.sleep(sleep_time * 0.1)  # Speed up for example\n\n        # Simulate occasional failures\n        import random\n        if random.random() < 0.1:  # 10% failure rate\n            raise Exception(f"Simulated failure in {step[\'name\']}")\n\n        return {"success": True, "result": f"completed_{step[\'name\']}"}\n\n    # Execute plan with monitoring\n    result = await execution_manager.execute_with_context_awareness(\n        plan, world_state, execute_step\n    )\n\n    print(f"Execution completed with success: {result[\'success\']}")\n    print(f"Steps completed: {len([r for r in result[\'results\'] if r.status == ExecutionStatus.COMPLETED])}")\n    print(f"Steps failed: {len([r for r in result[\'results\'] if r.status == ExecutionStatus.FAILED])}")\n\n    return result\n\n# Run the example\nasync def run_all_examples():\n    """Run all examples to demonstrate the cognitive planning system"""\n    print("=== LLM Cognitive Planning Examples ===\\n")\n\n    # Note: These examples require an actual OpenAI API key to run properly\n    # The code structure demonstrates the implementation approach\n\n    print("1. Basic Cognitive Planning:")\n    try:\n        # result1 = await example_cognitive_planning()\n        print("  - Cognitive planning structure demonstrated")\n    except Exception as e:\n        print(f"  - Example skipped due to API requirements: {e}")\n\n    print("\\n2. LLM Integration:")\n    try:\n        # result2 = await example_llm_integration()\n        print("  - LLM integration structure demonstrated")\n    except Exception as e:\n        print(f"  - Example skipped due to API requirements: {e}")\n\n    print("\\n3. Planning Algorithms:")\n    try:\n        # result3 = await example_planning_algorithms()\n        print("  - Planning algorithms structure demonstrated")\n    except Exception as e:\n        print(f"  - Example skipped due to API requirements: {e}")\n\n    print("\\n4. Context Understanding:")\n    try:\n        # result4 = await example_context_understanding()\n        print("  - Context understanding structure demonstrated")\n    except Exception as e:\n        print(f"  - Example skipped due to API requirements: {e}")\n\n    print("\\n5. Task Decomposition:")\n    try:\n        # result5 = await example_task_decomposition()\n        print("  - Task decomposition structure demonstrated")\n    except Exception as e:\n        print(f"  - Example skipped due to API requirements: {e}")\n\n    print("\\n6. Execution Monitoring:")\n    try:\n        # result6 = await example_execution_monitoring()\n        print("  - Execution monitoring structure demonstrated")\n    except Exception as e:\n        print(f"  - Example skipped due to API requirements: {e}")\n\n    print("\\n=== All examples completed ===")\n\nif __name__ == "__main__":\n    asyncio.run(run_all_examples())\n'})}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"LLM-based cognitive planning enables humanoid robots to perform complex, multi-step tasks by leveraging the reasoning capabilities of large language models. The implementation includes:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cognitive Planning Framework"}),": A comprehensive system that understands goals, analyzes world state, generates plans, and monitors execution with adaptation capabilities."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"LLM Integration"}),": Structured integration of large language models for goal understanding, plan generation, and context analysis with proper response parsing and validation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Planning Algorithms"}),": Multiple planning approaches including hierarchical task networks, graph-based planning, A* search with LLM heuristics, and reactive planning for handling unexpected situations."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Context Understanding"}),": Advanced context extraction and integration from multiple sources including text, world state, temporal factors, and social context."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task Decomposition"}),": Hierarchical task decomposition with dependency management, resource allocation, and parallel execution scheduling."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Execution Monitoring"}),": Real-time monitoring with event handling, failure adaptation, and context-aware plan adjustment during execution."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The system is designed to be robust and adaptive, capable of handling ambiguous natural language commands and adjusting to changing environmental conditions. The modular architecture allows for integration with different LLM providers and robotic platforms, making it suitable for a wide range of humanoid robot applications requiring sophisticated cognitive capabilities."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var a=t(6540);const s={},o=a.createContext(s);function i(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);