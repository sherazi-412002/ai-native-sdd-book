"use strict";(globalThis.webpackChunkclassic=globalThis.webpackChunkclassic||[]).push([[7764],{3008(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"module-3/synthetic-data","title":"Synthetic Data","description":"Generating Training Data with NVIDIA Isaac","source":"@site/docs/module-3/synthetic-data.md","sourceDirName":"module-3","slug":"/module-3/synthetic-data","permalink":"/ebsite/docs/docs/module-3/synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/sherazi-412002/ai-native-sdd-books/tree/main/docs/module-3/synthetic-data.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"curriculumSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/ebsite/docs/docs/category/module-3-the-ai-robot-brain-nvidia-isaac"},"next":{"title":"Isaac ROS VSLAM","permalink":"/ebsite/docs/docs/module-3/isaac-ros-vslam"}}');var i=t(4848),s=t(8453);const r={sidebar_position:1},o="Synthetic Data",l={},m=[{value:"Generating Training Data with NVIDIA Isaac",id:"generating-training-data-with-nvidia-isaac",level:2},{value:"Introduction to Synthetic Data",id:"introduction-to-synthetic-data",level:2},{value:"Isaac Sim for Data Generation",id:"isaac-sim-for-data-generation",level:2},{value:"Domain Randomization",id:"domain-randomization",level:2},{value:"Data Pipeline",id:"data-pipeline",level:2},{value:"Quality Assurance",id:"quality-assurance",level:2},{value:"Isaac Sim Setup Parameters",id:"isaac-sim-setup-parameters",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"synthetic-data",children:"Synthetic Data"})}),"\n",(0,i.jsx)(n.h2,{id:"generating-training-data-with-nvidia-isaac",children:"Generating Training Data with NVIDIA Isaac"}),"\n",(0,i.jsx)(n.p,{children:"This chapter covers generating synthetic data using NVIDIA Isaac for training AI models for humanoid robots. Synthetic data generation is a critical component of modern AI development, particularly for robotics applications where real-world data collection is expensive, time-consuming, and potentially dangerous. NVIDIA Isaac provides powerful tools for creating realistic synthetic datasets that can be used to train perception, navigation, and control systems for humanoid robots."}),"\n",(0,i.jsx)(n.p,{children:"The synthetic data pipeline combines high-fidelity physics simulation, realistic sensor models, and domain randomization techniques to create diverse training datasets that transfer effectively to real-world applications. This approach enables rapid development of robust AI models without requiring extensive real-world data collection."}),"\n",(0,i.jsx)(n.h2,{id:"introduction-to-synthetic-data",children:"Introduction to Synthetic Data"}),"\n",(0,i.jsx)(n.p,{children:"Synthetic data generation is the process of creating artificial data that mimics real-world observations. In robotics, this typically involves simulating sensors, environments, and robot behaviors to generate training data for machine learning models. For humanoid robots, synthetic data can include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"RGB images and depth maps"}),"\n",(0,i.jsx)(n.li,{children:"LiDAR point clouds"}),"\n",(0,i.jsx)(n.li,{children:"IMU readings"}),"\n",(0,i.jsx)(n.li,{children:"Joint position and velocity data"}),"\n",(0,i.jsx)(n.li,{children:"Force/torque sensor readings"}),"\n",(0,i.jsx)(n.li,{children:"Semantic segmentation masks"}),"\n",(0,i.jsx)(n.li,{children:"Instance segmentation annotations"}),"\n",(0,i.jsx)(n.li,{children:"Object detection bounding boxes"}),"\n",(0,i.jsx)(n.li,{children:"Human pose estimation data"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The advantages of synthetic data for humanoid robot development include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety"}),": Training can occur without physical robots"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost efficiency"}),": No need for expensive real-world data collection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Repeatability"}),": Exact same scenarios can be reproduced"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control"}),": Environmental conditions can be precisely controlled"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scalability"}),": Thousands of scenarios can be generated quickly"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety scenarios"}),": Dangerous situations can be safely simulated"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The key challenge in synthetic data generation is ensuring that the synthetic data is realistic enough to enable good transfer learning to real-world applications. This is achieved through domain randomization, high-fidelity physics simulation, and accurate sensor modeling."}),"\n",(0,i.jsx)(n.h2,{id:"isaac-sim-for-data-generation",children:"Isaac Sim for Data Generation"}),"\n",(0,i.jsx)(n.p,{children:"NVIDIA Isaac Sim provides a comprehensive platform for synthetic data generation with built-in tools for creating diverse, high-quality training datasets. The platform includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"High-fidelity physics simulation using PhysX"}),"\n",(0,i.jsx)(n.li,{children:"Realistic sensor models (cameras, LiDAR, IMU, etc.)"}),"\n",(0,i.jsx)(n.li,{children:"Domain randomization capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Synthetic data generation tools"}),"\n",(0,i.jsx)(n.li,{children:"Integration with Omniverse for collaborative workflows"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Here's an example of setting up synthetic data generation in Isaac Sim:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import omni\nimport carb\nimport omni.graph.core as og\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom pxr import Gf, Sdf, UsdGeom\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport json\nimport os\n\nclass IsaacSyntheticDataGenerator:\n    def __init__(self, world: World):\n        self.world = world\n        self.sd_helper = SyntheticDataHelper()\n        self.camera = None\n        self.data_buffer = []\n\n    def setup_camera(self, prim_path="/World/Camera", position=[0, 0, 1.5], rotation=[0, 0, 0]):\n        """Setup a camera for synthetic data capture"""\n        from omni.isaac.sensor import Camera\n\n        self.camera = Camera(\n            prim_path=prim_path,\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n        # Set camera pose\n        self.camera.set_world_pose(position, rotation)\n\n        return self.camera\n\n    def setup_lidar(self, prim_path="/World/Lidar", translation=[0, 0, 1.0]):\n        """Setup a LiDAR sensor for synthetic data capture"""\n        from omni.isaac.range_sensor import _range_sensor\n\n        lidar_interface = _range_sensor.acquire_lidar_sensor_interface()\n\n        # Create LiDAR sensor\n        lidar_config = {\n            "rotation_frequency": 20,\n            "number_of_channels": 16,\n            "points_per_second": 240000,\n            "horizontal_resolution": 4,\n            "vertical_resolution": 1,\n            "horizontal_lasers": 800,\n            "vertical_lasers": 16,\n            "range": [0.1, 25.0],\n            "rotation": [0, 0, 0]\n        }\n\n        lidar_interface.create_lidar_sensor(\n            prim_path,\n            translation,\n            lidar_config\n        )\n\n        return lidar_interface\n\n    def capture_rgb_data(self):\n        """Capture RGB image data from the camera"""\n        if self.camera:\n            rgb_data = self.camera.get_rgb()\n            return rgb_data\n        return None\n\n    def capture_depth_data(self):\n        """Capture depth data from the camera"""\n        if self.camera:\n            depth_data = self.camera.get_depth()\n            return depth_data\n        return None\n\n    def capture_segmentation_data(self):\n        """Capture semantic segmentation data"""\n        if self.camera:\n            segmentation_data = self.camera.get_semantic_segmentation()\n            return segmentation_data\n        return None\n\n    def capture_instance_data(self):\n        """Capture instance segmentation data"""\n        if self.camera:\n            instance_data = self.camera.get_instance_segmentation()\n            return instance_data\n        return None\n\n    def generate_training_data(self, num_samples=1000, output_dir="synthetic_data"):\n        """Generate synthetic training data"""\n        os.makedirs(output_dir, exist_ok=True)\n\n        for i in range(num_samples):\n            # Randomize environment\n            self.randomize_environment()\n\n            # Capture all sensor data\n            rgb = self.capture_rgb_data()\n            depth = self.capture_depth_data()\n            segmentation = self.capture_segmentation_data()\n            instance = self.capture_instance_data()\n\n            # Create sample data dictionary\n            sample = {\n                "sample_id": i,\n                "rgb": rgb,\n                "depth": depth,\n                "segmentation": segmentation,\n                "instance": instance,\n                "robot_state": self.get_robot_state(),\n                "environment_state": self.get_environment_state()\n            }\n\n            # Save sample to buffer\n            self.data_buffer.append(sample)\n\n            # Save individual files\n            self.save_sample(sample, os.path.join(output_dir, f"sample_{i:05d}"))\n\n            # Step simulation\n            self.world.step(render=True)\n\n            print(f"Generated sample {i+1}/{num_samples}")\n\n    def save_sample(self, sample, path):\n        """Save a single sample to disk"""\n        os.makedirs(path, exist_ok=True)\n\n        # Save RGB image\n        if sample["rgb"] is not None:\n            rgb_img = Image.fromarray(sample["rgb"])\n            rgb_img.save(os.path.join(path, "rgb.png"))\n\n        # Save depth image\n        if sample["depth"] is not None:\n            depth_img = Image.fromarray((sample["depth"] * 255).astype(np.uint8))\n            depth_img.save(os.path.join(path, "depth.png"))\n\n        # Save segmentation\n        if sample["segmentation"] is not None:\n            seg_img = Image.fromarray(sample["segmentation"])\n            seg_img.save(os.path.join(path, "segmentation.png"))\n\n        # Save instance segmentation\n        if sample["instance"] is not None:\n            inst_img = Image.fromarray(sample["instance"])\n            inst_img.save(os.path.join(path, "instance.png"))\n\n        # Save metadata\n        metadata = {\n            "sample_id": sample["sample_id"],\n            "robot_state": sample["robot_state"],\n            "environment_state": sample["environment_state"]\n        }\n\n        with open(os.path.join(path, "metadata.json"), "w") as f:\n            json.dump(metadata, f, indent=2)\n\n    def randomize_environment(self):\n        """Apply domain randomization to the environment"""\n        # Randomize lighting\n        self.randomize_lighting()\n\n        # Randomize object positions\n        self.randomize_object_positions()\n\n        # Randomize material properties\n        self.randomize_materials()\n\n        # Randomize camera parameters\n        self.randomize_camera_parameters()\n\n    def randomize_lighting(self):\n        """Randomize lighting conditions"""\n        # Get all lights in the scene\n        lights = self.world.scene.get_lights()\n\n        for light in lights:\n            # Randomize intensity\n            intensity = np.random.uniform(500, 2000)\n            light.set_attribute("intensity", intensity)\n\n            # Randomize color temperature\n            color_temp = np.random.uniform(3000, 8000)\n            light.set_attribute("color", self.color_temperature_to_rgb(color_temp))\n\n    def randomize_object_positions(self):\n        """Randomize positions of objects in the environment"""\n        # Get all rigid bodies in the scene\n        rigid_bodies = self.world.scene.get_rigid_bodies()\n\n        for body in rigid_bodies:\n            # Randomize position within bounds\n            pos = body.get_world_pose()[0]\n            new_pos = [\n                pos[0] + np.random.uniform(-0.5, 0.5),\n                pos[1] + np.random.uniform(-0.5, 0.5),\n                pos[2] + np.random.uniform(0, 0.5)\n            ]\n            body.set_world_pose(new_pos)\n\n    def randomize_materials(self):\n        """Randomize material properties"""\n        # This would involve changing material properties like albedo, roughness, etc.\n        # Implementation depends on the specific materials in the scene\n        pass\n\n    def randomize_camera_parameters(self):\n        """Randomize camera intrinsic and extrinsic parameters"""\n        if self.camera:\n            # Randomize camera pose\n            pos = self.camera.get_world_pose()[0]\n            new_pos = [\n                pos[0] + np.random.uniform(-0.1, 0.1),\n                pos[1] + np.random.uniform(-0.1, 0.1),\n                pos[2] + np.random.uniform(-0.1, 0.1)\n            ]\n            self.camera.set_world_pose(new_pos)\n\n    def color_temperature_to_rgb(self, color_temp):\n        """Convert color temperature to RGB values"""\n        temp = color_temp / 100\n        red = 0\n        green = 0\n        blue = 0\n\n        if temp <= 66:\n            red = 255\n        else:\n            red = temp - 60\n            red = 329.698727446 * (red ** -0.1332047592)\n            red = max(0, min(255, red))\n\n        if temp <= 66:\n            green = temp\n            green = 99.4708025861 * np.log(green) - 161.1195681661\n        else:\n            green = temp - 60\n            green = 288.1221695283 * (green ** -0.0755148492)\n\n        green = max(0, min(255, green))\n\n        if temp >= 66:\n            blue = 255\n        elif temp <= 19:\n            blue = 0\n        else:\n            blue = temp - 10\n            blue = 138.5177312231 * np.log(blue) - 305.0447927307\n            blue = max(0, min(255, blue))\n\n        return [red/255, green/255, blue/255]\n\n    def get_robot_state(self):\n        """Get current robot state"""\n        # This would return the current state of the robot\n        # including joint positions, velocities, etc.\n        return {}\n\n    def get_environment_state(self):\n        """Get current environment state"""\n        # This would return the current state of the environment\n        # including object positions, lighting, etc.\n        return {}\n\n# Example usage\ndef setup_synthetic_data_pipeline():\n    """Setup and run the synthetic data pipeline"""\n    # Initialize Isaac Sim world\n    world = World(stage_units_in_meters=1.0)\n\n    # Add a simple environment\n    world.scene.add_default_ground_plane()\n\n    # Create synthetic data generator\n    generator = IsaacSyntheticDataGenerator(world)\n\n    # Setup camera\n    camera = generator.setup_camera(\n        prim_path="/World/Camera",\n        position=[2.0, 0.0, 1.5],\n        rotation=[0, 0, 0]\n    )\n\n    # Run the simulation\n    world.reset()\n\n    # Generate synthetic data\n    generator.generate_training_data(\n        num_samples=100,\n        output_dir="synthetic_humanoid_data"\n    )\n\n    return generator\n'})}),"\n",(0,i.jsx)(n.h2,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,i.jsx)(n.p,{children:"Domain randomization is a technique used in synthetic data generation to improve the transfer of models trained on synthetic data to real-world applications. The core idea is to randomize various aspects of the simulation environment to make the model robust to variations in real-world conditions."}),"\n",(0,i.jsx)(n.p,{children:"The key aspects of domain randomization include:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Visual Domain Randomization"}),": Randomizing colors, textures, lighting, and camera parameters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Physical Domain Randomization"}),": Randomizing physical properties like friction, mass, and dynamics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Geometric Domain Randomization"}),": Randomizing object shapes, sizes, and positions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Temporal Domain Randomization"}),": Randomizing timing and dynamics"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Here's an implementation of domain randomization for humanoid robot synthetic data:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport random\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Optional\nimport omni\nfrom pxr import Gf, Sdf, UsdGeom\n\n@dataclass\nclass DomainRandomizationConfig:\n    """Configuration for domain randomization parameters"""\n    # Visual domain randomization\n    lighting_intensity_range: Tuple[float, float] = (500, 2000)\n    color_temperature_range: Tuple[float, float] = (3000, 8000)\n    albedo_range: Tuple[float, float] = (0.1, 1.0)\n    roughness_range: Tuple[float, float] = (0.0, 1.0)\n    metallic_range: Tuple[float, float] = (0.0, 1.0)\n\n    # Physical domain randomization\n    friction_range: Tuple[float, float] = (0.1, 1.0)\n    mass_multiplier_range: Tuple[float, float] = (0.8, 1.2)\n    gravity_range: Tuple[float, float] = (-9.9, -9.7)\n\n    # Geometric domain randomization\n    object_position_jitter: float = 0.5\n    object_rotation_jitter: float = 0.2\n    object_scale_range: Tuple[float, float] = (0.8, 1.2)\n\n    # Camera domain randomization\n    camera_position_jitter: float = 0.1\n    camera_rotation_jitter: float = 0.05\n    camera_fov_jitter: float = 0.1\n\n    # Sensor domain randomization\n    sensor_noise_range: Tuple[float, float] = (0.0, 0.01)\n    sensor_bias_range: Tuple[float, float] = (-0.01, 0.01)\n\nclass DomainRandomizer:\n    """Handles domain randomization for synthetic data generation"""\n\n    def __init__(self, config: DomainRandomizationConfig):\n        self.config = config\n        self.random_state = np.random.RandomState()\n\n    def randomize_lighting(self, light_prims: List[str]):\n        """Randomize lighting conditions"""\n        for prim_path in light_prims:\n            # Randomize intensity\n            intensity = self.random_state.uniform(\n                self.config.lighting_intensity_range[0],\n                self.config.lighting_intensity_range[1]\n            )\n\n            # Randomize color temperature\n            color_temp = self.random_state.uniform(\n                self.config.color_temperature_range[0],\n                self.config.color_temperature_range[1]\n            )\n\n            # Convert color temperature to RGB\n            color_rgb = self.color_temperature_to_rgb(color_temp)\n\n            # Apply changes to the light\n            light_prim = omni.usd.get_context().get_stage().GetPrimAtPath(prim_path)\n            if light_prim:\n                light_prim.GetAttribute("inputs:intensity").Set(intensity)\n                light_prim.GetAttribute("inputs:color").Set(Gf.Vec3f(*color_rgb))\n\n    def randomize_materials(self, material_prims: List[str]):\n        """Randomize material properties"""\n        for prim_path in material_prims:\n            # Randomize albedo\n            albedo = self.random_state.uniform(\n                self.config.albedo_range[0],\n                self.config.albedo_range[1]\n            )\n\n            # Randomize roughness\n            roughness = self.random_state.uniform(\n                self.config.roughness_range[0],\n                self.config.roughness_range[1]\n            )\n\n            # Randomize metallic\n            metallic = self.random_state.uniform(\n                self.config.metallic_range[0],\n                self.config.metallic_range[1]\n            )\n\n            # Apply changes to the material\n            material_prim = omni.usd.get_context().get_stage().GetPrimAtPath(prim_path)\n            if material_prim:\n                # This is pseudocode - actual implementation depends on material setup\n                if material_prim.HasAttribute("albedo"):\n                    material_prim.GetAttribute("albedo").Set(albedo)\n                if material_prim.HasAttribute("roughness"):\n                    material_prim.GetAttribute("roughness").Set(roughness)\n                if material_prim.HasAttribute("metallic"):\n                    material_prim.GetAttribute("metallic").Set(metallic)\n\n    def randomize_physics(self, rigid_body_prims: List[str]):\n        """Randomize physical properties"""\n        for prim_path in rigid_body_prims:\n            # Randomize friction\n            friction = self.random_state.uniform(\n                self.config.friction_range[0],\n                self.config.friction_range[1]\n            )\n\n            # Randomize mass\n            mass_multiplier = self.random_state.uniform(\n                self.config.mass_multiplier_range[0],\n                self.config.mass_multiplier_range[1]\n            )\n\n            # Apply changes to the rigid body\n            rigid_body_prim = omni.usd.get_context().get_stage().GetPrimAtPath(prim_path)\n            if rigid_body_prim:\n                # Apply friction\n                if rigid_body_prim.HasAttribute("friction"):\n                    rigid_body_prim.GetAttribute("friction").Set(friction)\n\n                # Apply mass multiplier\n                if rigid_body_prim.HasAttribute("mass"):\n                    current_mass = rigid_body_prim.GetAttribute("mass").Get()\n                    new_mass = current_mass * mass_multiplier\n                    rigid_body_prim.GetAttribute("mass").Set(new_mass)\n\n    def randomize_geometry(self, object_prims: List[str]):\n        """Randomize geometric properties"""\n        for prim_path in object_prims:\n            # Get current position\n            current_pos = self.get_current_position(prim_path)\n\n            # Add random jitter to position\n            new_pos = [\n                current_pos[0] + self.random_state.uniform(\n                    -self.config.object_position_jitter,\n                    self.config.object_position_jitter\n                ),\n                current_pos[1] + self.random_state.uniform(\n                    -self.config.object_position_jitter,\n                    self.config.object_position_jitter\n                ),\n                current_pos[2] + self.random_state.uniform(\n                    0,  # Only allow upward jitter for stability\n                    self.config.object_position_jitter\n                )\n            ]\n\n            # Add random rotation\n            rotation = [\n                self.random_state.uniform(\n                    -self.config.object_rotation_jitter,\n                    self.config.object_rotation_jitter\n                ),\n                self.random_state.uniform(\n                    -self.config.object_rotation_jitter,\n                    self.config.object_rotation_jitter\n                ),\n                self.random_state.uniform(\n                    -self.config.object_rotation_jitter,\n                    self.config.object_rotation_jitter\n                )\n            ]\n\n            # Apply random scale\n            scale = self.random_state.uniform(\n                self.config.object_scale_range[0],\n                self.config.object_scale_range[1]\n            )\n\n            # Apply changes to the object\n            self.set_object_transform(prim_path, new_pos, rotation, scale)\n\n    def randomize_camera(self, camera_prim_path: str):\n        """Randomize camera parameters"""\n        # Get current camera position\n        current_pos = self.get_current_position(camera_prim_path)\n\n        # Add random jitter to position\n        new_pos = [\n            current_pos[0] + self.random_state.uniform(\n                -self.config.camera_position_jitter,\n                self.config.camera_position_jitter\n            ),\n            current_pos[1] + self.random_state.uniform(\n                -self.config.camera_position_jitter,\n                self.config.camera_position_jitter\n            ),\n            current_pos[2] + self.random_state.uniform(\n                -self.config.camera_position_jitter,\n                self.config.camera_position_jitter\n            )\n        ]\n\n        # Add random rotation\n        rotation = [\n            self.random_state.uniform(\n                -self.config.camera_rotation_jitter,\n                self.config.camera_rotation_jitter\n            ),\n            self.random_state.uniform(\n                -self.config.camera_rotation_jitter,\n                self.config.camera_rotation_jitter\n            ),\n            self.random_state.uniform(\n                -self.config.camera_rotation_jitter,\n                self.config.camera_rotation_jitter\n            )\n        ]\n\n        # Apply changes to the camera\n        self.set_camera_transform(camera_prim_path, new_pos, rotation)\n\n        # Randomize field of view\n        current_fov = self.get_camera_fov(camera_prim_path)\n        new_fov = current_fov + self.random_state.uniform(\n            -self.config.camera_fov_jitter,\n            self.config.camera_fov_jitter\n        )\n        self.set_camera_fov(camera_prim_path, new_fov)\n\n    def randomize_sensors(self, sensor_prims: List[str]):\n        """Randomize sensor parameters"""\n        for prim_path in sensor_prims:\n            # Add random noise\n            noise = self.random_state.uniform(\n                self.config.sensor_noise_range[0],\n                self.config.sensor_noise_range[1]\n            )\n\n            # Add random bias\n            bias = self.random_state.uniform(\n                self.config.sensor_bias_range[0],\n                self.config.sensor_bias_range[1]\n            )\n\n            # Apply changes to the sensor\n            sensor_prim = omni.usd.get_context().get_stage().GetPrimAtPath(prim_path)\n            if sensor_prim:\n                if sensor_prim.HasAttribute("noise"):\n                    sensor_prim.GetAttribute("noise").Set(noise)\n                if sensor_prim.HasAttribute("bias"):\n                    sensor_prim.GetAttribute("bias").Set(bias)\n\n    def get_current_position(self, prim_path: str) -> List[float]:\n        """Get the current position of a prim"""\n        prim = omni.usd.get_context().get_stage().GetPrimAtPath(prim_path)\n        if prim:\n            # This is pseudocode - actual implementation depends on prim type\n            xform_api = UsdGeom.Xformable(prim)\n            transform = xform_api.ComputeLocalToWorldTransform(0)\n            pos = transform.ExtractTranslation()\n            return [pos[0], pos[1], pos[2]]\n        return [0.0, 0.0, 0.0]\n\n    def set_object_transform(self, prim_path: str, position: List[float],\n                           rotation: List[float], scale: float):\n        """Set the transform of an object"""\n        prim = omni.usd.get_context().get_stage().GetPrimAtPath(prim_path)\n        if prim:\n            xform_api = UsdGeom.Xformable(prim)\n            xform_ops = xform_api.GetOrderedXformOps()\n\n            # Apply position, rotation, and scale\n            for op in xform_ops:\n                if op.GetOpType() == UsdGeom.XformOp.TypeTranslate:\n                    op.Set(Gf.Vec3d(*position))\n                elif op.GetOpType() == UsdGeom.XformOp.TypeRotateXYZ:\n                    op.Set(Gf.Vec3d(*rotation))\n                elif op.GetOpType() == UsdGeom.XformOp.TypeScale:\n                    op.Set(Gf.Vec3d(scale, scale, scale))\n\n    def set_camera_transform(self, prim_path: str, position: List[float], rotation: List[float]):\n        """Set the transform of a camera"""\n        # Implementation similar to set_object_transform\n        self.set_object_transform(prim_path, position, rotation, 1.0)\n\n    def get_camera_fov(self, prim_path: str) -> float:\n        """Get the field of view of a camera"""\n        # This is pseudocode - actual implementation depends on camera setup\n        return 60.0\n\n    def set_camera_fov(self, prim_path: str, fov: float):\n        """Set the field of view of a camera"""\n        # This is pseudocode - actual implementation depends on camera setup\n        pass\n\n    def color_temperature_to_rgb(self, color_temp: float) -> List[float]:\n        """Convert color temperature to RGB values"""\n        temp = color_temp / 100\n        red = 0\n        green = 0\n        blue = 0\n\n        if temp <= 66:\n            red = 255\n        else:\n            red = temp - 60\n            red = 329.698727446 * (red ** -0.1332047592)\n            red = max(0, min(255, red))\n\n        if temp <= 66:\n            green = temp\n            green = 99.4708025861 * np.log(green) - 161.1195681661\n        else:\n            green = temp - 60\n            green = 288.1221695283 * (green ** -0.0755148492)\n\n        green = max(0, min(255, green))\n\n        if temp >= 66:\n            blue = 255\n        elif temp <= 19:\n            blue = 0\n        else:\n            blue = temp - 10\n            blue = 138.5177312231 * np.log(blue) - 305.0447927307\n            blue = max(0, min(255, blue))\n\n        return [red/255, green/255, blue/255]\n\n# Example usage of domain randomization\ndef apply_domain_randomization():\n    """Apply domain randomization to the simulation"""\n    # Create domain randomization configuration\n    config = DomainRandomizationConfig(\n        lighting_intensity_range=(300, 2500),\n        color_temperature_range=(2500, 9000),\n        object_position_jitter=0.8,\n        object_rotation_jitter=0.3\n    )\n\n    # Create domain randomizer\n    randomizer = DomainRandomizer(config)\n\n    # Apply randomization to different aspects of the scene\n    light_prims = ["/World/DirectionalLight", "/World/SpotLight"]\n    material_prims = ["/World/Materials/RobotMaterial", "/World/Materials/EnvironmentMaterial"]\n    rigid_body_prims = ["/World/Robot/Body", "/World/Environment/Object1"]\n    object_prims = ["/World/Environment/Object1", "/World/Environment/Object2"]\n    camera_prim = "/World/Camera"\n    sensor_prims = ["/World/Sensors/IMU", "/World/Sensors/Camera"]\n\n    # Apply domain randomization\n    randomizer.randomize_lighting(light_prims)\n    randomizer.randomize_materials(material_prims)\n    randomizer.randomize_physics(rigid_body_prims)\n    randomizer.randomize_geometry(object_prims)\n    randomizer.randomize_camera(camera_prim)\n    randomizer.randomize_sensors(sensor_prims)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"data-pipeline",children:"Data Pipeline"}),"\n",(0,i.jsx)(n.p,{children:"The synthetic data pipeline for humanoid robots involves several stages from simulation setup to final dataset generation. Here's a comprehensive implementation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import os\nimport json\nimport numpy as np\nfrom PIL import Image\nimport cv2\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Optional, Callable\nimport threading\nimport queue\nimport time\nfrom pathlib import Path\n\n@dataclass\nclass DataSample:\n    """Data structure for a single synthetic data sample"""\n    sample_id: int\n    rgb_image: np.ndarray\n    depth_image: np.ndarray\n    segmentation_mask: np.ndarray\n    instance_mask: np.ndarray\n    robot_state: Dict\n    environment_state: Dict\n    sensor_data: Dict\n    metadata: Dict\n\nclass SyntheticDataPipeline:\n    """Complete pipeline for generating synthetic data for humanoid robots"""\n\n    def __init__(self, output_dir: str = "synthetic_data",\n                 buffer_size: int = 100,\n                 num_workers: int = 4):\n        self.output_dir = Path(output_dir)\n        self.buffer_size = buffer_size\n        self.num_workers = num_workers\n\n        # Create output directories\n        self.rgb_dir = self.output_dir / "rgb"\n        self.depth_dir = self.output_dir / "depth"\n        self.seg_dir = self.output_dir / "segmentation"\n        self.inst_dir = self.output_dir / "instance"\n        self.meta_dir = self.output_dir / "metadata"\n\n        for dir_path in [self.rgb_dir, self.depth_dir, self.seg_dir, self.inst_dir, self.meta_dir]:\n            dir_path.mkdir(parents=True, exist_ok=True)\n\n        # Queues for pipeline stages\n        self.simulation_queue = queue.Queue(maxsize=buffer_size)\n        self.processing_queue = queue.Queue(maxsize=buffer_size)\n        self.save_queue = queue.Queue(maxsize=buffer_size)\n\n        # Statistics\n        self.stats = {\n            "samples_generated": 0,\n            "samples_processed": 0,\n            "samples_saved": 0,\n            "generation_rate": 0,\n            "processing_rate": 0,\n            "save_rate": 0\n        }\n\n        # Workers\n        self.simulation_workers = []\n        self.processing_workers = []\n        self.save_workers = []\n\n        # Control flags\n        self.running = False\n\n    def start_pipeline(self):\n        """Start the synthetic data generation pipeline"""\n        self.running = True\n\n        # Start simulation workers\n        for i in range(self.num_workers):\n            worker = threading.Thread(\n                target=self._simulation_worker,\n                args=(i,),\n                daemon=True\n            )\n            worker.start()\n            self.simulation_workers.append(worker)\n\n        # Start processing workers\n        for i in range(self.num_workers):\n            worker = threading.Thread(\n                target=self._processing_worker,\n                args=(i,),\n                daemon=True\n            )\n            worker.start()\n            self.processing_workers.append(worker)\n\n        # Start save workers\n        for i in range(self.num_workers):\n            worker = threading.Thread(\n                target=self._save_worker,\n                args=(i,),\n                daemon=True\n            )\n            worker.start()\n            self.save_workers.append(worker)\n\n        print("Synthetic data pipeline started")\n\n    def stop_pipeline(self):\n        """Stop the synthetic data generation pipeline"""\n        self.running = False\n\n        # Wait for all workers to finish\n        for worker in self.simulation_workers:\n            worker.join()\n\n        for worker in self.processing_workers:\n            worker.join()\n\n        for worker in self.save_workers:\n            worker.join()\n\n        print("Synthetic data pipeline stopped")\n        print(f"Statistics: {self.stats}")\n\n    def _simulation_worker(self, worker_id: int):\n        """Worker thread for simulation and data capture"""\n        while self.running:\n            try:\n                # Simulate environment and capture raw data\n                raw_data = self._simulate_environment()\n\n                # Create data sample\n                sample = DataSample(\n                    sample_id=self.stats["samples_generated"],\n                    rgb_image=raw_data["rgb"],\n                    depth_image=raw_data["depth"],\n                    segmentation_mask=raw_data["segmentation"],\n                    instance_mask=raw_data["instance"],\n                    robot_state=raw_data["robot_state"],\n                    environment_state=raw_data["environment_state"],\n                    sensor_data=raw_data["sensor_data"],\n                    metadata=raw_data["metadata"]\n                )\n\n                # Add to processing queue\n                self.simulation_queue.put(sample, timeout=1)\n\n                # Update statistics\n                self.stats["samples_generated"] += 1\n\n            except queue.Full:\n                continue\n            except Exception as e:\n                print(f"Simulation worker {worker_id} error: {e}")\n\n    def _processing_worker(self, worker_id: int):\n        """Worker thread for data processing and augmentation"""\n        while self.running:\n            try:\n                # Get sample from simulation queue\n                sample = self.simulation_queue.get(timeout=1)\n\n                # Apply domain randomization\n                processed_sample = self._apply_domain_randomization(sample)\n\n                # Apply data augmentation\n                augmented_sample = self._apply_augmentation(processed_sample)\n\n                # Add to save queue\n                self.processing_queue.put(augmented_sample, timeout=1)\n\n                # Update statistics\n                self.stats["samples_processed"] += 1\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                print(f"Processing worker {worker_id} error: {e}")\n\n    def _save_worker(self, worker_id: int):\n        """Worker thread for saving data to disk"""\n        while self.running:\n            try:\n                # Get sample from processing queue\n                sample = self.processing_queue.get(timeout=1)\n\n                # Save sample to disk\n                self._save_sample(sample)\n\n                # Update statistics\n                self.stats["samples_saved"] += 1\n\n            except queue.Empty:\n                continue\n            except Exception as e:\n                print(f"Save worker {worker_id} error: {e}")\n\n    def _simulate_environment(self) -> Dict:\n        """Simulate environment and capture sensor data"""\n        # This would interface with Isaac Sim to capture real data\n        # For now, we\'ll generate synthetic data\n\n        # Generate synthetic RGB image\n        rgb = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n\n        # Generate synthetic depth image\n        depth = np.random.uniform(0.1, 10.0, (480, 640)).astype(np.float32)\n\n        # Generate synthetic segmentation mask\n        segmentation = np.random.randint(0, 10, (480, 640), dtype=np.uint8)\n\n        # Generate synthetic instance mask\n        instance = np.random.randint(0, 50, (480, 640), dtype=np.uint8)\n\n        # Generate synthetic robot state\n        robot_state = {\n            "joint_positions": np.random.uniform(-np.pi, np.pi, 28).tolist(),  # 28 DOF humanoid\n            "joint_velocities": np.random.uniform(-10, 10, 28).tolist(),\n            "base_position": np.random.uniform(-1, 1, 3).tolist(),\n            "base_orientation": np.random.uniform(-1, 1, 4).tolist(),  # quaternion\n            "base_linear_velocity": np.random.uniform(-2, 2, 3).tolist(),\n            "base_angular_velocity": np.random.uniform(-2, 2, 3).tolist()\n        }\n\n        # Generate synthetic environment state\n        environment_state = {\n            "object_positions": np.random.uniform(-5, 5, (10, 3)).tolist(),\n            "object_orientations": np.random.uniform(-1, 1, (10, 4)).tolist(),  # quaternions\n            "lighting_conditions": {\n                "intensity": np.random.uniform(500, 2000),\n                "color_temperature": np.random.uniform(3000, 8000)\n            }\n        }\n\n        # Generate synthetic sensor data\n        sensor_data = {\n            "imu": {\n                "linear_acceleration": np.random.uniform(-10, 10, 3).tolist(),\n                "angular_velocity": np.random.uniform(-5, 5, 3).tolist(),\n                "orientation": np.random.uniform(-1, 1, 4).tolist()\n            },\n            "force_torque": {\n                "left_foot": np.random.uniform(-100, 100, 6).tolist(),\n                "right_foot": np.random.uniform(-100, 100, 6).tolist(),\n                "left_hand": np.random.uniform(-50, 50, 6).tolist(),\n                "right_hand": np.random.uniform(-50, 50, 6).tolist()\n            }\n        }\n\n        # Generate metadata\n        metadata = {\n            "timestamp": time.time(),\n            "sample_id": self.stats["samples_generated"],\n            "domain_randomization_params": self._get_current_dr_params()\n        }\n\n        return {\n            "rgb": rgb,\n            "depth": depth,\n            "segmentation": segmentation,\n            "instance": instance,\n            "robot_state": robot_state,\n            "environment_state": environment_state,\n            "sensor_data": sensor_data,\n            "metadata": metadata\n        }\n\n    def _apply_domain_randomization(self, sample: DataSample) -> DataSample:\n        """Apply domain randomization to the sample"""\n        # Apply visual domain randomization to RGB image\n        rgb_dr = self._apply_visual_dr(sample.rgb_image)\n\n        # Apply depth noise\n        depth_dr = self._apply_depth_noise(sample.depth_image)\n\n        # Update the sample\n        return DataSample(\n            sample_id=sample.sample_id,\n            rgb_image=rgb_dr,\n            depth_image=depth_dr,\n            segmentation_mask=sample.segmentation_mask,\n            instance_mask=sample.instance_mask,\n            robot_state=sample.robot_state,\n            environment_state=sample.environment_state,\n            sensor_data=sample.sensor_data,\n            metadata=sample.metadata\n        )\n\n    def _apply_visual_dr(self, rgb_image: np.ndarray) -> np.ndarray:\n        """Apply visual domain randomization to RGB image"""\n        # Convert to float for processing\n        img = rgb_image.astype(np.float32) / 255.0\n\n        # Apply color jitter\n        color_jitter = np.random.uniform(0.8, 1.2, 3)\n        img = img * color_jitter\n\n        # Apply brightness adjustment\n        brightness = np.random.uniform(0.8, 1.2)\n        img = img * brightness\n\n        # Apply contrast adjustment\n        contrast = np.random.uniform(0.8, 1.2)\n        img = np.clip((img - 0.5) * contrast + 0.5, 0, 1)\n\n        # Apply noise\n        noise = np.random.normal(0, 0.02, img.shape)\n        img = np.clip(img + noise, 0, 1)\n\n        # Convert back to uint8\n        return (img * 255).astype(np.uint8)\n\n    def _apply_depth_noise(self, depth_image: np.ndarray) -> np.ndarray:\n        """Apply noise to depth image"""\n        # Add Gaussian noise\n        noise = np.random.normal(0, 0.01, depth_image.shape)\n        noisy_depth = depth_image + noise\n\n        # Ensure valid depth range\n        return np.clip(noisy_depth, 0.1, 10.0)\n\n    def _apply_augmentation(self, sample: DataSample) -> DataSample:\n        """Apply data augmentation to the sample"""\n        # Apply random rotation\n        if np.random.random() < 0.5:\n            angle = np.random.uniform(-5, 5)  # degrees\n            sample = self._rotate_sample(sample, angle)\n\n        # Apply random scaling\n        if np.random.random() < 0.3:\n            scale = np.random.uniform(0.9, 1.1)\n            sample = self._scale_sample(sample, scale)\n\n        # Apply random cropping and padding\n        if np.random.random() < 0.2:\n            sample = self._random_crop_pad(sample)\n\n        return sample\n\n    def _rotate_sample(self, sample: DataSample, angle: float) -> DataSample:\n        """Rotate all images in the sample by the given angle"""\n        # Create rotation matrix\n        h, w = sample.rgb_image.shape[:2]\n        center = (w // 2, h // 2)\n        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n\n        # Rotate all images\n        rgb_rotated = cv2.warpAffine(sample.rgb_image, rotation_matrix, (w, h))\n        depth_rotated = cv2.warpAffine(sample.depth_image, rotation_matrix, (w, h))\n        seg_rotated = cv2.warpAffine(sample.segmentation_mask, rotation_matrix, (w, h))\n        inst_rotated = cv2.warpAffine(sample.instance_mask, rotation_matrix, (w, h))\n\n        return DataSample(\n            sample_id=sample.sample_id,\n            rgb_image=rgb_rotated,\n            depth_image=depth_rotated,\n            segmentation_mask=seg_rotated,\n            instance_mask=inst_rotated,\n            robot_state=sample.robot_state,\n            environment_state=sample.environment_state,\n            sensor_data=sample.sensor_data,\n            metadata=sample.metadata\n        )\n\n    def _scale_sample(self, sample: DataSample, scale: float) -> DataSample:\n        """Scale all images in the sample by the given factor"""\n        h, w = sample.rgb_image.shape[:2]\n        new_h, new_w = int(h * scale), int(w * scale)\n\n        # Resize all images\n        rgb_scaled = cv2.resize(sample.rgb_image, (new_w, new_h))\n        depth_scaled = cv2.resize(sample.depth_image, (new_w, new_h))\n        seg_scaled = cv2.resize(sample.segmentation_mask, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n        inst_scaled = cv2.resize(sample.instance_mask, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n\n        # Pad to original size if scaled up, crop if scaled down\n        if scale > 1.0:\n            # Crop to original size\n            start_h = (new_h - h) // 2\n            start_w = (new_w - w) // 2\n            rgb_scaled = rgb_scaled[start_h:start_h+h, start_w:start_w+w]\n            depth_scaled = depth_scaled[start_h:start_h+h, start_w:start_w+w]\n            seg_scaled = seg_scaled[start_h:start_h+h, start_w:start_w+w]\n            inst_scaled = inst_scaled[start_h:start_h+h, start_w:start_w+w]\n        else:\n            # Pad to original size\n            pad_h = h - new_h\n            pad_w = w - new_w\n            top_pad = pad_h // 2\n            bottom_pad = pad_h - top_pad\n            left_pad = pad_w // 2\n            right_pad = pad_w - left_pad\n\n            rgb_scaled = np.pad(rgb_scaled, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)), mode=\'constant\')\n            depth_scaled = np.pad(depth_scaled, ((top_pad, bottom_pad), (left_pad, right_pad)), mode=\'constant\')\n            seg_scaled = np.pad(seg_scaled, ((top_pad, bottom_pad), (left_pad, right_pad)), mode=\'constant\')\n            inst_scaled = np.pad(inst_scaled, ((top_pad, bottom_pad), (left_pad, right_pad)), mode=\'constant\')\n\n        return DataSample(\n            sample_id=sample.sample_id,\n            rgb_image=rgb_scaled,\n            depth_image=depth_scaled,\n            segmentation_mask=seg_scaled,\n            instance_mask=inst_scaled,\n            robot_state=sample.robot_state,\n            environment_state=sample.environment_state,\n            sensor_data=sample.sensor_data,\n            metadata=sample.metadata\n        )\n\n    def _random_crop_pad(self, sample: DataSample) -> DataSample:\n        """Apply random cropping and padding to all images"""\n        h, w = sample.rgb_image.shape[:2]\n\n        # Random crop parameters\n        crop_h = np.random.randint(int(h * 0.8), h)\n        crop_w = np.random.randint(int(w * 0.8), w)\n\n        start_h = np.random.randint(0, h - crop_h + 1)\n        start_w = np.random.randint(0, w - crop_w + 1)\n\n        # Crop all images\n        rgb_cropped = sample.rgb_image[start_h:start_h+crop_h, start_w:start_w+crop_w]\n        depth_cropped = sample.depth_image[start_h:start_h+crop_h, start_w:start_w+crop_w]\n        seg_cropped = sample.segmentation_mask[start_h:start_h+crop_h, start_w:start_w+crop_w]\n        inst_cropped = sample.instance_mask[start_h:start_h+crop_h, start_w:start_w+crop_w]\n\n        # Resize back to original size\n        rgb_resized = cv2.resize(rgb_cropped, (w, h))\n        depth_resized = cv2.resize(depth_cropped, (w, h))\n        seg_resized = cv2.resize(seg_cropped, (w, h), interpolation=cv2.INTER_NEAREST)\n        inst_resized = cv2.resize(inst_cropped, (w, h), interpolation=cv2.INTER_NEAREST)\n\n        return DataSample(\n            sample_id=sample.sample_id,\n            rgb_image=rgb_resized,\n            depth_image=depth_resized,\n            segmentation_mask=seg_resized,\n            instance_mask=inst_resized,\n            robot_state=sample.robot_state,\n            environment_state=sample.environment_state,\n            sensor_data=sample.sensor_data,\n            metadata=sample.metadata\n        )\n\n    def _save_sample(self, sample: DataSample):\n        """Save a sample to disk"""\n        # Save RGB image\n        rgb_path = self.rgb_dir / f"rgb_{sample.sample_id:06d}.png"\n        Image.fromarray(sample.rgb_image).save(rgb_path)\n\n        # Save depth image\n        depth_path = self.depth_dir / f"depth_{sample.sample_id:06d}.npy"\n        np.save(depth_path, sample.depth_image)\n\n        # Save segmentation mask\n        seg_path = self.seg_dir / f"seg_{sample.sample_id:06d}.png"\n        Image.fromarray(sample.segmentation_mask).save(seg_path)\n\n        # Save instance mask\n        inst_path = self.inst_dir / f"inst_{sample.sample_id:06d}.png"\n        Image.fromarray(sample.instance_mask).save(inst_path)\n\n        # Save metadata\n        meta_path = self.meta_dir / f"meta_{sample.sample_id:06d}.json"\n        with open(meta_path, \'w\') as f:\n            json.dump({\n                "sample_id": sample.sample_id,\n                "robot_state": sample.robot_state,\n                "environment_state": sample.environment_state,\n                "sensor_data": sample.sensor_data,\n                "metadata": sample.metadata\n            }, f, indent=2)\n\n    def _get_current_dr_params(self) -> Dict:\n        """Get current domain randomization parameters"""\n        return {\n            "rgb_color_jitter": np.random.uniform(0.8, 1.2, 3).tolist(),\n            "rgb_brightness": np.random.uniform(0.8, 1.2),\n            "rgb_contrast": np.random.uniform(0.8, 1.2),\n            "depth_noise_std": np.random.uniform(0.005, 0.02),\n            "rotation_angle": np.random.uniform(-5, 5),\n            "scale_factor": np.random.uniform(0.9, 1.1)\n        }\n\n# Example usage of the complete pipeline\ndef run_synthetic_data_pipeline():\n    """Run the complete synthetic data generation pipeline"""\n    # Create pipeline\n    pipeline = SyntheticDataPipeline(\n        output_dir="humanoid_synthetic_data",\n        buffer_size=200,\n        num_workers=6\n    )\n\n    # Start pipeline\n    pipeline.start_pipeline()\n\n    # Run for a specified time\n    start_time = time.time()\n    duration = 30  # seconds\n\n    try:\n        while time.time() - start_time < duration:\n            time.sleep(1)\n            print(f"Generated: {pipeline.stats[\'samples_generated\']}, "\n                  f"Processed: {pipeline.stats[\'samples_processed\']}, "\n                  f"Saved: {pipeline.stats[\'samples_saved\']}")\n    except KeyboardInterrupt:\n        print("Stopping pipeline...")\n\n    # Stop pipeline\n    pipeline.stop_pipeline()\n\n# Run the pipeline\nif __name__ == "__main__":\n    run_synthetic_data_pipeline()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"quality-assurance",children:"Quality Assurance"}),"\n",(0,i.jsx)(n.p,{children:"Quality assurance is critical for synthetic data generation to ensure that the generated data is suitable for training robust AI models. Here's a comprehensive quality assurance system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom PIL import Image\nimport cv2\nfrom typing import Dict, List, Tuple, Optional\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport pandas as pd\n\nclass SyntheticDataQualityAssurance:\n    """Quality assurance system for synthetic data"""\n\n    def __init__(self):\n        self.quality_metrics = {}\n        self.passed_checks = []\n        self.failed_checks = []\n\n    def run_comprehensive_qa(self, data_dir: str) -> Dict:\n        """Run comprehensive quality assurance on synthetic data"""\n        data_dir = Path(data_dir)\n\n        # Load data samples\n        samples = self._load_data_samples(data_dir)\n\n        # Run all quality checks\n        self.passed_checks = []\n        self.failed_checks = []\n\n        # Check 1: Data completeness\n        completeness_result = self._check_data_completeness(samples)\n        if completeness_result["passed"]:\n            self.passed_checks.append(("Data completeness", completeness_result))\n        else:\n            self.failed_checks.append(("Data completeness", completeness_result))\n\n        # Check 2: Data consistency\n        consistency_result = self._check_data_consistency(samples)\n        if consistency_result["passed"]:\n            self.passed_checks.append(("Data consistency", consistency_result))\n        else:\n            self.failed_checks.append(("Data consistency", consistency_result))\n\n        # Check 3: Data diversity\n        diversity_result = self._check_data_diversity(samples)\n        if diversity_result["passed"]:\n            self.passed_checks.append(("Data diversity", diversity_result))\n        else:\n            self.failed_checks.append(("Data diversity", diversity_result))\n\n        # Check 4: Data quality metrics\n        quality_result = self._check_data_quality(samples)\n        if quality_result["passed"]:\n            self.passed_checks.append(("Data quality", quality_result))\n        else:\n            self.failed_checks.append(("Data quality", quality_result))\n\n        # Generate quality report\n        quality_report = {\n            "total_samples": len(samples),\n            "passed_checks": len(self.passed_checks),\n            "failed_checks": len(self.failed_checks),\n            "passed_details": self.passed_checks,\n            "failed_details": self.failed_checks,\n            "overall_score": len(self.passed_checks) / (len(self.passed_checks) + len(self.failed_checks)) if (len(self.passed_checks) + len(self.failed_checks)) > 0 else 0\n        }\n\n        return quality_report\n\n    def _load_data_samples(self, data_dir: Path) -> List[Dict]:\n        """Load data samples from directory"""\n        samples = []\n\n        # Get all sample IDs\n        rgb_files = list((data_dir / "rgb").glob("*.png"))\n\n        for rgb_file in rgb_files:\n            sample_id = int(rgb_file.stem.split("_")[-1])\n\n            # Load all components of the sample\n            sample = {\n                "sample_id": sample_id,\n                "rgb": np.array(Image.open(rgb_file)),\n                "depth": np.load(data_dir / "depth" / f"depth_{sample_id:06d}.npy") if (data_dir / "depth" / f"depth_{sample_id:06d}.npy").exists() else None,\n                "segmentation": np.array(Image.open(data_dir / "segmentation" / f"seg_{sample_id:06d}.png")) if (data_dir / "segmentation" / f"seg_{sample_id:06d}.png").exists() else None,\n                "instance": np.array(Image.open(data_dir / "instance" / f"inst_{sample_id:06d}.png")) if (data_dir / "instance" / f"inst_{sample_id:06d}.png").exists() else None,\n                "metadata": self._load_json_file(data_dir / "metadata" / f"meta_{sample_id:06d}.json")\n            }\n\n            samples.append(sample)\n\n        return samples\n\n    def _load_json_file(self, file_path: Path) -> Optional[Dict]:\n        """Load JSON metadata file"""\n        if file_path.exists():\n            with open(file_path, \'r\') as f:\n                return json.load(f)\n        return None\n\n    def _check_data_completeness(self, samples: List[Dict]) -> Dict:\n        """Check if all required data components are present"""\n        required_components = ["rgb", "metadata"]\n        optional_components = ["depth", "segmentation", "instance"]\n\n        missing_samples = []\n\n        for sample in samples:\n            missing = []\n            for component in required_components:\n                if sample.get(component) is None:\n                    missing.append(component)\n\n            if missing:\n                missing_samples.append({\n                    "sample_id": sample["sample_id"],\n                    "missing_components": missing\n                })\n\n        completeness_score = 1.0 - (len(missing_samples) / len(samples)) if samples else 0\n\n        result = {\n            "passed": len(missing_samples) == 0,\n            "completeness_score": completeness_score,\n            "missing_samples": missing_samples,\n            "total_samples": len(samples),\n            "complete_samples": len(samples) - len(missing_samples)\n        }\n\n        return result\n\n    def _check_data_consistency(self, samples: List[Dict]) -> Dict:\n        """Check consistency of data dimensions and types"""\n        if not samples:\n            return {"passed": False, "error": "No samples to check"}\n\n        # Check RGB dimensions\n        rgb_dims = [sample["rgb"].shape for sample in samples if sample["rgb"] is not None]\n        if not rgb_dims:\n            return {"passed": False, "error": "No RGB data available"}\n\n        # Check if all RGB images have the same dimensions\n        first_dim = rgb_dims[0]\n        consistent_rgb = all(dim == first_dim for dim in rgb_dims)\n\n        # Check depth consistency if available\n        depth_samples = [sample for sample in samples if sample["depth"] is not None]\n        if depth_samples:\n            depth_dims = [sample["depth"].shape for sample in depth_samples]\n            consistent_depth = all(dim == first_dim[:2] for dim in depth_dims)  # Depth should match RGB dimensions\n        else:\n            consistent_depth = True  # No depth data to check\n\n        # Check metadata consistency\n        metadata_samples = [sample for sample in samples if sample["metadata"] is not None]\n        if metadata_samples:\n            required_metadata_keys = ["sample_id", "robot_state", "environment_state", "sensor_data"]\n            consistent_metadata = all(\n                all(key in sample["metadata"] for key in required_metadata_keys)\n                for sample in metadata_samples\n            )\n        else:\n            consistent_metadata = False\n\n        result = {\n            "passed": consistent_rgb and consistent_depth and consistent_metadata,\n            "rgb_consistent": consistent_rgb,\n            "depth_consistent": consistent_depth,\n            "metadata_consistent": consistent_metadata,\n            "rgb_dimension": first_dim if rgb_dims else None\n        }\n\n        return result\n\n    def _check_data_diversity(self, samples: List[Dict]) -> Dict:\n        """Check diversity of data across various dimensions"""\n        if not samples:\n            return {"passed": False, "error": "No samples to check"}\n\n        # Calculate diversity metrics\n        rgb_diversity = self._calculate_rgb_diversity(samples)\n        depth_diversity = self._calculate_depth_diversity(samples)\n        pose_diversity = self._calculate_pose_diversity(samples)\n\n        # Overall diversity score (weighted average)\n        diversity_score = (\n            0.4 * rgb_diversity["diversity_score"] +\n            0.3 * depth_diversity["diversity_score"] +\n            0.3 * pose_diversity["diversity_score"]\n        )\n\n        # Consider diverse if diversity score > 0.5\n        is_diverse = diversity_score > 0.5\n\n        result = {\n            "passed": is_diverse,\n            "diversity_score": diversity_score,\n            "rgb_diversity": rgb_diversity,\n            "depth_diversity": depth_diversity,\n            "pose_diversity": pose_diversity,\n            "threshold": 0.5\n        }\n\n        return result\n\n    def _calculate_rgb_diversity(self, samples: List[Dict]) -> Dict:\n        """Calculate diversity of RGB images"""\n        rgb_samples = [sample["rgb"] for sample in samples if sample["rgb"] is not None]\n\n        if len(rgb_samples) < 2:\n            return {"diversity_score": 0.0, "error": "Insufficient samples for diversity calculation"}\n\n        # Convert to grayscale for diversity calculation\n        gray_samples = [cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY) for rgb in rgb_samples]\n\n        # Calculate mean differences between consecutive frames\n        differences = []\n        for i in range(1, len(gray_samples)):\n            diff = cv2.absdiff(gray_samples[i-1], gray_samples[i])\n            differences.append(np.mean(diff))\n\n        # Calculate diversity as the mean of differences\n        diversity = np.mean(differences) if differences else 0.0\n\n        # Normalize to 0-1 range (assuming max possible difference is 255)\n        diversity_score = min(diversity / 255.0, 1.0)\n\n        return {\n            "diversity_score": diversity_score,\n            "mean_difference": diversity,\n            "num_comparisons": len(differences)\n        }\n\n    def _calculate_depth_diversity(self, samples: List[Dict]) -> Dict:\n        """Calculate diversity of depth images"""\n        depth_samples = [sample["depth"] for sample in samples if sample["depth"] is not None]\n\n        if len(depth_samples) < 2:\n            return {"diversity_score": 0.0, "error": "Insufficient depth samples for diversity calculation"}\n\n        # Calculate mean differences between consecutive frames\n        differences = []\n        for i in range(1, len(depth_samples)):\n            diff = np.abs(depth_samples[i-1] - depth_samples[i])\n            differences.append(np.mean(diff))\n\n        # Calculate diversity as the mean of differences\n        diversity = np.mean(differences) if differences else 0.0\n\n        # Normalize to 0-1 range (assuming max depth is 10m)\n        diversity_score = min(diversity / 10.0, 1.0)\n\n        return {\n            "diversity_score": diversity_score,\n            "mean_difference": diversity,\n            "num_comparisons": len(differences)\n        }\n\n    def _calculate_pose_diversity(self, samples: List[Dict]) -> Dict:\n        """Calculate diversity of robot poses"""\n        pose_samples = []\n        for sample in samples:\n            if sample["metadata"] and "robot_state" in sample["metadata"]:\n                robot_state = sample["metadata"]["robot_state"]\n                if "joint_positions" in robot_state:\n                    pose_samples.append(np.array(robot_state["joint_positions"]))\n\n        if len(pose_samples) < 2:\n            return {"diversity_score": 0.0, "error": "Insufficient pose samples for diversity calculation"}\n\n        # Calculate mean differences between consecutive poses\n        differences = []\n        for i in range(1, len(pose_samples)):\n            diff = np.abs(pose_samples[i-1] - pose_samples[i])\n            differences.append(np.mean(diff))\n\n        # Calculate diversity as the mean of differences\n        diversity = np.mean(differences) if differences else 0.0\n\n        # Normalize to 0-1 range (assuming max joint difference is 2*pi)\n        diversity_score = min(diversity / (2 * np.pi), 1.0)\n\n        return {\n            "diversity_score": diversity_score,\n            "mean_difference": diversity,\n            "num_comparisons": len(differences),\n            "num_joints": len(pose_samples[0]) if pose_samples else 0\n        }\n\n    def _check_data_quality(self, samples: List[Dict]) -> Dict:\n        """Check overall data quality metrics"""\n        if not samples:\n            return {"passed": False, "error": "No samples to check"}\n\n        # Check for common quality issues\n        quality_issues = []\n\n        # Check for blurry images\n        blur_scores = []\n        for sample in samples:\n            if sample["rgb"] is not None:\n                gray = cv2.cvtColor(sample["rgb"], cv2.COLOR_RGB2GRAY)\n                # Calculate Laplacian variance as a measure of focus\n                laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n                blur_scores.append(laplacian_var)\n\n        mean_blur_score = np.mean(blur_scores) if blur_scores else 0\n        # Consider blurry if Laplacian variance < threshold\n        blurry_count = sum(1 for score in blur_scores if score < 100)  # threshold may need adjustment\n\n        # Check for over/under exposed images\n        exposure_issues = 0\n        for sample in samples:\n            if sample["rgb"] is not None:\n                # Calculate mean brightness\n                mean_brightness = np.mean(cv2.cvtColor(sample["rgb"], cv2.COLOR_RGB2GRAY))\n                # Flag if too bright (> 200) or too dark (< 50)\n                if mean_brightness > 200 or mean_brightness < 50:\n                    exposure_issues += 1\n\n        # Check for depth validity\n        invalid_depth_count = 0\n        for sample in samples:\n            if sample["depth"] is not None:\n                # Count invalid depth values (0 or very high)\n                invalid_mask = (sample["depth"] <= 0) | (sample["depth"] > 20)  # 20m max reasonable depth\n                invalid_depth_count += np.sum(invalid_mask)\n\n        # Calculate quality score\n        total_pixels = sum(sample["rgb"].size for sample in samples if sample["rgb"] is not None)\n        invalid_ratio = (blurry_count + exposure_issues + invalid_depth_count) / total_pixels if total_pixels > 0 else 0\n        quality_score = max(0, 1 - invalid_ratio)\n\n        # Consider passed if quality score > 0.8\n        is_quality_high = quality_score > 0.8\n\n        result = {\n            "passed": is_quality_high,\n            "quality_score": quality_score,\n            "blurry_samples": blurry_count,\n            "exposure_issues": exposure_issues,\n            "invalid_depth_values": invalid_depth_count,\n            "mean_blur_score": mean_blur_score,\n            "threshold": 0.8\n        }\n\n        return result\n\n    def generate_quality_report(self, quality_report: Dict, output_path: str = "quality_report.html"):\n        """Generate a detailed quality report"""\n        html_content = f"""\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <title>Synthetic Data Quality Report</title>\n            <style>\n                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n                .header {{ background-color: #f0f0f0; padding: 10px; border-radius: 5px; }}\n                .section {{ margin: 20px 0; }}\n                .passed {{ color: green; font-weight: bold; }}\n                .failed {{ color: red; font-weight: bold; }}\n                table {{ border-collapse: collapse; width: 100%; }}\n                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n                th {{ background-color: #f2f2f2; }}\n            </style>\n        </head>\n        <body>\n            <div class="header">\n                <h1>Synthetic Data Quality Report</h1>\n                <p>Generated on: {time.strftime(\'%Y-%m-%d %H:%M:%S\')}</p>\n            </div>\n\n            <div class="section">\n                <h2>Overall Statistics</h2>\n                <table>\n                    <tr><td>Total Samples</td><td>{quality_report[\'total_samples\']}</td></tr>\n                    <tr><td>Passed Checks</td><td>{quality_report[\'passed_checks\']}</td></tr>\n                    <tr><td>Failed Checks</td><td>{quality_report[\'failed_checks\']}</td></tr>\n                    <tr><td>Overall Score</td><td>{quality_report[\'overall_score\']:.2f}</td></tr>\n                </table>\n            </div>\n\n            <div class="section">\n                <h2>Passed Checks</h2>\n                <ul>\n        """\n\n        for check_name, details in quality_report[\'passed_details\']:\n            html_content += f"<li><span class=\'passed\'>\u2713 {check_name}</span>: {details.get(\'diversity_score\', details.get(\'quality_score\', \'N/A\')):.2f}</li>"\n\n        html_content += """\n                </ul>\n            </div>\n\n            <div class="section">\n                <h2>Failed Checks</h2>\n                <ul>\n        """\n\n        for check_name, details in quality_report[\'failed_details\']:\n            html_content += f"<li><span class=\'failed\'>\u2717 {check_name}</span>: {details.get(\'error\', \'N/A\')}</li>"\n\n        html_content += """\n                </ul>\n            </div>\n        </body>\n        </html>\n        """\n\n        with open(output_path, \'w\') as f:\n            f.write(html_content)\n\n# Example usage of quality assurance\ndef run_quality_assurance():\n    """Run quality assurance on generated synthetic data"""\n    # Create QA system\n    qa_system = SyntheticDataQualityAssurance()\n\n    # Run comprehensive QA\n    quality_report = qa_system.run_comprehensive_qa("humanoid_synthetic_data")\n\n    # Print summary\n    print(f"Quality Report Summary:")\n    print(f"  Total Samples: {quality_report[\'total_samples\']}")\n    print(f"  Passed Checks: {quality_report[\'passed_checks\']}")\n    print(f"  Failed Checks: {quality_report[\'failed_checks\']}")\n    print(f"  Overall Score: {quality_report[\'overall_score\']:.2f}")\n\n    # Generate detailed report\n    qa_system.generate_quality_report(quality_report)\n\n    return quality_report\n'})}),"\n",(0,i.jsx)(n.h2,{id:"isaac-sim-setup-parameters",children:"Isaac Sim Setup Parameters"}),"\n",(0,i.jsx)(n.p,{children:"Here are the key Isaac Sim setup parameters for synthetic data generation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Isaac Sim Synthetic Data Setup] --\x3e B[Simulation Configuration]\n    A --\x3e C[Sensor Configuration]\n    A --\x3e D[Domain Randomization]\n    A --\x3e E[Data Pipeline]\n\n    B --\x3e B1[Physics Engine]\n    B --\x3e B2[Stage Units]\n    B --\x3e B3[Gravity Settings]\n    B --\x3e B4[Simulation Frequency]\n\n    C --\x3e C1[Camera Settings]\n    C --\x3e C2[LiDAR Configuration]\n    C --\x3e C3[IMU Parameters]\n    C --\x3e C4[Other Sensors]\n\n    D --\x3e D1[Lighting Randomization]\n    D --\x3e D2[Material Randomization]\n    D --\x3e D3[Geometry Randomization]\n    D --\x3e D4[Camera Randomization]\n\n    E --\x3e E1[Data Buffer Size]\n    E --\x3e E2[Processing Workers]\n    E --\x3e E3[Output Format]\n    E --\x3e E4[Quality Assurance]\n\n    B1 --\x3e B11[PhysX Parameters]\n    B2 --\x3e B21[Meter Units]\n    B3 --\x3e B31[-9.81 m/s\xb2]\n    B4 --\x3e B41[60 Hz Default]\n\n    C1 --\x3e C11[Resolution 640x480]\n    C1 --\x3e C12[FOV 60\xb0]\n    C1 --\x3e C13[Frame Rate 30 FPS]\n\n    C2 --\x3e C21[Channels 16]\n    C2 --\x3e C22[Range 0.1-25m]\n    C2 --\x3e C23[Points/Sec 240K]\n\n    D1 --\x3e D11[Intensity 500-2000]\n    D1 --\x3e D12[Color Temp 3000-8000K]\n\n    D2 --\x3e D21[Albedo 0.1-1.0]\n    D2 --\x3e D22[Roughness 0.0-1.0]\n    D2 --\x3e D23[Metallic 0.0-1.0]\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n"})}),"\n",(0,i.jsx)(n.p,{children:"Here are the detailed Isaac Sim configuration parameters for synthetic data generation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Isaac Sim Configuration for Synthetic Data Generation\nISAAC_SIM_CONFIG = {\n    # Stage and World Configuration\n    "stage_units_in_meters": 1.0,\n    "gravity": [-9.81, 0, 0],\n    "physics_dt": 1.0/60.0,  # Physics timestep\n    "rendering_dt": 1.0/60.0,  # Rendering timestep\n\n    # Physics Engine Configuration\n    "physics_engine": "physx",\n    "physx": {\n        "solver_type": "TGS",  # TGS or PG S\n        "solver_position_iteration_count": 4,\n        "solver_velocity_iteration_count": 1,\n        "sleep_threshold": 0.005,\n        "stabilization_threshold": 0.001,\n        "gpu_max_rigid_contact_count": 524288,\n        "gpu_max_rigid_patch_count": 33554432,\n        "gpu_found_lost_pairs_capacity": 1048576,\n        "gpu_found_lost_aggregate_pairs_capacity": 1048576,\n        "gpu_total_aggregate_pairs_capacity": 1048576,\n        "gpu_enqueue_mode": 2,  # CUDA_LAUNCH\n        "gpu_determinism_mode": False,\n        "gpu_max_particles": 1000000\n    },\n\n    # Rendering Configuration\n    "rendering": {\n        "width": 640,\n        "height": 480,\n        "msaa_samples": 1,  # Multi-sample anti-aliasing\n        "max_texture_resolution": 2048,\n        "max_materials": 1000,\n        "max_lights": 100\n    },\n\n    # Camera Configuration\n    "camera": {\n        "resolution": [640, 480],\n        "focal_length": 24.0,  # mm\n        "sensor_width": 36.0,  # mm\n        "clipping_range": [0.01, 100.0],\n        "horizontal_aperture": 36.0,\n        "vertical_aperture": 20.25,\n        "projection_type": "perspective",  # perspective or orthographic\n        "frequency": 30  # Hz\n    },\n\n    # LiDAR Configuration\n    "lidar": {\n        "rotation_frequency": 20,  # Hz\n        "number_of_channels": 16,\n        "points_per_second": 240000,\n        "horizontal_resolution": 4,\n        "vertical_resolution": 1,\n        "horizontal_lasers": 800,\n        "vertical_lasers": 16,\n        "range": [0.1, 25.0],  # meters\n        "rotation": [0, 0, 0],  # radians\n        "offset": [0, 0, 0]  # meters\n    },\n\n    # Domain Randomization Ranges\n    "domain_randomization": {\n        "lighting": {\n            "intensity_range": [500, 2000],  # Lumens\n            "color_temperature_range": [3000, 8000],  # Kelvin\n            "position_jitter": [0.5, 0.5, 0.5]  # meters\n        },\n        "materials": {\n            "albedo_range": [0.1, 1.0],\n            "roughness_range": [0.0, 1.0],\n            "metallic_range": [0.0, 1.0],\n            "specular_range": [0.0, 1.0]\n        },\n        "geometry": {\n            "position_jitter": 0.5,  # meters\n            "rotation_jitter": 0.2,  # radians\n            "scale_range": [0.8, 1.2]\n        },\n        "camera": {\n            "position_jitter": 0.1,  # meters\n            "rotation_jitter": 0.05,  # radians\n            "fov_jitter": 0.1  # radians\n        }\n    },\n\n    # Data Pipeline Configuration\n    "data_pipeline": {\n        "buffer_size": 200,\n        "num_workers": 6,\n        "output_format": "png_npy_json",  # Format for saved data\n        "compression": "none",  # none, jpeg, png for images\n        "quality": 95,  # For compressed formats\n        "async_mode": True,\n        "batch_size": 32\n    },\n\n    # Quality Assurance Configuration\n    "quality_assurance": {\n        "enable": True,\n        "completeness_threshold": 1.0,\n        "consistency_threshold": 1.0,\n        "diversity_threshold": 0.5,\n        "quality_threshold": 0.8,\n        "check_frequency": 100  # Check every N samples\n    }\n}\n\n# Isaac Sim Environment Setup\ndef setup_isaac_sim_environment():\n    """Setup Isaac Sim environment for synthetic data generation"""\n    from omni.isaac.core import World\n    from omni.isaac.core.utils.stage import add_reference_to_stage\n    from omni.isaac.core.utils.nucleus import get_assets_root_path\n    from omni.isaac.core.utils.prims import get_prim_at_path, create_prim\n    from omni.isaac.sensor import Camera\n    from omni.isaac.range_sensor import _range_sensor\n\n    # Initialize world with configuration\n    world = World(\n        stage_units_in_meters=ISAAC_SIM_CONFIG["stage_units_in_meters"],\n        physics_dt=ISAAC_SIM_CONFIG["physics_dt"],\n        rendering_dt=ISAAC_SIM_CONFIG["rendering_dt"],\n        backend="numpy"\n    )\n\n    # Set gravity\n    world.scene.set_gravity([ISAAC_SIM_CONFIG["gravity"][0], 0, 0])\n\n    # Add default ground plane\n    world.scene.add_default_ground_plane()\n\n    # Setup camera\n    camera = Camera(\n        prim_path="/World/Camera",\n        frequency=ISAAC_SIM_CONFIG["camera"]["frequency"],\n        resolution=ISAAC_SIM_CONFIG["camera"]["resolution"]\n    )\n\n    # Set camera parameters\n    camera.set_focal_length(ISAAC_SIM_CONFIG["camera"]["focal_length"])\n    camera.set_horizontal_aperture(ISAAC_SIM_CONFIG["camera"]["horizontal_aperture"])\n    camera.set_vertical_aperture(ISAAC_SIM_CONFIG["camera"]["vertical_aperture"])\n\n    # Setup LiDAR if needed\n    lidar_interface = _range_sensor.acquire_lidar_sensor_interface()\n\n    # Create LiDAR sensor\n    lidar_config = {\n        "rotation_frequency": ISAAC_SIM_CONFIG["lidar"]["rotation_frequency"],\n        "number_of_channels": ISAAC_SIM_CONFIG["lidar"]["number_of_channels"],\n        "points_per_second": ISAAC_SIM_CONFIG["lidar"]["points_per_second"],\n        "horizontal_resolution": ISAAC_SIM_CONFIG["lidar"]["horizontal_resolution"],\n        "vertical_resolution": ISAAC_SIM_CONFIG["lidar"]["vertical_resolution"],\n        "horizontal_lasers": ISAAC_SIM_CONFIG["lidar"]["horizontal_lasers"],\n        "vertical_lasers": ISAAC_SIM_CONFIG["lidar"]["vertical_lasers"],\n        "range": ISAAC_SIM_CONFIG["lidar"]["range"],\n        "rotation": ISAAC_SIM_CONFIG["lidar"]["rotation"]\n    }\n\n    lidar_interface.create_lidar_sensor(\n        "/World/Lidar",\n        [0, 0, 1.0],  # translation\n        lidar_config\n    )\n\n    return world, camera, lidar_interface\n'})}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Synthetic data generation for humanoid robots using NVIDIA Isaac provides a powerful approach to creating diverse, high-quality training datasets without the need for expensive real-world data collection. The comprehensive pipeline includes:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Isaac Sim Setup"}),": Proper configuration of physics, rendering, and sensor parameters for realistic simulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Domain Randomization"}),": Systematic randomization of visual, physical, and geometric properties to improve real-world transfer"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Pipeline"}),": Efficient multi-stage pipeline for simulation, processing, and storage of synthetic data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quality Assurance"}),": Comprehensive validation of data completeness, consistency, diversity, and quality"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The implementation includes Python classes for data generation, domain randomization, pipeline management, and quality assurance. The system is designed to be scalable and configurable for different humanoid robot applications, with parameters that can be adjusted based on specific requirements."}),"\n",(0,i.jsx)(n.p,{children:"The synthetic data generation pipeline enables rapid development of robust AI models for humanoid robots, reducing the time and cost associated with real-world data collection while maintaining high data quality and diversity."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>r,x:()=>o});var a=t(6540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);