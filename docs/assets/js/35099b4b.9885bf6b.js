"use strict";(globalThis.webpackChunkclassic=globalThis.webpackChunkclassic||[]).push([[2173],{5035(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"module-2/sensor-simulation","title":"Sensor Simulation: Advanced Perceptual Systems for Humanoid Robot Digital Twins","description":"Introduction to Sensor Simulation for Digital Twins","source":"@site/docs/module-2/sensor-simulation.md","sourceDirName":"module-2","slug":"/module-2/sensor-simulation","permalink":"/ai-native-sdd-books/docs/module-2/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/sherazi-412002/ai-native-sdd-books/tree/main/docs/module-2/sensor-simulation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"curriculumSidebar","previous":{"title":"Unity Rendering","permalink":"/ai-native-sdd-books/docs/module-2/unity-rendering"},"next":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","permalink":"/ai-native-sdd-books/docs/category/module-3-the-ai-robot-brain-nvidia-isaac"}}');var t=i(4848),s=i(8453);const r={sidebar_position:3},o="Sensor Simulation: Advanced Perceptual Systems for Humanoid Robot Digital Twins",l={},c=[{value:"Introduction to Sensor Simulation for Digital Twins",id:"introduction-to-sensor-simulation-for-digital-twins",level:2},{value:"Theoretical Foundation of Sensor Simulation",id:"theoretical-foundation-of-sensor-simulation",level:2},{value:"Sensor Modeling Principles",id:"sensor-modeling-principles",level:3},{value:"LiDAR Sensor Model",id:"lidar-sensor-model",level:4},{value:"IMU Sensor Model",id:"imu-sensor-model",level:4},{value:"Camera Sensor Model",id:"camera-sensor-model",level:3},{value:"Advanced LiDAR Simulation Techniques",id:"advanced-lidar-simulation-techniques",level:2},{value:"Multi-Echo LiDAR Simulation",id:"multi-echo-lidar-simulation",level:3},{value:"Integration with ROS 2 and Isaac Sim",id:"integration-with-ros-2-and-isaac-sim",level:2},{value:"ROS 2 Sensor Message Simulation",id:"ros-2-sensor-message-simulation",level:3},{value:"Isaac Sim Integration for Sensor Simulation",id:"isaac-sim-integration-for-sensor-simulation",level:2},{value:"Isaac Sim Configuration for Sensor Simulation",id:"isaac-sim-configuration-for-sensor-simulation",level:3},{value:"Performance Optimization for Real-time Sensor Simulation",id:"performance-optimization-for-real-time-sensor-simulation",level:2},{value:"Conclusion",id:"conclusion",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"sensor-simulation-advanced-perceptual-systems-for-humanoid-robot-digital-twins",children:"Sensor Simulation: Advanced Perceptual Systems for Humanoid Robot Digital Twins"})}),"\n",(0,t.jsx)(e.h2,{id:"introduction-to-sensor-simulation-for-digital-twins",children:"Introduction to Sensor Simulation for Digital Twins"}),"\n",(0,t.jsx)(e.p,{children:"Sensor simulation in digital twin environments for humanoid robots is critical for creating realistic perception systems that enable effective testing and development of algorithms before deployment on physical hardware. The simulation must accurately replicate the characteristics, noise patterns, and limitations of real-world sensors to ensure transferability of learned behaviors and algorithms."}),"\n",(0,t.jsx)(e.p,{children:"Modern digital twin systems require sophisticated sensor simulation that includes:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Realistic Noise Modeling"}),": Accurate representation of sensor noise, biases, and drift"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Environmental Interactions"}),": Proper simulation of how sensors interact with different materials and lighting conditions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Dynamics"}),": Accurate timing and synchronization between different sensors"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Physics-Based Rendering"}),": Proper simulation of sensor-specific physics phenomena"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"theoretical-foundation-of-sensor-simulation",children:"Theoretical Foundation of Sensor Simulation"}),"\n",(0,t.jsx)(e.h3,{id:"sensor-modeling-principles",children:"Sensor Modeling Principles"}),"\n",(0,t.jsx)(e.p,{children:"The mathematical foundation for sensor simulation is based on the concept of forward rendering, where real-world phenomena are projected onto sensor-specific representations:"}),"\n",(0,t.jsx)(e.h4,{id:"lidar-sensor-model",children:"LiDAR Sensor Model"}),"\n",(0,t.jsx)(e.p,{children:"LiDAR sensors operate on the principle of time-of-flight measurement, where laser pulses are emitted and the time taken for the reflection to return is measured. The sensor model can be expressed as:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"d = (c * \u0394t) / 2\n"})}),"\n",(0,t.jsxs)(e.p,{children:["Where ",(0,t.jsx)(e.code,{children:"d"})," is the distance, ",(0,t.jsx)(e.code,{children:"c"})," is the speed of light, and ",(0,t.jsx)(e.code,{children:"\u0394t"})," is the time difference between emission and reception."]}),"\n",(0,t.jsx)(e.p,{children:"The simulated LiDAR measurement includes several noise components:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"// Advanced LiDAR sensor simulation model\n#include <vector>\n#include <random>\n#include <cmath>\n#include <memory>\n\nclass LiDARSensorModel {\npublic:\n    struct RayData {\n        float distance;\n        float intensity;\n        uint8_t return_type;  // Single, first, last, etc.\n        int8_t noise_level;\n        float angle_horizontal;\n        float angle_vertical;\n    };\n\n    struct SensorConfig {\n        float range_min = 0.1f;        // Minimum detectable range (m)\n        float range_max = 100.0f;      // Maximum detectable range (m)\n        float fov_horizontal = 360.0f; // Horizontal field of view (degrees)\n        float fov_vertical = 30.0f;    // Vertical field of view (degrees)\n        int beams_horizontal = 1080;   // Number of horizontal beams\n        int beams_vertical = 64;       // Number of vertical beams\n        float angular_resolution_horizontal;\n        float angular_resolution_vertical;\n        float noise_std_dev = 0.02f;   // Standard deviation of noise (m)\n        float intensity_base = 100.0f; // Base intensity value\n        float drop_rate = 0.01f;       // Probability of dropouts\n    };\n\nprivate:\n    SensorConfig config_;\n    std::mt19937 rng_;\n    std::normal_distribution<float> noise_dist_;\n    std::uniform_real_distribution<float> dropout_dist_;\n    std::vector<std::vector<RayData>> scan_data_;\n\npublic:\n    LiDARSensorModel(const SensorConfig& config)\n        : config_(config),\n          rng_(std::random_device{}()),\n          noise_dist_(0.0f, config.noise_std_dev),\n          dropout_dist_(0.0f, 1.0f) {\n\n        config_.angular_resolution_horizontal =\n            config_.fov_horizontal / config_.beams_horizontal;\n        config_.angular_resolution_vertical =\n            config_.fov_vertical / config_.beams_vertical;\n\n        scan_data_.resize(config_.beams_vertical);\n        for (auto& row : scan_data_) {\n            row.resize(config_.beams_horizontal);\n        }\n    }\n\n    void simulateScan(const std::vector<std::vector<float>>& environment_depth_map,\n                     const std::vector<std::vector<float>>& reflectivity_map,\n                     std::vector<std::vector<RayData>>& output_scan) {\n\n        output_scan.resize(config_.beams_vertical);\n        for (auto& row : output_scan) {\n            row.resize(config_.beams_horizontal);\n        }\n\n        for (int v_idx = 0; v_idx < config_.beams_vertical; ++v_idx) {\n            for (int h_idx = 0; h_idx < config_.beams_horizontal; ++h_idx) {\n                float base_distance = getEnvironmentDistance(environment_depth_map, h_idx, v_idx);\n\n                if (base_distance > 0 && base_distance < config_.range_max) {\n                    // Apply noise model\n                    float noise = noise_dist_(rng_);\n                    float noisy_distance = base_distance + noise;\n\n                    // Apply dropout probability\n                    if (dropout_dist_(rng_) < config_.drop_rate) {\n                        noisy_distance = 0.0f; // Invalid measurement\n                    }\n\n                    // Apply intensity based on reflectivity and distance\n                    float reflectivity = getReflectivity(reflectivity_map, h_idx, v_idx);\n                    float intensity = calculateIntensity(reflectivity, noisy_distance);\n\n                    output_scan[v_idx][h_idx] = {\n                        noisy_distance,\n                        intensity,\n                        0, // return type\n                        static_cast<int8_t>(noise * 100), // noise level indicator\n                        h_idx * config_.angular_resolution_horizontal,\n                        v_idx * config_.angular_resolution_vertical\n                    };\n                } else {\n                    output_scan[v_idx][h_idx] = {0.0f, 0.0f, 0, 0, 0.0f, 0.0f};\n                }\n            }\n        }\n    }\n\nprivate:\n    float getEnvironmentDistance(const std::vector<std::vector<float>>& depth_map,\n                               int h_idx, int v_idx) {\n        // Interpolate from environment depth map based on beam angles\n        // This is a simplified representation - real implementation would ray-cast\n        // against 3D environment models\n        if (h_idx < depth_map.size() && v_idx < depth_map[0].size()) {\n            return depth_map[h_idx][v_idx];\n        }\n        return config_.range_max + 1.0f; // Out of range\n    }\n\n    float getReflectivity(const std::vector<std::vector<float>>& reflectivity_map,\n                         int h_idx, int v_idx) {\n        if (h_idx < reflectivity_map.size() && v_idx < reflectivity_map[0].size()) {\n            return reflectivity_map[h_idx][v_idx];\n        }\n        return 0.5f; // Default reflectivity\n    }\n\n    float calculateIntensity(float reflectivity, float distance) {\n        // Intensity model: decreases with distance squared\n        if (distance > 0) {\n            float base_intensity = config_.intensity_base * reflectivity / (distance * distance);\n            // Apply some additional noise to intensity\n            float intensity_noise = (dropout_dist_(rng_) - 0.5f) * 10.0f;\n            return std::max(0.0f, base_intensity + intensity_noise);\n        }\n        return 0.0f;\n    }\n};\n"})}),"\n",(0,t.jsx)(e.h4,{id:"imu-sensor-model",children:"IMU Sensor Model"}),"\n",(0,t.jsx)(e.p,{children:"Inertial Measurement Units (IMUs) measure linear acceleration and angular velocity. The sensor model includes several error sources:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Bias"}),": Systematic offset that changes over time"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scale Factor Error"}),": Deviation from ideal scaling"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Cross-Axis Sensitivity"}),": Crosstalk between different axes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Noise"}),": Random fluctuations following various statistical models"]}),"\n"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"// Advanced IMU sensor simulation\n#include <vector>\n#include <random>\n#include <chrono>\n\nclass IMUSensorModel {\npublic:\n    struct IMUReading {\n        std::array<float, 3> linear_acceleration;    // m/s\xb2\n        std::array<float, 3> angular_velocity;       // rad/s\n        std::array<float, 3> magnetic_field;         // \xb5T\n        std::chrono::time_point<std::chrono::system_clock> timestamp;\n    };\n\n    struct SensorSpecs {\n        // Noise parameters (Allan variance coefficients)\n        float accel_noise_density = 0.002f;      // m/s/sqrt(Hz)\n        float accel_bias_random_walk = 0.0004f;  // m/s\xb2/sqrt(Hz)\n        float gyro_noise_density = 0.00024f;     // rad/s/sqrt(Hz)\n        float gyro_bias_random_walk = 2.6e-6f;   // rad/s\xb2/sqrt(Hz)\n\n        // Bias parameters\n        std::array<float, 3> initial_accel_bias = {0.0f, 0.0f, 0.0f};\n        std::array<float, 3> initial_gyro_bias = {0.0f, 0.0f, 0.0f};\n\n        // Scale factor errors\n        std::array<float, 3> accel_scale_error = {0.0f, 0.0f, 0.0f};\n        std::array<float, 3> gyro_scale_error = {0.0f, 0.0f, 0.0f};\n\n        // Cross-axis sensitivity\n        std::array<std::array<float, 3>, 3> accel_cross_axis =\n            {{{1.0f, 0.0f, 0.0f}, {0.0f, 1.0f, 0.0f}, {0.0f, 0.0f, 1.0f}}};\n        std::array<std::array<float, 3>, 3> gyro_cross_axis =\n            {{{1.0f, 0.0f, 0.0f}, {0.0f, 1.0f, 0.0f}, {0.0f, 0.0f, 1.0f}}};\n    };\n\nprivate:\n    SensorSpecs specs_;\n    std::mt19937 rng_;\n    std::normal_distribution<float> accel_noise_dist_;\n    std::normal_distribution<float> gyro_noise_dist_;\n\n    // Current bias states (random walk)\n    std::array<float, 3> accel_bias_;\n    std::array<float, 3> gyro_bias_;\n\n    float dt_;  // Time step for bias random walk\n\npublic:\n    IMUSensorModel(const SensorSpecs& specs, float update_rate = 100.0f)\n        : specs_(specs),\n          rng_(std::random_device{}()),\n          accel_noise_dist_(0.0f, specs_.accel_noise_density),\n          gyro_noise_dist_(0.0f, specs_.gyro_noise_density),\n          dt_(1.0f / update_rate) {\n\n        accel_bias_ = specs_.initial_accel_bias;\n        gyro_bias_ = specs_.initial_gyro_bias;\n    }\n\n    IMUReading simulateReading(const std::array<float, 3>& true_accel,\n                              const std::array<float, 3>& true_gyro,\n                              const std::array<float, 3>& true_mag) {\n\n        IMUReading reading;\n        reading.timestamp = std::chrono::system_clock::now();\n\n        // Apply noise and bias to accelerometer\n        for (int i = 0; i < 3; ++i) {\n            // Add noise\n            float noise = accel_noise_dist_(rng_) / std::sqrt(dt_);\n\n            // Update bias using random walk model\n            float bias_walk = std::normal_distribution<float>(\n                0.0f, specs_.accel_bias_random_walk * std::sqrt(dt_)\n            )(rng_);\n            accel_bias_[i] += bias_walk;\n\n            // Apply all error sources\n            float noisy_accel = true_accel[i] + accel_bias_[i] + noise;\n\n            // Apply scale factor error\n            noisy_accel *= (1.0f + specs_.accel_scale_error[i]);\n\n            // Apply cross-axis sensitivity\n            float corrected_accel = 0.0f;\n            for (int j = 0; j < 3; ++j) {\n                corrected_accel += specs_.accel_cross_axis[i][j] * noisy_accel;\n            }\n\n            reading.linear_acceleration[i] = corrected_accel;\n        }\n\n        // Apply noise and bias to gyroscope\n        for (int i = 0; i < 3; ++i) {\n            // Add noise\n            float noise = gyro_noise_dist_(rng_) / std::sqrt(dt_);\n\n            // Update bias using random walk model\n            float bias_walk = std::normal_distribution<float>(\n                0.0f, specs_.gyro_bias_random_walk * std::sqrt(dt_)\n            )(rng_);\n            gyro_bias_[i] += bias_walk;\n\n            // Apply all error sources\n            float noisy_gyro = true_gyro[i] + gyro_bias_[i] + noise;\n\n            // Apply scale factor error\n            noisy_gyro *= (1.0f + specs_.gyro_scale_error[i]);\n\n            // Apply cross-axis sensitivity\n            float corrected_gyro = 0.0f;\n            for (int j = 0; j < 3; ++j) {\n                corrected_gyro += specs_.gyro_cross_axis[i][j] * noisy_gyro;\n            }\n\n            reading.angular_velocity[i] = corrected_gyro;\n        }\n\n        // Apply simple noise model to magnetometer\n        for (int i = 0; i < 3; ++i) {\n            float noise = std::normal_distribution<float>(0.0f, 0.1f)(rng_);\n            reading.magnetic_field[i] = true_mag[i] + noise;\n        }\n\n        return reading;\n    }\n\n    // Reset bias states (for calibration purposes)\n    void resetBias() {\n        accel_bias_ = specs_.initial_accel_bias;\n        gyro_bias_ = specs_.initial_gyro_bias;\n    }\n\n    // Get current bias states\n    const std::array<float, 3>& getAccelBias() const { return accel_bias_; }\n    const std::array<float, 3>& getGyroBias() const { return gyro_bias_; }\n};\n"})}),"\n",(0,t.jsx)(e.h3,{id:"camera-sensor-model",children:"Camera Sensor Model"}),"\n",(0,t.jsx)(e.p,{children:"Camera sensors require sophisticated modeling of optical phenomena including distortion, noise, and dynamic range limitations:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"// Advanced camera sensor simulation\n#include <opencv2/opencv.hpp>\n#include <vector>\n#include <random>\n\nclass CameraSensorModel {\npublic:\n    struct CameraConfig {\n        int width = 640;\n        int height = 480;\n        float fx = 320.0f;           // Focal length x\n        float fy = 320.0f;           // Focal length y\n        float cx = 320.0f;           // Principal point x\n        float cy = 240.0f;           // Principal point y\n        std::array<float, 4> distortion_coeffs = {0.0f, 0.0f, 0.0f, 0.0f}; // k1, k2, p1, p2\n        float fov_horizontal = 60.0f; // Field of view (degrees)\n        float fov_vertical = 45.0f;\n\n        // Noise parameters\n        float gaussian_noise_std = 10.0f;    // Standard deviation of Gaussian noise\n        float poisson_noise_factor = 0.01f;  // Factor for Poisson noise\n        float dark_current = 0.1f;           // Dark current (electrons/second)\n        float read_noise = 2.0f;             // Read noise (electrons)\n        float quantization_noise = 0.5f;     // Quantization noise\n    };\n\n    struct ExposureParams {\n        float exposure_time = 0.033f; // 33ms for 30fps\n        float gain = 1.0f;\n        float gamma = 1.0f;\n        float iso = 100.0f;\n    };\n\nprivate:\n    CameraConfig config_;\n    ExposureParams exposure_params_;\n    std::mt19937 rng_;\n    std::normal_distribution<float> gaussian_noise_dist_;\n    std::poisson_distribution<int> poisson_noise_dist_;\n\npublic:\n    CameraSensorModel(const CameraConfig& config, const ExposureParams& exposure = {})\n        : config_(config), exposure_params_(exposure),\n          rng_(std::random_device{}()),\n          gaussian_noise_dist_(0.0f, config_.gaussian_noise_std) {\n    }\n\n    cv::Mat simulateImage(const cv::Mat& scene_radiance) {\n        cv::Mat image = scene_radiance.clone();\n\n        // Apply optical effects and noise\n        applyLensEffects(image);\n        applySensorNoise(image);\n        applyDigitalProcessing(image);\n\n        // Ensure proper data range\n        cv::normalize(image, image, 0, 255, cv::NORM_MINMAX);\n        image.convertTo(image, CV_8UC3);\n\n        return image;\n    }\n\nprivate:\n    void applyLensEffects(cv::Mat& image) {\n        // Apply radial and tangential distortion\n        cv::Mat camera_matrix = (cv::Mat_<float>(3, 3) <<\n            config_.fx, 0, config_.cx,\n            0, config_.fy, config_.cy,\n            0, 0, 1);\n\n        cv::Mat dist_coeffs = (cv::Mat_<float>(1, 4) <<\n            config_.distortion_coeffs[0], config_.distortion_coeffs[1],\n            config_.distortion_coeffs[2], config_.distortion_coeffs[3]);\n\n        cv::Mat undistorted;\n        cv::undistort(image, undistorted, camera_matrix, dist_coeffs);\n        image = undistorted;\n    }\n\n    void applySensorNoise(cv::Mat& image) {\n        cv::Mat noise = cv::Mat::zeros(image.size(), image.type());\n\n        // Add Gaussian noise\n        for (int i = 0; i < image.rows; ++i) {\n            for (int j = 0; j < image.cols; ++j) {\n                for (int c = 0; c < image.channels(); ++c) {\n                    float pixel_noise = gaussian_noise_dist_(rng_);\n                    noise.at<cv::Vec3b>(i, j)[c] = static_cast<uint8_t>(\n                        std::max(0.0f, std::min(255.0f, pixel_noise))\n                    );\n                }\n            }\n        }\n\n        // Add Poisson noise (photon noise)\n        cv::Mat poisson_image = image.clone();\n        for (int i = 0; i < image.rows; ++i) {\n            for (int j = 0; j < image.cols; ++j) {\n                for (int c = 0; c < image.channels(); ++c) {\n                    float intensity = image.at<cv::Vec3b>(i, j)[c] / 255.0f;\n                    float photon_count = intensity * 1000.0f; // Scale to reasonable photon count\n                    int noisy_count = std::poisson_distribution<int>(photon_count)(rng_);\n                    poisson_image.at<cv::Vec3b>(i, j)[c] =\n                        static_cast<uint8_t>((noisy_count / 1000.0f) * 255.0f);\n                }\n            }\n        }\n\n        // Combine with original image\n        cv::addWeighted(image, 0.5, poisson_image, 0.5, 0.0, image);\n    }\n\n    void applyDigitalProcessing(cv::Mat& image) {\n        // Apply gamma correction\n        cv::Mat gamma_corrected;\n        applyGammaCorrection(image, gamma_corrected, exposure_params_.gamma);\n\n        // Apply gain\n        gamma_corrected.convertTo(image, -1, exposure_params_.gain, 0);\n\n        // Add quantization noise\n        for (int i = 0; i < image.rows; ++i) {\n            for (int j = 0; j < image.cols; ++j) {\n                for (int c = 0; c < image.channels(); ++c) {\n                    int value = image.at<cv::Vec3b>(i, j)[c];\n                    value += static_cast<int>(gaussian_noise_dist_(rng_));\n                    image.at<cv::Vec3b>(i, j)[c] =\n                        static_cast<uint8_t>(std::max(0, std::min(255, value)));\n                }\n            }\n        }\n    }\n\n    void applyGammaCorrection(const cv::Mat& src, cv::Mat& dst, float gamma) {\n        cv::Mat lookUpTable(1, 256, CV_8U);\n        uchar* ptr = lookUpTable.ptr();\n\n        for (int i = 0; i < 256; ++i) {\n            ptr[i] = cv::saturate_cast<uchar>(std::pow(i / 255.0, 1.0 / gamma) * 255.0);\n        }\n\n        cv::LUT(src, lookUpTable, dst);\n    }\n};\n"})}),"\n",(0,t.jsx)(e.h2,{id:"advanced-lidar-simulation-techniques",children:"Advanced LiDAR Simulation Techniques"}),"\n",(0,t.jsx)(e.h3,{id:"multi-echo-lidar-simulation",children:"Multi-Echo LiDAR Simulation"}),"\n",(0,t.jsx)(e.p,{children:"Modern LiDAR sensors can detect multiple returns from a single pulse, which is essential for simulating complex environments with transparent or semi-transparent objects:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"// Multi-echo LiDAR simulation\n#include <vector>\n#include <map>\n#include <algorithm>\n\nstruct MultiEchoMeasurement {\n    std::vector<float> distances;    // Multiple distance measurements\n    std::vector<float> intensities;  // Corresponding intensities\n    std::vector<uint8_t> types;      // Return types (first, last, intermediate)\n};\n\nclass MultiEchoLiDAR {\nprivate:\n    LiDARSensorModel::SensorConfig config_;\n    float multi_echo_threshold_ = 0.1f; // Minimum distance between echoes\n\npublic:\n    MultiEchoLiDAR(const LiDARSensorModel::SensorConfig& config)\n        : config_(config) {}\n\n    MultiEchoMeasurement simulateMultiEcho(const std::vector<float>& depth_profile) {\n        MultiEchoMeasurement result;\n\n        // Sort depth values and identify potential multiple returns\n        std::vector<std::pair<float, float>> sorted_depths; // (distance, reflectivity)\n        for (size_t i = 0; i < depth_profile.size(); ++i) {\n            if (depth_profile[i] > config_.range_min &&\n                depth_profile[i] < config_.range_max) {\n                sorted_depths.push_back({depth_profile[i], 0.5f}); // Assume default reflectivity\n            }\n        }\n\n        std::sort(sorted_depths.begin(), sorted_depths.end());\n\n        // Group nearby returns as potential multi-echo returns\n        if (!sorted_depths.empty()) {\n            float current_dist = sorted_depths[0].first;\n            result.distances.push_back(current_dist);\n            result.intensities.push_back(calculateIntensity(sorted_depths[0].second, current_dist));\n            result.types.push_back(0); // First return\n\n            for (size_t i = 1; i < sorted_depths.size(); ++i) {\n                float dist = sorted_depths[i].first;\n                if (dist - current_dist > multi_echo_threshold_) {\n                    // Significant gap - likely a separate return\n                    result.distances.push_back(dist);\n                    result.intensities.push_back(calculateIntensity(sorted_depths[i].second, dist));\n                    result.types.push_back(1); // Subsequent return\n                    current_dist = dist;\n                }\n            }\n\n            if (!result.distances.empty()) {\n                result.types.back() = 2; // Last return\n            }\n        }\n\n        return result;\n    }\n\nprivate:\n    float calculateIntensity(float reflectivity, float distance) {\n        if (distance > 0) {\n            return reflectivity / (distance * distance);\n        }\n        return 0.0f;\n    }\n};\n"})}),"\n",(0,t.jsx)(e.h2,{id:"integration-with-ros-2-and-isaac-sim",children:"Integration with ROS 2 and Isaac Sim"}),"\n",(0,t.jsx)(e.h3,{id:"ros-2-sensor-message-simulation",children:"ROS 2 Sensor Message Simulation"}),"\n",(0,t.jsx)(e.p,{children:"Proper integration with ROS 2 requires simulating sensor messages with the correct format and timing:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:'// ROS 2 sensor message simulation\n#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/laser_scan.hpp>\n#include <sensor_msgs/msg/imu.hpp>\n#include <sensor_msgs/msg/camera_info.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <tf2/LinearMath/Quaternion.h>\n#include <tf2_geometry_msgs/tf2_geometry_msgs.h>\n\nclass ROSSensorSimulator : public rclcpp::Node {\nprivate:\n    rclcpp::Publisher<sensor_msgs::msg::LaserScan>::SharedPtr lidar_pub_;\n    rclcpp::Publisher<sensor_msgs::msg::Imu>::SharedPtr imu_pub_;\n    rclcpp::Publisher<sensor_msgs::msg::Image>::SharedPtr camera_pub_;\n    rclcpp::Publisher<sensor_msgs::msg::CameraInfo>::SharedPtr camera_info_pub_;\n\n    LiDARSensorModel lidar_model_;\n    IMUSensorModel imu_model_;\n    CameraSensorModel camera_model_;\n\n    rclcpp::TimerBase::SharedPtr timer_;\n    size_t scan_count_;\n\npublic:\n    ROSSensorSimulator() : Node("sensor_simulator"), scan_count_(0) {\n        // Initialize publishers\n        lidar_pub_ = this->create_publisher<sensor_msgs::msg::LaserScan>("scan", 10);\n        imu_pub_ = this->create_publisher<sensor_msgs::msg::Imu>("imu/data", 10);\n        camera_pub_ = this->create_publisher<sensor_msgs::msg::Image>("camera/image_raw", 10);\n        camera_info_pub_ = this->create_publisher<sensor_msgs::msg::CameraInfo>("camera/camera_info", 10);\n\n        // Initialize sensor models with realistic parameters\n        LiDARSensorModel::SensorConfig lidar_config;\n        lidar_config.range_min = 0.1f;\n        lidar_config.range_max = 100.0f;\n        lidar_config.beams_horizontal = 1080;\n        lidar_config.fov_horizontal = 360.0f;\n        lidar_config.noise_std_dev = 0.02f;\n        lidar_model_ = LiDARSensorModel(lidar_config);\n\n        IMUSensorModel::SensorSpecs imu_specs;\n        imu_specs.accel_noise_density = 0.002f;\n        imu_specs.gyro_noise_density = 0.00024f;\n        imu_model_ = IMUSensorModel(imu_specs, 100.0f); // 100Hz update rate\n\n        CameraSensorModel::CameraConfig camera_config;\n        camera_config.width = 640;\n        camera_config.height = 480;\n        camera_config.fx = 320.0f;\n        camera_config.fy = 320.0f;\n        camera_config.cx = 320.0f;\n        camera_config.cy = 240.0f;\n        camera_model_ = CameraSensorModel(camera_config);\n\n        // Create timer for sensor simulation\n        timer_ = this->create_wall_timer(\n            std::chrono::milliseconds(10), // 100Hz for LiDAR/IMU\n            std::bind(&ROSSensorSimulator::timer_callback, this)\n        );\n    }\n\nprivate:\n    void timer_callback() {\n        // Simulate and publish LiDAR data\n        publish_lidar_data();\n\n        // Simulate and publish IMU data\n        publish_imu_data();\n\n        // Publish camera data at lower frequency (e.g., 30Hz)\n        if (scan_count_ % 3 == 0) { // Every 3rd call = ~33Hz\n            publish_camera_data();\n        }\n\n        scan_count_++;\n    }\n\n    void publish_lidar_data() {\n        auto msg = sensor_msgs::msg::LaserScan();\n        msg.header.stamp = this->get_clock()->now();\n        msg.header.frame_id = "laser_frame";\n\n        msg.angle_min = -M_PI;\n        msg.angle_max = M_PI;\n        msg.angle_increment = 2.0 * M_PI / 1080.0;\n        msg.time_increment = 0.0; // Not used for simulation\n        msg.scan_time = 0.1; // 10Hz\n        msg.range_min = 0.1;\n        msg.range_max = 100.0;\n\n        // Simulate some range data (in a real system, this would come from scene geometry)\n        msg.ranges.resize(1080);\n        for (size_t i = 0; i < msg.ranges.size(); ++i) {\n            // Create a simple pattern for simulation\n            float angle = msg.angle_min + i * msg.angle_increment;\n            float distance = 10.0f + 5.0f * std::sin(4 * angle); // 5m to 15m range\n\n            // Add some noise\n            std::random_device rd;\n            std::mt19937 gen(rd());\n            std::normal_distribution<float> noise(0.0, 0.02);\n            distance += noise(gen);\n\n            msg.ranges[i] = std::max(0.1f, std::min(100.0f, distance));\n        }\n\n        msg.intensities.resize(1080);\n        for (size_t i = 0; i < msg.intensities.size(); ++i) {\n            msg.intensities[i] = 100.0; // Constant intensity for simplicity\n        }\n\n        lidar_pub_->publish(msg);\n    }\n\n    void publish_imu_data() {\n        auto msg = sensor_msgs::msg::Imu();\n        msg.header.stamp = this->get_clock()->now();\n        msg.header.frame_id = "imu_frame";\n\n        // Simulate IMU data with some motion\n        std::array<float, 3> true_accel = {0.0f, 0.0f, 9.81f}; // Gravity\n        std::array<float, 3> true_gyro = {0.01f, 0.0f, 0.0f}; // Small rotation\n        std::array<float, 3> true_mag = {0.2f, 0.0f, 0.4f}; // Magnetic field\n\n        auto simulated_reading = imu_model_.simulateReading(true_accel, true_gyro, true_mag);\n\n        // Fill ROS message\n        msg.linear_acceleration.x = simulated_reading.linear_acceleration[0];\n        msg.linear_acceleration.y = simulated_reading.linear_acceleration[1];\n        msg.linear_acceleration.z = simulated_reading.linear_acceleration[2];\n\n        msg.angular_velocity.x = simulated_reading.angular_velocity[0];\n        msg.angular_velocity.y = simulated_reading.angular_velocity[1];\n        msg.angular_velocity.z = simulated_reading.angular_velocity[2];\n\n        // For quaternion, we\'ll create a simple orientation\n        tf2::Quaternion q;\n        q.setRPY(0.01, 0.0, 0.0); // Small roll due to acceleration\n        msg.orientation.x = q.x();\n        msg.orientation.y = q.y();\n        msg.orientation.z = q.z();\n        msg.orientation.w = q.w();\n\n        // Set covariance (information matrix) - indicate uncertainty\n        for (int i = 0; i < 9; ++i) {\n            msg.linear_acceleration_covariance[i] = 0.0;\n            msg.angular_velocity_covariance[i] = 0.0;\n            msg.orientation_covariance[i] = 0.0;\n        }\n        // Set diagonal elements to indicate sensor noise\n        msg.linear_acceleration_covariance[0] = 0.01; // 100ug^2\n        msg.linear_acceleration_covariance[4] = 0.01;\n        msg.linear_acceleration_covariance[8] = 0.01;\n        msg.angular_velocity_covariance[0] = 0.001; // (0.1 deg/s)^2\n        msg.angular_velocity_covariance[4] = 0.001;\n        msg.angular_velocity_covariance[8] = 0.001;\n\n        imu_pub_->publish(msg);\n    }\n\n    void publish_camera_data() {\n        // Create a synthetic image for simulation\n        cv::Mat synthetic_image = cv::Mat(480, 640, CV_8UC3);\n\n        // Create a simple test pattern\n        for (int i = 0; i < synthetic_image.rows; ++i) {\n            for (int j = 0; j < synthetic_image.cols; ++j) {\n                synthetic_image.at<cv::Vec3b>(i, j) = cv::Vec3b(\n                    static_cast<uint8_t>((i * 255) / synthetic_image.rows),\n                    static_cast<uint8_t>((j * 255) / synthetic_image.cols),\n                    128\n                );\n            }\n        }\n\n        // Simulate with our camera model\n        cv::Mat simulated_image = camera_model_.simulateImage(synthetic_image);\n\n        // Convert to ROS message\n        auto img_msg = cv_bridge::CvImage(\n            std_msgs::msg::Header(), "bgr8", simulated_image\n        ).toImageMsg();\n\n        img_msg->header.stamp = this->get_clock()->now();\n        img_msg->header.frame_id = "camera_frame";\n\n        camera_pub_->publish(*img_msg);\n\n        // Publish camera info\n        auto info_msg = sensor_msgs::msg::CameraInfo();\n        info_msg.header = img_msg->header;\n        info_msg.width = 640;\n        info_msg.height = 480;\n        info_msg.distortion_model = "plumb_bob";\n        info_msg.d = {0.0, 0.0, 0.0, 0.0, 0.0}; // No distortion\n        info_msg.k = {320.0, 0.0, 320.0, 0.0, 320.0, 240.0, 0.0, 0.0, 1.0};\n        info_msg.r = {1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0};\n        info_msg.p = {320.0, 0.0, 320.0, 0.0, 0.0, 320.0, 240.0, 0.0, 0.0, 0.0, 1.0, 0.0};\n\n        camera_info_pub_->publish(info_msg);\n    }\n};\n\n// Example main function to run the ROS 2 sensor simulator\nint main(int argc, char * argv[]) {\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<ROSSensorSimulator>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"isaac-sim-integration-for-sensor-simulation",children:"Isaac Sim Integration for Sensor Simulation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-mermaid",children:'graph TB\n    A[Real World Sensors] --\x3e B{Isaac Sim Sensor Simulation}\n    B --\x3e C[LiDAR Sensor]\n    B --\x3e D[IMU Sensor]\n    B --\x3e E[Camera Sensor]\n    B --\x3e F[GPS Sensor]\n\n    C --\x3e G[RosBridge - LiDAR]\n    D --\x3e H[RosBridge - IMU]\n    E --\x3e I[RosBridge - Camera]\n    F --\x3e J[RosBridge - GPS]\n\n    G --\x3e K[ROS 2 Network]\n    H --\x3e K\n    I --\x3e K\n    J --\x3e K\n\n    K --\x3e L[Robot Control System]\n    L --\x3e M[Perception Algorithms]\n    L --\x3e N[Localization System]\n    L --\x3e O[Planning System]\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style L fill:#e8f5e8\n    style M fill:#fff3e0\n</graph>\n\n## Sensor Calibration in Simulation\n\nCalibration is critical for accurate sensor simulation. The process involves determining both intrinsic and extrinsic parameters:\n\n### Intrinsic Calibration\n\n```python\nimport numpy as np\nimport cv2\nfrom scipy.optimize import least_squares\nfrom typing import List, Tuple\n\nclass SensorCalibrator:\n    """Class for sensor calibration in simulation environments."""\n\n    def __init__(self):\n        self.calibration_data = []\n\n    def calibrate_camera_intrinsic(self, images: List[np.ndarray],\n                                 pattern_size: Tuple[int, int] = (9, 6)) -> Tuple[np.ndarray, np.ndarray]:\n        """Calibrate camera intrinsic parameters using a chessboard pattern."""\n        # Prepare object points (3D points in real world space)\n        objp = np.zeros((pattern_size[0] * pattern_size[1], 3), np.float32)\n        objp[:, :2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1, 2)\n\n        # Arrays to store object points and image points from all images\n        objpoints = []  # 3d points in real world space\n        imgpoints = []  # 2d points in image plane\n\n        for img in images:\n            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n            # Find the chess board corners\n            ret, corners = cv2.findChessboardCorners(gray, pattern_size, None)\n\n            # If found, add object points, image points (after refining them)\n            if ret:\n                objpoints.append(objp)\n\n                # Refine corner locations\n                criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n                corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n                imgpoints.append(corners2)\n\n        # Perform camera calibration\n        if len(objpoints) > 0 and len(imgpoints) > 0:\n            ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(\n                objpoints, imgpoints, gray.shape[::-1], None, None\n            )\n\n            return mtx, dist  # Camera matrix and distortion coefficients\n        else:\n            raise ValueError("Could not find chessboard corners in any images")\n\n    def calibrate_lidar_camera_extrinsics(self, lidar_points: np.ndarray,\n                                        camera_points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        """Calibrate LiDAR to camera extrinsic parameters."""\n        # This is a simplified version - in practice, you\'d need corresponding points\n        # from both sensors to perform the calibration\n\n        # For simulation purposes, we can synthesize the transformation\n        # using known sensor placements in the robot model\n\n        # Assuming we have corresponding points from both sensors\n        # Use RANSAC or other robust estimation methods\n        if len(lidar_points) != len(camera_points):\n            raise ValueError("Mismatched number of points between sensors")\n\n        # Compute transformation using Open3D or similar\n        # For this example, we\'ll return a plausible transformation matrix\n\n        # Create a plausible transformation matrix\n        transform = np.eye(4, dtype=np.float32)\n        # Example: LiDAR is 0.2m forward, 0.1m up, and 0.05m to the right of camera\n        transform[0, 3] = 0.2  # x translation\n        transform[1, 3] = 0.05 # y translation\n        transform[2, 3] = 0.1  # z translation (height)\n\n        # Example: Small rotation between sensors\n        rx, ry, rz = 0.01, 0.02, 0.005  # Small rotations in radians\n        transform[0:3, 0:3] = self.euler_to_rotation_matrix(rx, ry, rz)\n\n        return transform[0:3, 0:3], transform[0:3, 3]  # Rotation matrix and translation vector\n\n    def calibrate_imu_to_body(self, measured_data: List[dict]) -> np.ndarray:\n        """Calibrate IMU to body frame transformation."""\n        # This would use IMU measurements in known static positions\n        # to determine the IMU\'s bias and mounting orientation\n\n        # Accumulate measurements over time\n        accel_measurements = []\n        gyro_measurements = []\n        expected_gravity = np.array([0, 0, 9.81])  # Expected gravity in body frame\n\n        for measurement in measured_data:\n            accel = np.array(measurement[\'accel\'])\n            gyro = np.array(measurement[\'gyro\'])\n            accel_measurements.append(accel)\n            gyro_measurements.append(gyro)\n\n        # Calculate average bias\n        avg_accel = np.mean(accel_measurements, axis=0)\n        avg_gyro = np.mean(gyro_measurements, axis=0)\n\n        # Determine transformation from measured gravity direction\n        # This is a simplified approach - full calibration is more complex\n        gravity_direction = avg_accel - np.array([0, 0, 9.81])  # Remove expected gravity\n\n        # Create calibration result structure\n        calibration_result = {\n            \'accel_bias\': avg_accel,\n            \'gyro_bias\': avg_gyro,\n            \'gravity_alignment\': gravity_direction\n        }\n\n        return calibration_result\n\n    def euler_to_rotation_matrix(self, rx: float, ry: float, rz: float) -> np.ndarray:\n        """Convert Euler angles to rotation matrix."""\n        # Rotation around x axis\n        Rx = np.array([\n            [1, 0, 0],\n            [0, np.cos(rx), -np.sin(rx)],\n            [0, np.sin(rx), np.cos(rx)]\n        ])\n\n        # Rotation around y axis\n        Ry = np.array([\n            [np.cos(ry), 0, np.sin(ry)],\n            [0, 1, 0],\n            [-np.sin(ry), 0, np.cos(ry)]\n        ])\n\n        # Rotation around z axis\n        Rz = np.array([\n            [np.cos(rz), -np.sin(rz), 0],\n            [np.sin(rz), np.cos(rz), 0],\n            [0, 0, 1]\n        ])\n\n        # Combined rotation\n        R = Rz @ Ry @ Rx\n        return R\n\n# Calibration pipeline example\ndef calibration_pipeline_example():\n    """Example of a complete sensor calibration pipeline."""\n    calibrator = SensorCalibrator()\n\n    # Step 1: Camera intrinsic calibration\n    print("Performing camera intrinsic calibration...")\n\n    # In a real scenario, you would capture images of a calibration pattern\n    # For this example, we\'ll create synthetic calibration images\n    calib_images = []\n    for i in range(10):\n        # Create synthetic calibration image\n        img = np.zeros((480, 640, 3), dtype=np.uint8)\n        # Add a synthetic chessboard pattern with distortions\n        for row in range(6):\n            for col in range(9):\n                x = col * 60 + 50\n                y = row * 60 + 50\n                if (row + col) % 2 == 0:\n                    cv2.rectangle(img, (x, y), (x+50, y+50), (255, 255, 255), -1)\n                else:\n                    cv2.rectangle(img, (x, y), (x+50, y+50), (0, 0, 0), -1)\n\n        # Add some synthetic distortion\n        k1, k2, p1, p2 = -0.1, 0.05, 0.001, -0.001\n        # Apply distortion would happen here in a real implementation\n        calib_images.append(img)\n\n    try:\n        camera_matrix, distortion_coeffs = calibrator.calibrate_camera_intrinsic(calib_images)\n        print(f"Camera calibration completed!")\n        print(f"Camera matrix:\\n{camera_matrix}")\n        print(f"Distortion coefficients: {distortion_coeffs}")\n    except ValueError as e:\n        print(f"Camera calibration failed: {e}")\n\n    # Step 2: LiDAR-Camera extrinsic calibration\n    print("\\nPerforming LiDAR-camera extrinsic calibration...")\n\n    # Create synthetic corresponding points\n    lidar_pts = np.random.rand(100, 3).astype(np.float32) * 10  # Points within 10m\n    camera_pts = lidar_pts.copy()  # Simplified - in reality these would be different\n\n    try:\n        rotation, translation = calibrator.calibrate_lidar_camera_extrinsics(lidar_pts, camera_pts)\n        print(f"LiDAR-Camera extrinsic calibration completed!")\n        print(f"Rotation matrix:\\n{rotation}")\n        print(f"Translation vector: {translation}")\n    except ValueError as e:\n        print(f"LiDAR-camera calibration failed: {e}")\n\n    # Step 3: IMU calibration\n    print("\\nPerforming IMU calibration...")\n\n    # Create synthetic IMU data (stationary)\n    imu_data = []\n    for i in range(100):\n        # Simulate IMU readings with some bias\n        accel = np.array([0.1, -0.05, 9.85]) + np.random.normal(0, 0.01, 3)  # Slightly offset from true gravity\n        gyro = np.array([0.001, -0.002, 0.003]) + np.random.normal(0, 0.001, 3)  # Small drift\n\n        imu_data.append({\n            \'accel\': accel,\n            \'gyro\': gyro,\n            \'timestamp\': i * 0.01  # 100Hz data\n        })\n\n    imu_calibration = calibrator.calibrate_imu_to_body(imu_data)\n    print(f"IMU calibration completed!")\n    print(f"Accel bias: {imu_calibration[\'accel_bias\']}")\n    print(f"Gyro bias: {imu_calibration[\'gyro_bias\']}")\n\nif __name__ == "__main__":\n    calibration_pipeline_example()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"isaac-sim-configuration-for-sensor-simulation",children:"Isaac Sim Configuration for Sensor Simulation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-json",children:'{\n  "isaac_sim_sensor_config": {\n    "lidar": {\n      "enabled": true,\n      "lidar_params": {\n        "rotation_frequency": 10,\n        "channels": 64,\n        "points_per_channel": 1080,\n        "upper_fov": 2.0,\n        "lower_fov": -24.8,\n        "range": 100.0,\n        "return_empty_spaces": false,\n        "return_strongest_echo": true,\n        "return_all_echoes": false,\n        "sensor_x": 0.3,\n        "sensor_y": 0.0,\n        "sensor_z": 1.0,\n        "sensor_rotx": 0.0,\n        "sensor_roty": 0.0,\n        "sensor_rotz": 0.0,\n        "enable_composite_sweep": true,\n        "noise_mean": 0.0,\n        "noise_std": 0.01,\n        "motion_blur_enabled": true,\n        "motion_blur_samples": 4\n      }\n    },\n    "camera": {\n      "enabled": true,\n      "camera_params": {\n        "resolution": [640, 480],\n        "focal_length": [320.0, 320.0],\n        "principal_point": [320.0, 240.0],\n        "distortion_model": "plumb_bob",\n        "distortion_coefficients": [0.0, 0.0, 0.0, 0.0, 0.0],\n        "sensor_x": 0.2,\n        "sensor_y": 0.0,\n        "sensor_z": 0.8,\n        "sensor_rotx": 0.0,\n        "sensor_roty": 0.0,\n        "sensor_rotz": 0.0,\n        "fps": 30,\n        "exposure": 0.033,\n        "iso": 100,\n        "gain": 1.0,\n        "noise_params": {\n          "gaussian_noise_std": 10.0,\n          "poisson_noise_factor": 0.01,\n          "dark_current": 0.1,\n          "read_noise": 2.0\n        }\n      }\n    },\n    "imu": {\n      "enabled": true,\n      "imu_params": {\n        "sensor_x": 0.0,\n        "sensor_y": 0.0,\n        "sensor_z": 0.7,\n        "sensor_rotx": 0.0,\n        "sensor_roty": 0.0,\n        "sensor_rotz": 0.0,\n        "update_frequency": 100,\n        "linear_acceleration_noise": 0.002,\n        "angular_velocity_noise": 0.00024,\n        "linear_acceleration_bias": [0.0, 0.0, 0.0],\n        "angular_velocity_bias": [0.0, 0.0, 0.0],\n        "linear_acceleration_random_walk": 0.0004,\n        "angular_velocity_random_walk": 2.6e-06\n      }\n    },\n    "ros_bridge": {\n      "enabled": true,\n      "topics": {\n        "lidar_topic": "/scan",\n        "camera_topic": "/camera/image_raw",\n        "camera_info_topic": "/camera/camera_info",\n        "imu_topic": "/imu/data",\n        "tf_topic": "/tf"\n      },\n      "frame_ids": {\n        "lidar_frame": "laser_frame",\n        "camera_frame": "camera_frame",\n        "imu_frame": "imu_frame",\n        "base_frame": "base_link"\n      }\n    },\n    "sensor_fusion": {\n      "enabled": true,\n      "fusion_params": {\n        "time_sync_tolerance": 0.01,\n        "calibration_file": "/path/to/calibration.json",\n        "extrinsic_transforms": {\n          "lidar_to_camera": [0.2, 0.05, 0.1, 0.01, 0.02, 0.005],\n          "imu_to_body": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n        }\n      }\n    }\n  }\n}\n'})}),"\n",(0,t.jsx)(e.h2,{id:"performance-optimization-for-real-time-sensor-simulation",children:"Performance Optimization for Real-time Sensor Simulation"}),"\n",(0,t.jsx)(e.p,{children:"For real-time digital twin applications, sensor simulation must be optimized for performance:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-cpp",children:"// Optimized sensor simulation using multi-threading\n#include <thread>\n#include <mutex>\n#include <queue>\n#include <atomic>\n#include <memory>\n#include <functional>\n\nclass OptimizedSensorSimulator {\nprivate:\n    std::atomic<bool> running_{false};\n    std::vector<std::thread> worker_threads_;\n    std::queue<std::function<void()>> task_queue_;\n    std::mutex queue_mutex_;\n    std::condition_variable condition_;\n    int num_threads_;\n\n    // Sensor simulation models\n    std::unique_ptr<LiDARSensorModel> lidar_model_;\n    std::unique_ptr<IMUSensorModel> imu_model_;\n    std::unique_ptr<CameraSensorModel> camera_model_;\n\npublic:\n    OptimizedSensorSimulator(int num_threads = 4) : num_threads_(num_threads) {\n        // Initialize sensor models\n        LiDARSensorModel::SensorConfig lidar_config;\n        lidar_config.range_min = 0.1f;\n        lidar_config.range_max = 100.0f;\n        lidar_config.beams_horizontal = 1080;\n        lidar_config.beams_vertical = 64;\n        lidar_model_ = std::make_unique<LiDARSensorModel>(lidar_config);\n\n        IMUSensorModel::SensorSpecs imu_specs;\n        imu_model_ = std::make_unique<IMUSensorModel>(imu_specs, 100.0f);\n\n        CameraSensorModel::CameraConfig camera_config;\n        camera_config.width = 640;\n        camera_config.height = 480;\n        camera_model_ = std::make_unique<CameraSensorModel>(camera_config);\n\n        start();\n    }\n\n    ~OptimizedSensorSimulator() {\n        stop();\n    }\n\n    void start() {\n        running_ = true;\n\n        // Launch worker threads\n        for (int i = 0; i < num_threads_; ++i) {\n            worker_threads_.emplace_back([this] {\n                worker_thread();\n            });\n        }\n    }\n\n    void stop() {\n        running_ = false;\n        condition_.notify_all();\n\n        for (auto& thread : worker_threads_) {\n            if (thread.joinable()) {\n                thread.join();\n            }\n        }\n    }\n\n    void schedule_sensor_task(std::function<void()> task) {\n        {\n            std::unique_lock<std::mutex> lock(queue_mutex_);\n            task_queue_.push(task);\n        }\n        condition_.notify_one();\n    }\n\n    void simulate_lidar_async(const std::vector<std::vector<float>>& env_data,\n                            std::function<void(const std::vector<std::vector<LiDARSensorModel::RayData>>&)> callback) {\n        auto task = [this, env_data, callback]() {\n            std::vector<std::vector<LiDARSensorModel::RayData>> scan_data;\n            lidar_model_->simulateScan(env_data, std::vector<std::vector<float>>(), scan_data);\n            callback(scan_data);\n        };\n\n        schedule_sensor_task(task);\n    }\n\n    void simulate_imu_async(const std::array<float, 3>& true_accel,\n                          const std::array<float, 3>& true_gyro,\n                          const std::array<float, 3>& true_mag,\n                          std::function<void(const IMUSensorModel::IMUReading&)> callback) {\n        auto task = [this, true_accel, true_gyro, true_mag, callback]() {\n            auto reading = imu_model_->simulateReading(true_accel, true_gyro, true_mag);\n            callback(reading);\n        };\n\n        schedule_sensor_task(task);\n    }\n\nprivate:\n    void worker_thread() {\n        while (running_) {\n            std::function<void()> task;\n\n            {\n                std::unique_lock<std::mutex> lock(queue_mutex_);\n                condition_.wait(lock, [this] { return !task_queue_.empty() || !running_; });\n\n                if (!running_ && task_queue_.empty()) {\n                    return;\n                }\n\n                if (!task_queue_.empty()) {\n                    task = task_queue_.front();\n                    task_queue_.pop();\n                }\n            }\n\n            if (task) {\n                task();\n            }\n        }\n    }\n};\n"})}),"\n",(0,t.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(e.p,{children:"Sensor simulation in digital twin environments for humanoid robots requires sophisticated modeling that accounts for the full range of sensor characteristics, environmental interactions, and system integration requirements. The implementation of realistic sensor models with proper noise, bias, and calibration considerations enables effective development and testing of perception and control algorithms before deployment on physical systems."}),"\n",(0,t.jsx)(e.p,{children:"The integration with ROS 2 and Isaac Sim provides the necessary infrastructure for real-time simulation that can be used in conjunction with robot control systems, making the transition from simulation to reality more reliable and predictable."})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>r,x:()=>o});var a=i(6540);const t={},s=a.createContext(t);function r(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);